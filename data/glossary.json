[
  {
    "name": "Neural Network",
    "slug": "neural-network",
    "shortDef": "A computing system inspired by biological neural networks that learns to perform tasks by considering examples without being explicitly programmed.",
    "fullDef": "A neural network is a computational model composed of layers of interconnected nodes (neurons) that process information using connectionist approaches. Each connection between neurons carries a weight that is adjusted during training, allowing the network to learn patterns from data.\n\nNeural networks typically consist of an input layer, one or more hidden layers, and an output layer. During forward propagation, data flows through these layers, with each neuron applying a weighted sum followed by an activation function to produce its output. The network learns by comparing its predictions to expected outputs and adjusting weights through backpropagation.\n\nModern neural networks form the backbone of most AI systems today, from image classifiers to language models. Their ability to approximate complex functions and learn hierarchical representations of data has made them the dominant paradigm in machine learning research and applications.",
    "category": "Fundamentals",
    "relatedTerms": [
      "deep-learning",
      "activation-function",
      "backpropagation"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Deep Learning",
    "slug": "deep-learning",
    "shortDef": "A subset of machine learning that uses neural networks with many layers to learn complex patterns and representations from large amounts of data.",
    "fullDef": "Deep learning refers to neural networks with multiple hidden layers (hence \"deep\") that can learn increasingly abstract representations of data at each successive layer. This hierarchical feature learning is what distinguishes deep learning from shallow machine learning approaches and enables it to tackle highly complex tasks.\n\nIn a deep learning model, early layers might learn simple features such as edges in images or phonemes in speech, while deeper layers compose these into more complex concepts like faces or words. This automatic feature extraction eliminates the need for manual feature engineering, which was a major bottleneck in traditional machine learning pipelines.\n\nDeep learning has driven breakthroughs in computer vision, natural language processing, speech recognition, and generative AI. Architectures such as convolutional neural networks, recurrent neural networks, and transformers are all forms of deep learning that have achieved superhuman performance on specific benchmarks.",
    "category": "Fundamentals",
    "relatedTerms": [
      "neural-network",
      "machine-learning",
      "convolutional-neural-network"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Machine Learning",
    "slug": "machine-learning",
    "shortDef": "A field of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed.",
    "fullDef": "Machine learning is a branch of artificial intelligence focused on building algorithms and statistical models that allow computer systems to improve their performance on a task through experience. Rather than following rigid, hand-coded rules, ML systems identify patterns in data and use those patterns to make predictions or decisions.\n\nMachine learning is broadly categorized into supervised learning (learning from labeled examples), unsupervised learning (finding structure in unlabeled data), and reinforcement learning (learning through trial-and-error interaction with an environment). Each paradigm is suited to different types of problems, from classification and regression to clustering, dimensionality reduction, and sequential decision-making.\n\nThe field has grown rapidly thanks to increases in available data, computational power, and algorithmic innovations. Machine learning underpins a vast range of modern applications including recommendation systems, fraud detection, autonomous vehicles, medical diagnostics, and language translation.",
    "category": "Fundamentals",
    "relatedTerms": [
      "deep-learning",
      "reinforcement-learning",
      "overfitting"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Transformer",
    "slug": "transformer",
    "shortDef": "A neural network architecture based on self-attention mechanisms that processes input data in parallel, forming the basis of modern large language models.",
    "fullDef": "The transformer is a neural network architecture introduced in the 2017 paper \"Attention Is All You Need\" by Vaswani et al. Unlike recurrent neural networks that process sequences step by step, transformers use self-attention mechanisms to process all positions of the input simultaneously, enabling massive parallelization during training.\n\nA transformer consists of an encoder and a decoder, each built from stacks of layers containing multi-head self-attention and feed-forward sub-layers. The self-attention mechanism allows each token in a sequence to attend to every other token, capturing long-range dependencies far more effectively than RNNs. Positional encodings are added to the input embeddings to retain information about token order.\n\nTransformers have become the dominant architecture in natural language processing and are increasingly used in computer vision, audio processing, and multimodal AI. Models like GPT, BERT, and their successors are all built on the transformer architecture, and they power today's most capable large language models.",
    "category": "NLP",
    "relatedTerms": [
      "attention-mechanism",
      "large-language-model",
      "embedding"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Attention Mechanism",
    "slug": "attention-mechanism",
    "shortDef": "A technique that allows neural networks to focus on relevant parts of the input when producing each element of the output.",
    "fullDef": "An attention mechanism is a component in neural networks that computes a weighted combination of input representations, where the weights indicate the relevance of each input element to the current processing step. This allows the model to dynamically focus on the most pertinent information rather than relying on a fixed-size hidden state.\n\nIn the context of transformers, self-attention (also called scaled dot-product attention) computes queries, keys, and values from the input. The attention score between any two positions is the dot product of their query and key vectors, scaled and passed through a softmax function. Multi-head attention extends this by running several attention operations in parallel, allowing the model to attend to information from different representation subspaces.\n\nAttention mechanisms were originally developed for sequence-to-sequence models in machine translation but have since been adopted across virtually all areas of deep learning. They are the key innovation that makes transformers so effective and have been adapted for use in computer vision (vision transformers), speech processing, and protein structure prediction.",
    "category": "Deep Learning",
    "relatedTerms": [
      "transformer",
      "neural-network",
      "recurrent-neural-network"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Gradient Descent",
    "slug": "gradient-descent",
    "shortDef": "An optimization algorithm that iteratively adjusts model parameters in the direction that minimizes the loss function.",
    "fullDef": "Gradient descent is the foundational optimization algorithm used to train neural networks and many other machine learning models. It works by computing the gradient (partial derivatives) of the loss function with respect to each model parameter, then updating the parameters in the opposite direction of the gradient to reduce the loss.\n\nThere are several variants of gradient descent. Batch gradient descent computes the gradient over the entire training dataset, which is accurate but computationally expensive. Stochastic gradient descent (SGD) updates parameters using a single example at a time, introducing noise but enabling faster iterations. Mini-batch gradient descent strikes a balance by computing gradients over small batches of data and is the most commonly used approach in practice.\n\nModern deep learning typically employs adaptive gradient methods such as Adam, RMSProp, and AdaGrad, which maintain per-parameter learning rates that adapt based on the history of gradients. The choice of optimizer, learning rate, and learning rate schedule significantly impacts training speed and the quality of the final model.",
    "category": "Fundamentals",
    "relatedTerms": [
      "backpropagation",
      "loss-function",
      "batch-normalization"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Backpropagation",
    "slug": "backpropagation",
    "shortDef": "An algorithm for computing gradients of the loss function with respect to each weight in a neural network by applying the chain rule layer by layer.",
    "fullDef": "Backpropagation (short for \"backward propagation of errors\") is the algorithm that makes training deep neural networks computationally feasible. It efficiently computes the gradient of the loss function with respect to every weight in the network by applying the chain rule of calculus from the output layer back through each hidden layer to the input.\n\nDuring a training step, the forward pass computes the network's prediction, and the loss function measures the error. Backpropagation then works backward: it calculates how much each weight contributed to the error by propagating gradients through the layers. These gradients are then used by an optimization algorithm like gradient descent to update the weights.\n\nBackpropagation was popularized in the 1986 paper by Rumelhart, Hinton, and Williams, and it remains the primary method for training neural networks today. Modern deep learning frameworks like PyTorch and TensorFlow implement automatic differentiation, which generalizes backpropagation and handles gradient computation transparently for arbitrary computational graphs.",
    "category": "Fundamentals",
    "relatedTerms": [
      "gradient-descent",
      "loss-function",
      "neural-network"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Loss Function",
    "slug": "loss-function",
    "shortDef": "A function that measures the difference between a model's predictions and the actual target values, guiding the optimization process during training.",
    "fullDef": "A loss function (also called a cost function or objective function) quantifies how well or poorly a model's predictions match the expected outputs. During training, the goal is to minimize this function, which drives the model to learn the correct mapping from inputs to outputs.\n\nCommon loss functions include mean squared error (MSE) for regression tasks, cross-entropy loss for classification tasks, and specialized losses like contrastive loss for embedding learning or adversarial loss for GANs. The choice of loss function encodes assumptions about the task and directly affects what the model learns to optimize.\n\nDesigning appropriate loss functions is a critical aspect of machine learning engineering. A poorly chosen loss function can lead to models that optimize for the wrong objective, producing technically low-loss outputs that fail to meet real-world requirements. Researchers continue to develop novel loss functions tailored to specific domains, including perceptual losses for image generation and reward-based losses for reinforcement learning from human feedback.",
    "category": "Fundamentals",
    "relatedTerms": [
      "gradient-descent",
      "backpropagation",
      "overfitting"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Activation Function",
    "slug": "activation-function",
    "shortDef": "A mathematical function applied to a neuron's output that introduces non-linearity, enabling neural networks to learn complex patterns.",
    "fullDef": "An activation function is a non-linear transformation applied to the weighted sum of inputs at each neuron in a neural network. Without activation functions, a neural network would be equivalent to a single linear transformation regardless of its depth, severely limiting its ability to model complex relationships.\n\nCommon activation functions include the Rectified Linear Unit (ReLU), which outputs the input if positive and zero otherwise; the sigmoid function, which squashes values to the range (0, 1); the hyperbolic tangent (tanh), which maps values to (-1, 1); and the Gaussian Error Linear Unit (GELU), which is widely used in transformers. Each has trade-offs in terms of gradient flow, computational cost, and suitability for specific tasks.\n\nThe choice of activation function significantly impacts training dynamics. ReLU and its variants (Leaky ReLU, PReLU) addressed the vanishing gradient problem that plagued sigmoid and tanh activations in deep networks. More recent activation functions like SiLU/Swish and GELU have shown improvements in certain architectures, particularly in large language models and vision transformers.",
    "category": "Deep Learning",
    "relatedTerms": [
      "neural-network",
      "backpropagation",
      "deep-learning"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Overfitting",
    "slug": "overfitting",
    "shortDef": "A phenomenon where a model learns the training data too well, including its noise and outliers, resulting in poor performance on unseen data.",
    "fullDef": "Overfitting occurs when a machine learning model captures not only the true underlying patterns in the training data but also the random noise and idiosyncrasies specific to that dataset. An overfitted model will perform exceptionally well on training data but fail to generalize to new, unseen examples, which defeats the purpose of building a predictive model.\n\nSigns of overfitting include a large gap between training accuracy and validation accuracy, or a validation loss that begins increasing while training loss continues to decrease. Overfitting is more likely when the model is too complex relative to the amount of training data, when training runs for too many epochs, or when the data contains significant noise.\n\nNumerous techniques exist to combat overfitting. Regularization methods (L1, L2, dropout) constrain the model's capacity. Data augmentation increases the effective size of the training set. Early stopping halts training when validation performance begins to degrade. Batch normalization can also have a regularizing effect. Cross-validation helps detect overfitting during model selection, and ensemble methods can improve generalization by combining multiple models.",
    "category": "Fundamentals",
    "relatedTerms": [
      "machine-learning",
      "loss-function",
      "batch-normalization"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Convolutional Neural Network",
    "slug": "convolutional-neural-network",
    "shortDef": "A neural network architecture that uses convolutional layers to automatically learn spatial hierarchies of features, primarily used for image and video analysis.",
    "fullDef": "A convolutional neural network (CNN) is a specialized type of neural network designed to process data with a grid-like topology, such as images. CNNs use convolutional layers that apply learnable filters (kernels) across the input, detecting local patterns like edges, textures, and shapes. This parameter-sharing approach is far more efficient than fully connected layers for spatial data.\n\nA typical CNN architecture consists of alternating convolutional and pooling layers followed by one or more fully connected layers. Convolutional layers extract features at increasing levels of abstraction, pooling layers reduce spatial dimensions and provide a degree of translational invariance, and the fully connected layers at the end perform classification or regression based on the extracted features.\n\nCNNs have driven major advances in computer vision, including image classification (AlexNet, ResNet, EfficientNet), object detection (YOLO, Faster R-CNN), semantic segmentation (U-Net), and image generation. While vision transformers have recently challenged CNN dominance in some benchmarks, CNNs remain widely used due to their efficiency, well-understood behavior, and strong performance especially with limited data.",
    "category": "Computer Vision",
    "relatedTerms": [
      "deep-learning",
      "neural-network",
      "activation-function"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Recurrent Neural Network",
    "slug": "recurrent-neural-network",
    "shortDef": "A neural network architecture with loops that allow information to persist across time steps, designed for processing sequential data.",
    "fullDef": "A recurrent neural network (RNN) is a class of neural networks where connections between neurons form directed cycles, creating an internal state (memory) that allows the network to process sequences of inputs. At each time step, the RNN takes the current input and its previous hidden state to produce an output and a new hidden state, enabling it to model temporal dependencies.\n\nVanilla RNNs suffer from the vanishing and exploding gradient problems, making it difficult to learn long-range dependencies. This led to the development of gated architectures like Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), which use gating mechanisms to control the flow of information and better preserve gradients over long sequences.\n\nWhile RNNs were once the dominant architecture for sequential tasks like language modeling, machine translation, and speech recognition, they have been largely superseded by transformers for most NLP applications. However, RNNs and their variants still find use in time-series forecasting, real-time audio processing, and other domains where sequential processing is natural or where computational constraints favor recurrent over attention-based architectures.",
    "category": "Deep Learning",
    "relatedTerms": [
      "neural-network",
      "transformer",
      "attention-mechanism"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Large Language Model",
    "slug": "large-language-model",
    "shortDef": "A neural network trained on vast amounts of text data that can understand and generate human language with remarkable fluency and versatility.",
    "fullDef": "A large language model (LLM) is a transformer-based neural network with billions (or trillions) of parameters trained on massive corpora of text data. LLMs learn statistical patterns in language through self-supervised pretraining, typically using a next-token prediction objective, enabling them to generate coherent and contextually appropriate text across a wide range of tasks.\n\nThe capabilities of LLMs scale with model size, data quantity, and compute investment, a relationship described by scaling laws. Modern LLMs like GPT-4, Claude, Gemini, and Llama demonstrate emergent abilities at sufficient scale, including in-context learning, chain-of-thought reasoning, and the ability to follow complex instructions. Post-training techniques such as reinforcement learning from human feedback (RLHF) further align these models with human preferences and values.\n\nLLMs have transformed AI applications across industries, powering chatbots, code assistants, content generation tools, and research aids. Active areas of research include improving reasoning capabilities, reducing hallucinations, extending context windows, enabling tool use, and developing more efficient architectures that deliver strong performance at smaller scales.",
    "category": "NLP",
    "relatedTerms": [
      "transformer",
      "fine-tuning",
      "tokenization"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Fine-tuning",
    "slug": "fine-tuning",
    "shortDef": "The process of further training a pretrained model on a smaller, task-specific dataset to adapt it for a particular use case.",
    "fullDef": "Fine-tuning is a transfer learning technique in which a model that has been pretrained on a large general-purpose dataset is further trained on a smaller dataset specific to the target task. This approach leverages the knowledge already encoded in the pretrained model's weights, typically requiring far less data and compute than training from scratch.\n\nThere are several approaches to fine-tuning. Full fine-tuning updates all model parameters, which can be expensive for large models. Parameter-efficient fine-tuning methods such as LoRA (Low-Rank Adaptation), prefix tuning, and adapter layers update only a small fraction of parameters while achieving competitive performance. Instruction fine-tuning trains models on input-output pairs formatted as instructions, improving their ability to follow directions.\n\nFine-tuning is central to deploying large language models and other foundation models for specific applications. It allows organizations to customize general-purpose models for domain-specific tasks like medical question answering, legal document analysis, or code generation, achieving strong performance with relatively modest datasets and computational budgets.",
    "category": "MLOps",
    "relatedTerms": [
      "transfer-learning",
      "large-language-model",
      "machine-learning"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Transfer Learning",
    "slug": "transfer-learning",
    "shortDef": "A technique where a model trained on one task is reused as the starting point for a model on a different but related task.",
    "fullDef": "Transfer learning is a machine learning strategy that takes a model developed for one task and repurposes it for a second, related task. The core insight is that features learned from large datasets on general tasks (such as ImageNet for vision or large text corpora for NLP) are broadly useful and can be transferred to more specific problems where labeled data may be scarce.\n\nIn practice, transfer learning typically involves taking a pretrained model, freezing some or all of its early layers (which capture general features), and training the later layers on the new task's dataset. This dramatically reduces training time and data requirements compared to training from scratch. The pretrained model serves as a strong initialization that already understands fundamental patterns in the data domain.\n\nTransfer learning has been a driving force behind the democratization of AI. Before its widespread adoption, achieving state-of-the-art results required massive datasets and computational resources. Now, practitioners can fine-tune openly available pretrained models to achieve strong results on specialized tasks with limited data, making advanced AI accessible to smaller teams and organizations.",
    "category": "Fundamentals",
    "relatedTerms": [
      "fine-tuning",
      "deep-learning",
      "embedding"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Reinforcement Learning",
    "slug": "reinforcement-learning",
    "shortDef": "A machine learning paradigm where an agent learns to make decisions by taking actions in an environment and receiving rewards or penalties.",
    "fullDef": "Reinforcement learning (RL) is a type of machine learning in which an agent interacts with an environment by taking actions, observing the resulting states, and receiving reward signals. The agent's goal is to learn a policy - a mapping from states to actions - that maximizes cumulative reward over time. Unlike supervised learning, RL does not require labeled input-output pairs; instead, the agent discovers optimal behavior through trial and error.\n\nKey concepts in RL include the state space, action space, reward function, value function, and policy. Algorithms are broadly divided into model-free methods (such as Q-learning, policy gradient, and actor-critic methods) and model-based methods that learn a model of the environment. Deep reinforcement learning combines deep neural networks with RL algorithms, enabling agents to handle high-dimensional state spaces like raw images.\n\nReinforcement learning has achieved landmark results including superhuman gameplay in Go (AlphaGo), Atari games, and StarCraft II. Beyond games, RL is applied in robotics, autonomous driving, recommendation systems, and increasingly in fine-tuning large language models through RLHF (reinforcement learning from human feedback), where human preferences serve as the reward signal.",
    "category": "Reinforcement Learning",
    "relatedTerms": [
      "machine-learning",
      "neural-network",
      "large-language-model"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Generative Adversarial Network",
    "slug": "generative-adversarial-network",
    "shortDef": "A framework consisting of two neural networks - a generator and a discriminator - that compete against each other to produce increasingly realistic synthetic data.",
    "fullDef": "A generative adversarial network (GAN) is a deep learning framework introduced by Ian Goodfellow in 2014 that trains two neural networks simultaneously in a minimax game. The generator network learns to produce synthetic data (such as images) that resemble the training data, while the discriminator network learns to distinguish between real and generated samples. Through this adversarial process, both networks improve, and the generator eventually produces highly realistic outputs.\n\nTraining GANs is notoriously challenging due to issues like mode collapse (where the generator produces limited variety), training instability, and the difficulty of evaluating generation quality. Numerous GAN variants have been developed to address these challenges, including DCGAN, Wasserstein GAN, StyleGAN, and Progressive GAN. Techniques such as spectral normalization, progressive growing, and careful architectural choices have significantly improved GAN training stability.\n\nGANs have produced remarkable results in image synthesis, producing photorealistic faces, artwork, and scenes. They have been applied to image super-resolution, style transfer, data augmentation, drug discovery, and video generation. While diffusion models have recently surpassed GANs in some image generation benchmarks, GANs remain influential and continue to be used in applications requiring fast inference and real-time generation.",
    "category": "Deep Learning",
    "relatedTerms": [
      "deep-learning",
      "neural-network",
      "loss-function"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Embedding",
    "slug": "embedding",
    "shortDef": "A learned dense vector representation that maps discrete entities like words or items into a continuous vector space where similar items are closer together.",
    "fullDef": "An embedding is a dense, low-dimensional vector representation of a discrete object such as a word, sentence, image, or user profile. Embeddings are learned during training and capture semantic relationships: objects with similar meanings or properties end up close together in the vector space, enabling mathematical operations on concepts.\n\nWord embeddings (such as Word2Vec, GloVe, and contextual embeddings from BERT or GPT) revolutionized NLP by providing meaningful numerical representations of words. The classic example is that the vector arithmetic \"king - man + woman\" yields a vector close to \"queen.\" Modern transformer-based models produce contextual embeddings where the same word receives different vectors depending on its surrounding context.\n\nEmbeddings are fundamental to modern AI systems far beyond NLP. They are used in recommendation systems (user and item embeddings), information retrieval (document embeddings for semantic search), computer vision (image embeddings from CNNs or ViTs), and multimodal systems (joint embeddings of text and images, as in CLIP). Embedding-based similarity search powers many production applications, from search engines to retrieval-augmented generation (RAG) systems.",
    "category": "NLP",
    "relatedTerms": [
      "tokenization",
      "transformer",
      "large-language-model"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Tokenization",
    "slug": "tokenization",
    "shortDef": "The process of breaking text into smaller units called tokens, which serve as the fundamental input elements for language models.",
    "fullDef": "Tokenization is the process of converting raw text into a sequence of discrete tokens that a language model can process. Tokens can be whole words, subwords, individual characters, or even bytes, depending on the tokenization strategy. The tokenizer defines the vocabulary of a model and directly impacts its ability to handle different languages, rare words, and specialized terminology.\n\nModern language models primarily use subword tokenization algorithms such as Byte Pair Encoding (BPE), WordPiece, and SentencePiece. These methods strike a balance between character-level tokenization (which has a tiny vocabulary but produces long sequences) and word-level tokenization (which cannot handle out-of-vocabulary words). Subword tokenizers learn to split rare words into meaningful sub-units while keeping common words as single tokens.\n\nTokenization has significant practical implications for language model performance and cost. The number of tokens in a text determines computational requirements and, for commercial APIs, the cost of inference. Different tokenizers handle multilingual text, code, and special characters with varying effectiveness. Research into tokenizer-free models and byte-level processing aims to overcome some of the limitations inherent in predefined tokenization schemes.",
    "category": "NLP",
    "relatedTerms": [
      "embedding",
      "large-language-model",
      "transformer"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Batch Normalization",
    "slug": "batch-normalization",
    "shortDef": "A technique that normalizes the inputs of each layer in a neural network across the current mini-batch, stabilizing and accelerating training.",
    "fullDef": "Batch normalization (BatchNorm) is a technique introduced by Ioffe and Szegedy in 2015 that normalizes the activations of each layer by adjusting and scaling them based on the mean and variance computed over the current mini-batch. After normalization, learned scale and shift parameters are applied, allowing the network to undo the normalization if that is optimal for the task.\n\nThe primary benefits of batch normalization include faster training convergence, the ability to use higher learning rates, and reduced sensitivity to weight initialization. It also provides a mild regularization effect, reducing the need for other regularization techniques like dropout in some cases. During inference, running averages of the batch statistics computed during training are used instead of per-batch statistics.\n\nBatch normalization has become a standard component in many deep learning architectures, particularly in convolutional neural networks for computer vision. However, it has limitations: it works poorly with very small batch sizes and can behave differently during training and inference. Alternatives like Layer Normalization (used in transformers), Group Normalization, and Instance Normalization have been developed for scenarios where BatchNorm is suboptimal.",
    "category": "Deep Learning",
    "relatedTerms": [
      "deep-learning",
      "gradient-descent",
      "overfitting"
    ],
    "lastUpdated": "2026-02-20"
  },
  {
    "name": "Perceptron",
    "slug": "perceptron",
    "shortDef": "The simplest neural network unit that computes a weighted sum of inputs, adds a bias, and passes the result through an activation function to produce an output.",
    "fullDef": "A perceptron is the fundamental building block of neural networks, originally proposed by Frank Rosenblatt in 1958. It takes multiple inputs, multiplies each by a learnable weight, sums them together with a bias term, and passes the result through a step function (or other activation) to produce a binary output.\n\nThe perceptron can learn to classify linearly separable data by adjusting its weights through a simple learning rule. However, it famously cannot solve problems like XOR that require non-linear decision boundaries. This limitation was highlighted by Minsky and Papert in 1969 and led to the first AI winter. The solution came decades later with multi-layer perceptrons (MLPs) -- stacking multiple layers of perceptrons with non-linear activations, which can approximate any continuous function.\n\nModern neural networks are essentially vast networks of perceptron-like units. Each neuron in a neural network performs the same fundamental operation: weighted sum plus bias, followed by a non-linear activation. The key insight is that while a single perceptron is limited, layers of them create the representational power needed for deep learning.",
    "category": "Fundamentals",
    "relatedTerms": [
      "neural-network",
      "activation-function",
      "deep-learning"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Bias-Variance Tradeoff",
    "slug": "bias-variance-tradeoff",
    "shortDef": "The fundamental tension in machine learning between a model being too simple to capture patterns (high bias) and too complex, fitting noise instead of signal (high variance).",
    "fullDef": "The bias-variance tradeoff is arguably the most important concept in machine learning. Prediction errors come from two sources: bias (the model is too simple to capture the true pattern, leading to systematic underprediction or overprediction) and variance (the model is too sensitive to the specific training data, performing well on training examples but poorly on new data).\n\nHigh-bias models underfit -- they have high error on both training and test data because they cannot represent the complexity of the underlying relationship. High-variance models overfit -- they have low training error but high test error because they memorize noise in the training data rather than learning the true signal. The classic illustration is fitting polynomials: a degree-1 polynomial underfits a sine wave (high bias), while a degree-15 polynomial overfits to noise (high variance).\n\nThe goal is to find the sweet spot that balances both. Techniques like regularization, cross-validation, and early stopping help manage this tradeoff. In practice, you start simple (high bias, low variance), evaluate whether the model underfits or overfits, and adjust complexity accordingly. Understanding this tradeoff is essential for choosing model complexity, tuning hyperparameters, and diagnosing why a model fails in production.",
    "category": "Fundamentals",
    "relatedTerms": [
      "overfitting",
      "underfitting",
      "regularization"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Regularization",
    "slug": "regularization",
    "shortDef": "A set of techniques that constrain model complexity during training to prevent overfitting and improve generalization to unseen data.",
    "fullDef": "Regularization adds a penalty for model complexity to the training objective, encoding the principle that simpler models generalize better (Occam's Razor). Instead of just minimizing prediction error, the model minimizes error plus a complexity penalty: `Loss = Error + lambda * Complexity`.\n\nThe most common forms are L2 regularization (Ridge), which penalizes the sum of squared weights and shrinks all weights toward zero, and L1 regularization (Lasso), which penalizes the sum of absolute weights and drives some weights to exactly zero, performing automatic feature selection. Elastic Net combines both approaches. The regularization strength lambda controls the bias-variance tradeoff: higher lambda means simpler models with more bias but less variance.\n\nRegularization has deep mathematical foundations. L2 regularization is equivalent to maximum a posteriori estimation with a Gaussian prior on weights (encoding the belief that weights should be small). L1 regularization corresponds to a Laplace prior (encoding sparsity). Other regularization techniques include dropout (randomly zeroing neurons during training), early stopping (halting training before the model overfits), and data augmentation (artificially expanding the training set). All regularization methods share the same philosophy: constrain the hypothesis space to prevent the model from fitting noise.",
    "category": "Optimization",
    "relatedTerms": [
      "overfitting",
      "dropout",
      "bias-variance-tradeoff",
      "loss-function"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Dropout",
    "slug": "dropout",
    "shortDef": "A regularization technique that randomly sets a fraction of neuron activations to zero during each training step, preventing co-adaptation and reducing overfitting.",
    "fullDef": "Dropout, introduced by Srivastava et al. in 2014, works by randomly zeroing out neuron activations with a given probability (typically 0.5 for hidden layers) during each forward pass in training. The remaining activations are scaled by 1/(1-p) to maintain the expected value. At test time, all neurons are used without dropout.\n\nThe technique works through multiple complementary mechanisms. First, it prevents co-adaptation: neurons cannot rely on specific other neurons being present, forcing each to learn more robust and independent features. Second, it acts as an implicit ensemble: each training step uses a different random sub-network, and the full network at test time approximates the average prediction of exponentially many (2^n) sub-networks. For linear models, dropout on inputs is mathematically equivalent to L2 regularization.\n\nIn practice, dropout rate of 0.5 is common for hidden layers, 0.2 for input layers, and 0 for output layers. Convolutional layers typically use lower rates (0.1-0.2) or spatial dropout. Dropout is one of the most widely used regularization techniques in deep learning and was a key innovation that made training large neural networks practical without catastrophic overfitting.",
    "category": "Deep Learning",
    "relatedTerms": [
      "regularization",
      "overfitting",
      "neural-network"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Learning Rate",
    "slug": "learning-rate",
    "shortDef": "A hyperparameter that controls how much model weights are adjusted in response to the estimated error during each step of gradient descent optimization.",
    "fullDef": "The learning rate determines the step size when updating model parameters during training. At each iteration, weights are updated by: `weight = weight - learning_rate * gradient`. This single number has an outsized impact on whether training succeeds or fails.\n\nIf the learning rate is too high, updates overshoot the minimum and training diverges -- loss oscillates wildly or goes to infinity (NaN). If the learning rate is too low, training converges extremely slowly, requiring thousands of extra iterations and potentially getting stuck in poor local minima. The ideal learning rate depends on the loss landscape, model architecture, batch size, and training stage.\n\nModern practice often uses learning rate schedules that change the rate during training: warmup (start small and increase), cosine annealing (gradually decrease), or step decay (reduce at fixed intervals). Adaptive optimizers like Adam automatically adjust effective learning rates per-parameter. Finding the right learning rate is often the single most important hyperparameter tuning decision, and techniques like learning rate range tests help identify good starting values.",
    "category": "Optimization",
    "relatedTerms": [
      "gradient-descent",
      "hyperparameter",
      "stochastic-gradient-descent",
      "adam-optimizer"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Epoch",
    "slug": "epoch",
    "shortDef": "One complete pass through the entire training dataset during model training.",
    "fullDef": "An epoch represents one full cycle through every example in the training dataset. If you have 10,000 training examples and use a batch size of 100, one epoch consists of 100 gradient update steps. Training typically runs for multiple epochs -- anywhere from a few to hundreds -- allowing the model to see and learn from each example multiple times.\n\nThe number of epochs is a critical training decision tied to the bias-variance tradeoff. Too few epochs means the model hasn't learned enough from the data (underfitting). Too many epochs means the model starts memorizing training data rather than learning generalizable patterns (overfitting). The optimal number is typically determined by monitoring validation loss and applying early stopping when it begins to increase.\n\nIn practice, the term is also used to describe training progress and schedule milestones. Learning rate schedules are often defined in terms of epochs (e.g., reduce learning rate by 10x at epoch 30 and 60). It is important to distinguish epochs from iterations (individual gradient update steps) and from the total number of gradient updates, which depends on both epochs and batch size.",
    "category": "Fundamentals",
    "relatedTerms": [
      "overfitting",
      "learning-rate",
      "hyperparameter"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Stochastic Gradient Descent",
    "slug": "stochastic-gradient-descent",
    "shortDef": "An optimization algorithm that updates model parameters using the gradient computed on a small random subset (mini-batch) of the training data rather than the entire dataset.",
    "fullDef": "Stochastic Gradient Descent (SGD) is a variant of gradient descent that computes parameter updates using randomly sampled mini-batches instead of the full training set. While standard gradient descent computes the exact gradient over all training examples (expensive for large datasets), SGD uses noisy gradient estimates from small batches (typically 32-256 examples), making each update much faster.\n\nThe noise in SGD's gradient estimates is actually beneficial. It helps escape saddle points and poor local minima in the high-dimensional loss landscape of neural networks. Research has shown that in high-dimensional spaces, most critical points are saddle points rather than local minima, and SGD's inherent noise provides the perturbation needed to escape them. SGD also has an implicit regularization effect -- it biases the optimization toward flatter minima that tend to generalize better.\n\nSGD with momentum is the standard variant, where a running average of past gradients accelerates convergence and dampens oscillations. While adaptive methods like Adam have become popular, SGD with momentum and careful learning rate tuning often achieves the best final generalization performance, particularly for computer vision tasks. The choice between SGD and adaptive methods remains an active area of practice.",
    "category": "Optimization",
    "relatedTerms": [
      "gradient-descent",
      "learning-rate",
      "adam-optimizer",
      "backpropagation"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Adam Optimizer",
    "slug": "adam-optimizer",
    "shortDef": "An adaptive learning rate optimization algorithm that maintains per-parameter learning rates based on first and second moment estimates of gradients.",
    "fullDef": "Adam (Adaptive Moment Estimation) is one of the most widely used optimizers in deep learning, combining ideas from momentum (tracking exponential moving averages of gradients) and RMSprop (tracking exponential moving averages of squared gradients). For each parameter, Adam adapts the learning rate based on the history of that parameter's gradients.\n\nAdam maintains two moving averages per parameter: the first moment (mean of gradients, providing momentum) and the second moment (mean of squared gradients, providing per-parameter scaling). Parameters with consistently large gradients get smaller effective learning rates, while parameters with small or sparse gradients get larger ones. This is particularly useful for problems with sparse gradients or when different parameters operate at different scales.\n\nIn practice, Adam converges faster than vanilla SGD and requires less learning rate tuning, making it the default choice for many practitioners. Common hyperparameters are learning rate 0.001, beta1 0.9, and beta2 0.999. However, Adam can sometimes generalize worse than well-tuned SGD, particularly in computer vision. Variants like AdamW (which fixes weight decay behavior) and LAMB (for large-batch training) address some of Adam's limitations.",
    "category": "Optimization",
    "relatedTerms": [
      "gradient-descent",
      "stochastic-gradient-descent",
      "learning-rate"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Cross-Entropy",
    "slug": "cross-entropy",
    "shortDef": "A loss function that measures the difference between a model's predicted probability distribution and the true distribution, widely used for classification tasks.",
    "fullDef": "Cross-entropy H(p, q) = -sum(p(x) * log(q(x))) measures the average number of bits needed to encode data from the true distribution p using a code optimized for the predicted distribution q. When the model's predictions perfectly match reality, cross-entropy equals entropy (the theoretical minimum). Any deviation increases cross-entropy.\n\nFor classification, cross-entropy loss is the standard choice because of several deep mathematical properties. It is equivalent to negative log-likelihood (maximizing the probability the model assigns to correct labels), it penalizes confident wrong predictions exponentially more than uncertain ones (-log(0.01) = 4.6 vs -log(0.5) = 0.69), and when combined with softmax, it produces elegantly simple gradients (prediction minus truth). Binary cross-entropy is the special case for two-class problems.\n\nCross-entropy connects information theory to machine learning: minimizing cross-entropy is equivalent to minimizing KL divergence between true and predicted distributions (since the entropy of the true distribution is constant). This gives a principled reason why cross-entropy is the right loss for classification -- it directly measures how well the model's probability estimates match the true data distribution.",
    "category": "Optimization",
    "relatedTerms": [
      "loss-function",
      "softmax",
      "gradient-descent"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Softmax",
    "slug": "softmax",
    "shortDef": "A function that converts a vector of real numbers into a probability distribution, where each output is between 0 and 1 and all outputs sum to 1.",
    "fullDef": "The softmax function takes a vector of arbitrary real-valued scores (logits) and transforms them into probabilities: softmax(x_i) = exp(x_i) / sum(exp(x_j)). It is used as the final layer in classification networks to produce class probabilities and as a normalization step in the attention mechanism of transformers.\n\nSoftmax amplifies differences through exponentiation: larger values get disproportionately more probability mass. For example, logits [5, 4, 0] become approximately [0.73, 0.27, 0.005] -- the highest score dominates. This sparsifying behavior is controlled by a temperature parameter: dividing logits by temperature tau before softmax adjusts sharpness. Low temperature (tau approaching 0) makes the output nearly one-hot, while high temperature (tau approaching infinity) makes it nearly uniform.\n\nIn transformers, softmax converts attention scores into attention weights (probability distributions over which tokens to attend to). The scaling factor 1/sqrt(d_k) in scaled dot-product attention prevents logits from growing too large with high dimensionality, which would cause softmax to saturate and produce near-zero gradients. Understanding softmax is essential for working with both classification models and modern transformer architectures.",
    "category": "Deep Learning",
    "relatedTerms": [
      "cross-entropy",
      "attention-mechanism",
      "temperature"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Sigmoid",
    "slug": "sigmoid",
    "shortDef": "An S-shaped activation function that maps any real number to a value between 0 and 1, historically important but largely replaced by ReLU in hidden layers.",
    "fullDef": "The sigmoid function sigma(x) = 1/(1 + e^(-x)) squashes any input into the range (0, 1), making it useful for producing probability-like outputs. It was one of the earliest activation functions used in neural networks and is still used in the output layer for binary classification and in gating mechanisms (like LSTMs).\n\nHowever, sigmoid has significant drawbacks for hidden layers in deep networks. It saturates for large positive or negative inputs (sigma'(x) approaches 0), causing the vanishing gradient problem: gradients become extremely small in deep networks, making early layers nearly impossible to train. Additionally, sigmoid outputs are not zero-centered (always positive), which can cause inefficient zigzag gradient updates.\n\nFor these reasons, ReLU and its variants have largely replaced sigmoid in hidden layers of modern deep networks. Sigmoid remains important in specific contexts: binary classification output layers, attention gating in LSTMs and GRUs, and any situation where you need a smooth function mapping to (0, 1). Understanding why sigmoid fails in deep hidden layers is a key insight in the history of deep learning.",
    "category": "Deep Learning",
    "relatedTerms": [
      "activation-function",
      "relu",
      "vanishing-gradients"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "ReLU",
    "slug": "relu",
    "shortDef": "The Rectified Linear Unit activation function, defined as max(0, x), which has become the default non-linearity in modern deep networks due to its simple gradient and computational efficiency.",
    "fullDef": "ReLU (Rectified Linear Unit) computes f(x) = max(0, x): it outputs the input directly if positive, and zero otherwise. Despite its simplicity, ReLU was a breakthrough that helped make deep learning practical. Its gradient is either 1 (for positive inputs) or 0 (for negative inputs), eliminating the vanishing gradient problem that plagued sigmoid and tanh activations.\n\nReLU offers several advantages: its gradient does not saturate for positive values (unlike sigmoid/tanh), enabling effective training of deep networks; it produces sparse activations (roughly half of neurons output zero), which is computationally efficient and creates more interpretable representations; and max(0, x) is trivially fast to compute compared to exponentials in sigmoid/tanh.\n\nThe main drawback is the dying ReLU problem: neurons that receive only negative inputs always output zero and stop learning entirely (their gradient is permanently zero). Variants address this: Leaky ReLU allows a small gradient for negative inputs (f(x) = max(0.01x, x)), and GELU (used in transformers) provides a smoother approximation. He initialization was specifically developed to account for ReLU's variance reduction (zeroing half of activations), using Var(w) = 2/n_in instead of Xavier's 2/(n_in + n_out).",
    "category": "Deep Learning",
    "relatedTerms": [
      "activation-function",
      "sigmoid",
      "weight-initialization",
      "vanishing-gradients"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Weight Initialization",
    "slug": "weight-initialization",
    "shortDef": "The strategy for setting initial values of neural network parameters before training begins, critical for ensuring stable signal and gradient propagation through deep networks.",
    "fullDef": "Weight initialization determines the starting point of optimization and can mean the difference between a network that trains in 10 epochs and one that never trains at all. Initializing all weights to zero creates a symmetry problem where all neurons compute identical functions and receive identical gradients, making learning impossible. Initializing too large causes activations and gradients to explode; too small causes them to vanish.\n\nThe key principle is variance preservation: initialize weights so that the variance of activations remains approximately constant across layers during forward propagation, and gradient variance remains constant during backpropagation. Xavier (Glorot) initialization sets Var(w) = 2/(n_in + n_out), balancing forward and backward signal flow, and works well with tanh and sigmoid activations. He initialization sets Var(w) = 2/n_in, compensating for the fact that ReLU zeros out roughly half of activations.\n\nProper initialization makes the network an approximate isometry -- a transformation that preserves distances -- so information flows forward and gradients flow backward without amplification or attenuation. Before Xavier and He initialization (pre-2010), training networks deeper than 5 layers was nearly impossible. These initialization methods, along with batch normalization and residual connections, were key breakthroughs enabling modern deep learning.",
    "category": "Deep Learning",
    "relatedTerms": [
      "vanishing-gradients",
      "exploding-gradients",
      "relu",
      "neural-network"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Residual Connection",
    "slug": "residual-connection",
    "shortDef": "A shortcut that adds a layer's input directly to its output (y = F(x) + x), enabling training of very deep networks by providing a gradient highway that prevents vanishing gradients.",
    "fullDef": "Residual connections, introduced in ResNets by He et al. in 2015, add the identity of the input to the output of a layer: y = F(x) + x, where F(x) represents the learned transformation. Instead of learning the full desired mapping H(x), the network only needs to learn the residual F(x) = H(x) - x. If the optimal transformation is close to identity, the network just needs to learn F(x) close to zero, which is much easier.\n\nResidual connections solve the gradient vanishing problem through a gradient highway. During backpropagation, the gradient through a residual block is d(y)/d(x) = d(F)/d(x) + I, where I is the identity matrix. The +I term ensures gradients can always flow directly backward regardless of what the learned transformation does. This enables training networks with 100+ layers, where plain networks fail even to fit training data beyond ~20 layers.\n\nResidual networks can also be viewed as implicit ensembles of exponentially many paths of varying depth (2^L paths for L blocks), where most gradient flow uses short paths of length O(log L). They are now a fundamental component of virtually all deep architectures: CNNs (ResNet, DenseNet), transformers (every layer uses residual connections around attention and feed-forward sub-layers), and generative models (U-Net skip connections).",
    "category": "Deep Learning",
    "relatedTerms": [
      "vanishing-gradients",
      "neural-network",
      "deep-learning"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Universal Approximation Theorem",
    "slug": "universal-approximation-theorem",
    "shortDef": "A theorem proving that a neural network with a single hidden layer and non-linear activation can approximate any continuous function to arbitrary precision, given enough neurons.",
    "fullDef": "The Universal Approximation Theorem (Cybenko 1989, Hornik et al. 1989) states that a feedforward network with one hidden layer containing a finite number of neurons with a non-polynomial activation function (such as sigmoid or ReLU) can approximate any continuous function on a compact domain to any desired accuracy. Each neuron defines a ridge in input space, and by combining many such ridges with different orientations and positions, any smooth surface can be approximated.\n\nHowever, the theorem is often misleadingly interpreted as neural networks can learn anything. It only guarantees existence of an approximation, not that gradient descent will find it, that a reasonable number of neurons will suffice (the required width could be exponentially large), or that the approximation will generalize from finite training data. It is a statement about representational capacity, not about learnability.\n\nThe practical significance is that neural networks are not fundamentally limited in what functions they can represent. The real challenges lie in optimization (finding good weights), generalization (performing well on unseen data), and efficiency (using a reasonable number of parameters). Deep networks are preferred over wide shallow ones because they can represent compositional functions exponentially more efficiently, exploiting the hierarchical structure present in real-world data.",
    "category": "Deep Learning",
    "relatedTerms": [
      "neural-network",
      "activation-function",
      "deep-learning"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Feature Engineering",
    "slug": "feature-engineering",
    "shortDef": "The process of transforming raw data into informative input features that make patterns more accessible to machine learning models.",
    "fullDef": "Feature engineering is the art and science of creating input representations that help models learn effectively. Raw data is often not in a form that models can directly use: categorical variables like zip codes need encoding, numerical features may benefit from log transforms to capture diminishing returns, and interaction terms (e.g., bedrooms times square footage) can reveal relationships invisible to simple models.\n\nIn classical machine learning, feature engineering is where most of the intelligence resides. A well-engineered feature set with a simple linear model often outperforms a complex model on raw features, especially with limited data. Domain knowledge is critical: knowing that house prices have diminishing returns on size suggests using log(sqft), while knowing that neighborhood matters suggests one-hot encoding zip codes rather than treating them as numbers.\n\nDeep learning automates much of feature engineering by learning representations directly from raw data -- early layers learn simple features (edges, textures), while deeper layers compose them into complex concepts. However, feature engineering remains important even in deep learning: choosing what data to include, how to preprocess it, and what domain-specific transformations to apply can significantly impact performance, especially with limited training data.",
    "category": "Fundamentals",
    "relatedTerms": [
      "machine-learning",
      "bias-variance-tradeoff"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Hyperparameter",
    "slug": "hyperparameter",
    "shortDef": "A configuration value set before training begins that controls the learning process itself, as opposed to model parameters which are learned from data.",
    "fullDef": "Hyperparameters are the knobs you set before training that determine how the model learns. Unlike model parameters (weights, biases) that are learned from data via gradient descent, hyperparameters are chosen by the practitioner and include learning rate, batch size, number of layers, number of neurons per layer, regularization strength (lambda), dropout rate, and number of training epochs.\n\nHyperparameter selection is critical and surprisingly difficult. The learning rate alone can determine whether training succeeds or fails entirely. Common approaches include grid search (trying all combinations from a predefined set), random search (which is often more efficient than grid search), and Bayesian optimization (which uses past results to intelligently choose what to try next). Cross-validation is used to evaluate each hyperparameter setting's generalization performance.\n\nA common trap is tuning hyperparameters on the test set, which leaks test information into the model and produces overly optimistic performance estimates. The correct approach uses a separate validation set (or cross-validation on the training set) for hyperparameter tuning, reserving the test set for final evaluation only. In production, hyperparameter tuning is often automated with frameworks like Optuna or Ray Tune.",
    "category": "Fundamentals",
    "relatedTerms": [
      "learning-rate",
      "regularization",
      "overfitting"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Underfitting",
    "slug": "underfitting",
    "shortDef": "When a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and test sets.",
    "fullDef": "Underfitting occurs when a model has high bias -- it cannot represent the complexity of the true relationship between inputs and outputs. A classic example is fitting a straight line (degree-1 polynomial) to data that follows a sine wave: the model is fundamentally incapable of capturing the curved pattern, resulting in high error on both training and test data.\n\nUnderfitting is the opposite end of the bias-variance spectrum from overfitting. While overfitting shows low training error but high test error (the model memorized noise), underfitting shows high error everywhere (the model cannot even learn the signal). Signs of underfitting include training loss that plateaus at a high value, similar training and validation errors (both poor), and predictions that miss systematic patterns visible in the data.\n\nTo address underfitting, increase model complexity: add more features, use a more expressive model (e.g., switch from linear to polynomial, or increase neural network depth/width), reduce regularization strength, or train for more epochs. Feature engineering can also help by making patterns more accessible to the model. The key diagnostic is comparing training error to an acceptable baseline -- if training error is too high, the model is underfitting.",
    "category": "Fundamentals",
    "relatedTerms": [
      "overfitting",
      "bias-variance-tradeoff",
      "regularization"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Inference",
    "slug": "inference",
    "shortDef": "The process of using a trained model to make predictions on new, unseen data, as opposed to the training phase where the model learns from labeled examples.",
    "fullDef": "Inference is the deployment-time operation where a trained model processes new inputs to produce predictions. During training, the model learns parameters from labeled data through forward and backward passes; during inference, only the forward pass runs -- input goes in, prediction comes out, and no weight updates occur. This distinction matters for computational requirements, latency, and cost.\n\nInference has different performance characteristics than training. It typically requires less memory (no need to store intermediate activations for backpropagation), can run on less powerful hardware, and must meet latency constraints that training does not face. In production, inference latency is often the bottleneck: ad auctions require predictions in milliseconds, while training the same model may take hours or days.\n\nOptimizing inference is a major concern in production AI. Techniques include model quantization (reducing numerical precision from 32-bit to 8-bit), pruning (removing unimportant weights), knowledge distillation (training a smaller model to mimic a larger one), batching requests for GPU efficiency, and caching results for repeated queries. For LLMs specifically, KV caching avoids recomputing attention for previously processed tokens, dramatically reducing generation latency.",
    "category": "MLOps",
    "relatedTerms": [
      "neural-network",
      "large-language-model"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Self-Attention",
    "slug": "self-attention",
    "shortDef": "An attention mechanism where queries, keys, and values all come from the same input sequence, allowing each token to attend to every other token in the sequence including itself.",
    "fullDef": "Self-attention is the core operation in transformer architectures where each position in a sequence computes attention weights over all other positions in the same sequence. Given an input sequence X, self-attention projects it into queries Q = XW_Q, keys K = XW_K, and values V = XW_V, then computes: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V.\n\nEach output token is a weighted sum of all value vectors, where the weights are determined by the similarity between that token's query and every other token's key. This allows the model to route information from relevant context positions to each output position -- for example, when processing the ambiguous word 'bank', self-attention can look at surrounding words like 'river' or 'money' to determine the correct meaning.\n\nSelf-attention contrasts with cross-attention (where queries come from one sequence and keys/values from another, as in encoder-decoder models) and with masked self-attention (used in decoder-only models like GPT, where tokens can only attend to previous positions to prevent looking at future tokens during generation). Self-attention's ability to directly connect any two positions in a sequence, regardless of distance, is what gives transformers their advantage over RNNs for capturing long-range dependencies.",
    "category": "Deep Learning",
    "relatedTerms": [
      "attention-mechanism",
      "multi-head-attention",
      "transformer"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Multi-Head Attention",
    "slug": "multi-head-attention",
    "shortDef": "An extension of attention that runs multiple attention operations in parallel with different learned projections, allowing the model to capture different types of relationships simultaneously.",
    "fullDef": "Multi-head attention divides the attention computation into h parallel 'heads', each with its own learned projection matrices. Each head computes attention independently in a lower-dimensional subspace (dimension d_model/h), then the results are concatenated and projected back to the full dimension: MultiHead(Q, K, V) = Concat(head_1, ..., head_h) * W_O.\n\nThe key benefit is that different heads can learn to attend to different types of relationships in the data. In language models, researchers have observed that individual heads specialize: some track syntactic dependencies (subject-verb agreement), others capture semantic similarity, and others focus on local context (adjacent tokens). This specialization emerges naturally during training without explicit programming.\n\nMultationally, multi-head attention has the same computational cost as single-head attention with the same total dimension: O(n^2 * d). The parallelism across heads is efficiently handled by GPUs. Typical configurations use 8-16 heads (e.g., 8 heads with d_k = 64 for d_model = 512). Multi-head attention is a core component of every transformer layer, applied in self-attention (within a sequence), cross-attention (between encoder and decoder), and masked self-attention (for autoregressive generation).",
    "category": "Deep Learning",
    "relatedTerms": [
      "attention-mechanism",
      "self-attention",
      "transformer"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Positional Encoding",
    "slug": "positional-encoding",
    "shortDef": "A technique that injects information about token position into transformer inputs, since the attention mechanism itself is permutation-invariant and has no inherent notion of sequence order.",
    "fullDef": "Self-attention treats its input as a set, not a sequence -- swapping the order of tokens produces permuted but structurally equivalent attention weights. This means 'dog bites man' and 'man bites dog' would be indistinguishable without positional information. Positional encodings solve this by adding position-dependent vectors to token embeddings before they enter the transformer.\n\nThe original transformer uses sinusoidal positional encodings: PE(pos, 2i) = sin(pos / 10000^(2i/d_model)) and PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model)). This design is elegant: each position gets a unique encoding, relative positions can be computed as linear transformations (rotation matrices), different dimensions encode position at different frequency scales (like a clock's second/minute/hour hands), and it requires zero learnable parameters. Alternatively, many modern models (BERT, GPT) use learned positional embeddings.\n\nModern architectures increasingly use relative positional encodings. RoPE (Rotary Positional Embedding), used in LLaMA and PaLM, applies rotation matrices to queries and keys so that attention scores depend only on relative position. ALiBi adds a linear distance penalty to attention scores. These approaches generalize better to sequence lengths not seen during training, which is critical for deploying LLMs on variable-length inputs.",
    "category": "NLP",
    "relatedTerms": [
      "transformer",
      "self-attention",
      "embedding"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Feed-Forward Network",
    "slug": "feed-forward-network",
    "shortDef": "A simple neural network layer within each transformer block that independently transforms each token's representation through two linear transformations with a non-linear activation in between.",
    "fullDef": "In transformer architectures, the feed-forward network (FFN) is applied independently to each token position after the attention sub-layer. It typically consists of two linear transformations with a non-linear activation: FFN(x) = W_2 * activation(W_1 * x + b_1) + b_2. The inner dimension is usually 4x the model dimension (e.g., 2048 for d_model=512).\n\nThe FFN serves a complementary role to attention. While attention mixes information across token positions (inter-token processing), the FFN processes each token's representation independently (intra-token processing). Research suggests that FFN layers act as key-value memories, storing factual knowledge learned during training. The expansion to a larger inner dimension followed by compression back creates a bottleneck that forces the network to learn efficient representations.\n\nModern variants replace the standard two-layer FFN with gated architectures like SwiGLU (used in LLaMA, PaLM) which use a gating mechanism for better gradient flow. Each transformer block alternates between attention (communication between positions) and FFN (computation within each position), with residual connections and layer normalization around both sub-layers.",
    "category": "Deep Learning",
    "relatedTerms": [
      "transformer",
      "residual-connection",
      "neural-network"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Temperature",
    "slug": "temperature",
    "shortDef": "A parameter that controls the randomness of token sampling during LLM text generation by scaling the logits before applying softmax.",
    "fullDef": "Temperature is a scalar that divides the model's output logits before the softmax function: softmax(x/tau). It controls how peaked or flat the resulting probability distribution is, directly affecting the diversity and creativity of generated text.\n\nAt low temperature (tau approaching 0), softmax becomes nearly one-hot -- almost all probability mass goes to the highest-scoring token, making generation deterministic and repetitive. At temperature 1.0, the original model distribution is used. At high temperature (tau > 1), the distribution becomes flatter and more uniform, making less probable tokens more likely to be sampled, which increases creativity but also incoherence and potential for nonsensical output.\n\nIn practice, temperature is one of the most important generation-time parameters. Customer-facing chatbots typically use low temperatures (0.1-0.3) for consistent, factual responses. Creative writing applications use higher temperatures (0.7-1.0). Values above 1.5 generally produce text that is too random to be useful. Temperature interacts with other sampling parameters like top-k and top-p (nucleus sampling) to give fine-grained control over the generation quality-diversity tradeoff.",
    "category": "NLP",
    "relatedTerms": [
      "softmax",
      "large-language-model",
      "inference"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Hallucination",
    "slug": "hallucination",
    "shortDef": "When an AI model generates plausible-sounding but factually incorrect or fabricated information with apparent confidence.",
    "fullDef": "Hallucination occurs when large language models produce outputs that are fluent and confident but factually wrong. The model might fabricate citations, invent historical events, or provide detailed but entirely fictional instructions. This happens because LLMs are trained to predict statistically likely next tokens, not to determine truth -- they optimize for fluency and plausibility, not accuracy.\n\nSeveral factors cause hallucinations. LLMs have no grounding in truth; their training data contains facts, opinions, fiction, and errors in equal measure. Maximum likelihood training means the model learns to produce plausible text, not truthful text. The model also overgeneralizes: having learned patterns like 'X is the capital of Y', it can confidently generate fictional capitals for fictional countries. Critically, LLMs have no mechanism to express uncertainty -- they will produce the most likely token even when all options are unlikely.\n\nMitigations include Retrieval-Augmented Generation (RAG), which grounds responses in retrieved documents; instruction tuning to teach models to say 'I don't know'; RLHF to penalize false statements; and chain-of-thought prompting to surface reasoning steps. However, no complete fix exists because the fundamental architecture is a pattern matcher, not a truth engine. Production systems must always include verification, fallback logic, and appropriate disclaimers.",
    "category": "NLP",
    "relatedTerms": [
      "large-language-model",
      "retrieval-augmented-generation",
      "prompt-engineering"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Retrieval-Augmented Generation",
    "slug": "retrieval-augmented-generation",
    "shortDef": "An architecture pattern that reduces LLM hallucination by retrieving relevant documents from an external knowledge base and including them as context before generating a response.",
    "fullDef": "Retrieval-Augmented Generation (RAG) addresses the fundamental limitation that LLMs can only rely on knowledge memorized during training. Instead of asking the model to answer from memory, RAG first retrieves relevant documents from an external database, then augments the prompt with this retrieved context, and finally generates an answer grounded in the retrieved information.\n\nA typical RAG pipeline consists of: a document store (vector database or search index) containing the knowledge base, an embedding model that converts both queries and documents into vectors, a retrieval step that finds the top-k most similar documents using cosine similarity, and an LLM that generates answers given the query plus retrieved context. This architecture is powerful because knowledge can be updated by simply adding documents to the store, without retraining the model.\n\nRAG is preferred over fine-tuning when knowledge changes frequently, when you need to cite sources, or when compute resources are limited. However, RAG does not eliminate hallucination entirely -- if retrieval fails to find relevant documents, the LLM may still generate fabricated answers. Production RAG systems need fallback logic, relevance thresholds, and monitoring. Often the best approach combines RAG (for up-to-date factual grounding) with fine-tuning (for domain-specific style and reasoning).",
    "category": "NLP",
    "relatedTerms": [
      "large-language-model",
      "embedding",
      "hallucination",
      "fine-tuning"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "AI Agent",
    "slug": "ai-agent",
    "shortDef": "An LLM-based system that can autonomously plan multi-step tasks, use external tools, and take actions in the real world to achieve specified goals.",
    "fullDef": "An AI agent extends a language model beyond text generation by giving it the ability to use tools (search engines, calculators, APIs, databases), plan multi-step task sequences, observe results of its actions, and iterate until a goal is achieved. The basic agent loop is: observe the current state, reason about what action to take, execute the action, evaluate the result, and repeat until done.\n\nAgents are powerful for tasks that require multiple steps and external information -- for example, a research agent that searches the web, synthesizes findings, and produces a report. However, they amplify LLM limitations: errors compound across steps, agents can enter infinite loops, each step incurs API costs, and emergent multi-step behavior is extremely difficult to test. In production, an agent that called the wrong API or misinterpreted search results caused thousands of incorrect refund transactions.\n\nFor these reasons, production agents typically operate in constrained domains with limited action spaces, require human-in-the-loop confirmation for irreversible actions, include robust input validation and fallback logic, and log everything for debugging. The key lesson is that agents are semi-autonomous tools requiring careful guardrails, not fully autonomous replacements for human judgment.",
    "category": "Agents",
    "relatedTerms": [
      "large-language-model",
      "retrieval-augmented-generation",
      "prompt-engineering"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Prompt Engineering",
    "slug": "prompt-engineering",
    "shortDef": "The practice of carefully crafting input text to elicit desired behavior from large language models, including techniques like few-shot examples, chain-of-thought reasoning, and system instructions.",
    "fullDef": "Prompt engineering is the art of designing inputs that guide LLMs toward producing useful, accurate, and appropriately formatted outputs. Since LLMs are next-token predictors, the way a question or instruction is phrased dramatically affects the quality of the response. Small changes in wording can cause large changes in output, making prompt design both powerful and brittle.\n\nKey techniques include zero-shot prompting (direct instructions), few-shot prompting (providing examples of desired input-output pairs), chain-of-thought (asking the model to show its reasoning step-by-step), and system prompts (setting context and behavioral guidelines). More advanced approaches include retrieval-augmented prompts (injecting relevant context), structured output instructions (requesting JSON or specific formats), and prompt chaining (breaking complex tasks into sequential simpler prompts).\n\nWhile prompt engineering has become a critical skill for working with LLMs, it has limitations. Prompting is inherently brittle -- the same prompt can produce different results across model versions or even across runs. For production systems, prompts should be versioned like code, tested against regression suites, and monitored for output quality. When consistent, domain-specific behavior is needed, fine-tuning often provides more reliable results than prompt engineering alone.",
    "category": "NLP",
    "relatedTerms": [
      "large-language-model",
      "hallucination",
      "fine-tuning"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Vanishing Gradients",
    "slug": "vanishing-gradients",
    "shortDef": "A training problem where gradients become exponentially smaller as they propagate backward through many layers, effectively preventing early layers from learning.",
    "fullDef": "Vanishing gradients occur when the gradient signal diminishes as it is backpropagated through many layers of a deep network. Since backpropagation multiplies gradients at each layer via the chain rule, if each layer's gradient contribution is less than 1, the overall gradient decays exponentially: after L layers with gradient factor 0.9, the signal is 0.9^L. For 50 layers, this gives 0.9^50 = 0.005 -- the first layers receive essentially zero learning signal.\n\nThis problem was historically most severe with sigmoid and tanh activations, which saturate (gradient approaches zero) for large inputs. In deep recurrent networks processing long sequences, the same weight matrix is applied at each time step, making vanishing gradients particularly acute -- information from early tokens cannot influence later computations. This is why RNNs struggle with long-range dependencies.\n\nMultiple solutions have been developed: ReLU activation (gradient is 1 for positive inputs, never saturates), proper weight initialization (Xavier, He), residual connections (providing a gradient highway that bypasses learned layers), LSTM/GRU architectures (with gating mechanisms that control information flow), and normalization techniques (batch norm, layer norm) that keep activations in well-behaved ranges. These innovations collectively enabled training of networks with hundreds of layers.",
    "category": "Deep Learning",
    "relatedTerms": [
      "exploding-gradients",
      "backpropagation",
      "residual-connection",
      "relu"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Exploding Gradients",
    "slug": "exploding-gradients",
    "shortDef": "A training problem where gradients grow exponentially large as they propagate backward through many layers, causing weight updates to be enormous and training to diverge.",
    "fullDef": "Exploding gradients are the opposite of vanishing gradients: when gradient factors at each layer are greater than 1, the backpropagated signal grows exponentially. After L layers with gradient factor 1.1, the gradient magnitude becomes 1.1^L. For 50 layers, this is 1.1^50 = 117 -- gradients become enormous, causing massive weight updates that destabilize training. Loss values spike to infinity or become NaN.\n\nThis problem commonly manifests in deep recurrent networks and very deep feedforward networks without proper initialization or normalization. A team training a 50-layer RNN experienced loss going to NaN within 10 iterations due to gradient explosion. The symptoms are unmistakable: loss suddenly jumps to very large values or NaN, weights become extremely large, and training completely breaks down.\n\nThe primary solution is gradient clipping: capping the maximum gradient magnitude to a threshold (e.g., clip gradients to norm 1.0). Other solutions include proper weight initialization (Xavier, He), normalization layers (batch norm, layer norm), residual connections, and using LSTM/GRU architectures for sequential data. Gradient clipping is a simple but essential technique: `if ||gradient|| > threshold: gradient = gradient * threshold / ||gradient||`. Modern deep learning frameworks apply gradient clipping as a standard training practice.",
    "category": "Deep Learning",
    "relatedTerms": [
      "vanishing-gradients",
      "backpropagation",
      "gradient-clipping"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Gradient Clipping",
    "slug": "gradient-clipping",
    "shortDef": "A technique that caps gradient magnitudes during training to prevent exploding gradients from destabilizing the optimization process.",
    "fullDef": "Gradient clipping limits the size of gradients during backpropagation by rescaling them when their norm exceeds a specified threshold. The most common approach, gradient norm clipping, works as follows: compute the total gradient norm, and if it exceeds the threshold, scale all gradients down proportionally so the total norm equals the threshold. This preserves gradient direction while limiting magnitude.\n\nThe technique directly addresses the exploding gradient problem that occurs in deep networks and recurrent architectures. Without clipping, a single batch with unusually large gradients can destroy hours of training progress by making enormous weight updates. Gradient clipping acts as a safety mechanism: normal gradient steps proceed unchanged, but catastrophically large gradients are reined in before they can cause damage.\n\nIn practice, gradient clipping is nearly universal in training recurrent networks and transformers. Common threshold values range from 0.5 to 5.0, with 1.0 being a frequent default. It is typically applied after computing gradients but before the optimizer step. While gradient clipping does not solve the root cause of unstable gradients (architecture or initialization issues), it provides robust protection against training divergence and is a standard component of any deep learning training pipeline.",
    "category": "Optimization",
    "relatedTerms": [
      "exploding-gradients",
      "backpropagation",
      "gradient-descent"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Data Augmentation",
    "slug": "data-augmentation",
    "shortDef": "A regularization technique that artificially expands the training dataset by applying label-preserving transformations to existing examples, forcing the model to learn invariances.",
    "fullDef": "Data augmentation creates modified versions of training examples by applying transformations that change the input but not the label. For images, this includes random cropping, flipping, rotation, color jittering, and scaling. For text, techniques include synonym replacement, back-translation, and random insertion/deletion. For audio, time stretching and pitch shifting are common.\n\nThe technique works as a form of regularization by encoding prior knowledge about invariances. Flipping an image of a cat horizontally still shows a cat -- by training on both the original and flipped versions, the model learns horizontal flip invariance without needing twice as many unique examples. This is particularly valuable when labeled data is expensive or limited, which is common in medical imaging, satellite imagery, and specialized domains.\n\nMore advanced augmentation methods include mixup (blending two training images and their labels), cutout (masking random patches), and automated augmentation search (learning optimal augmentation policies). Data augmentation is one of the most reliably effective techniques for improving model generalization in computer vision, often providing larger gains than architectural changes. It directly addresses the fundamental problem that models need to see more variation than real datasets provide.",
    "category": "Fundamentals",
    "relatedTerms": [
      "regularization",
      "overfitting",
      "machine-learning"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Latent Space",
    "slug": "latent-space",
    "shortDef": "A lower-dimensional representation space learned by a model where similar inputs are mapped to nearby points, capturing the essential structure of the data.",
    "fullDef": "A latent space is the internal representation space that a model uses to encode input data. When a neural network processes an image of a face, the intermediate layers transform the raw pixel values into a compact vector in latent space that captures essential attributes like pose, expression, lighting, and identity. Similar faces map to nearby points in this space, while dissimilar faces are far apart.\n\nLatent spaces are central to many AI architectures. Autoencoders compress inputs into a latent bottleneck and reconstruct them, learning efficient representations. Variational Autoencoders (VAEs) impose structure on the latent space (typically a Gaussian distribution), enabling smooth interpolation and generation of new examples. Generative Adversarial Networks learn latent spaces where different directions correspond to interpretable attributes (e.g., adding glasses, changing hair color).\n\nThe power of latent spaces is that they capture the intrinsic dimensionality of data, which is typically much lower than the raw input dimension. A 256x256 RGB image has 196,608 dimensions, but the space of meaningful face images can be represented in a few hundred dimensions. Word embeddings are another example: the latent space of word vectors captures semantic relationships (king - man + woman = queen), enabling arithmetic on meaning.",
    "category": "Deep Learning",
    "relatedTerms": [
      "embedding",
      "generative-adversarial-network",
      "deep-learning"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "KL Divergence",
    "slug": "kl-divergence",
    "shortDef": "A measure of how one probability distribution differs from a reference distribution, quantifying the information lost when approximating one distribution with another.",
    "fullDef": "Kullback-Leibler (KL) divergence D_KL(p || q) = sum(p(x) * log(p(x)/q(x))) measures the extra bits needed to encode data from distribution p using a code optimized for distribution q. It is always non-negative and equals zero only when the two distributions are identical. Importantly, it is not symmetric: D_KL(p || q) is not equal to D_KL(q || p).\n\nKL divergence connects to cross-entropy through the identity: H(p, q) = H(p) + D_KL(p || q). Since entropy H(p) of the true distribution is constant during training, minimizing cross-entropy loss is equivalent to minimizing KL divergence -- making the model's predictions match reality. This gives a principled information-theoretic justification for why cross-entropy is the standard classification loss.\n\nKL divergence appears throughout machine learning: as a regularization term in Variational Autoencoders (encouraging the latent distribution to match a prior), in policy optimization for reinforcement learning (constraining how much a policy changes between updates), in knowledge distillation (measuring how well a student model mimics a teacher), and in drift detection (measuring how much input distributions have shifted from the training distribution).",
    "category": "Fundamentals",
    "relatedTerms": [
      "cross-entropy",
      "loss-function",
      "regularization"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Knowledge Graph",
    "slug": "knowledge-graph",
    "shortDef": "A structured representation of knowledge as entities (nodes) and relationships (edges), often with properties attached to both, enabling logical traversal and multi-hop reasoning over data.",
    "fullDef": "A knowledge graph is a graph-structured database where knowledge is stored as entities (nodes) and relationships (edges), with properties attached to both. Unlike relational databases that store data in tables, knowledge graphs model information as interconnected networks, making it natural to represent and query complex relationships between concepts.\n\nKnowledge graphs excel at multi-hop reasoning -- answering questions like \"find friends-of-friends who work at competitors\" -- which would require expensive self-joins in SQL. They are used by major companies including Google (Google Knowledge Graph), Microsoft (GraphRAG), and Amazon (product knowledge graphs). In hybrid RAG+KG systems, the knowledge graph provides structured facts and relationship traversal while RAG provides unstructured document retrieval.\n\nCommon implementations use graph databases like Neo4j with query languages such as Cypher or SPARQL. Knowledge graphs are built from triples (subject-predicate-object), and their construction involves entity extraction, relation extraction, entity linking, and schema design.",
    "category": "Knowledge Graphs",
    "relatedTerms": [
      "rag",
      "neo4j",
      "triple",
      "ontology",
      "graph-traversal",
      "entity-linking",
      "cypher"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Vector Database",
    "slug": "vector-database",
    "shortDef": "A specialized database optimized for storing and querying high-dimensional vector embeddings, supporting efficient similarity search operations.",
    "fullDef": "A vector database is a purpose-built database designed to store, index, and search high-dimensional vector embeddings efficiently. Unlike traditional databases optimized for exact lookups, vector databases excel at finding the most similar vectors to a given query vector using distance metrics like cosine similarity or Euclidean distance.\n\nVector databases are a core component of RAG systems, where document chunks are converted to embeddings and stored for later retrieval. Key operations include inserting vectors with metadata, performing k-nearest neighbor (kNN) search, and combining vector search with metadata filtering. Popular implementations include FAISS (local, developed by Meta), ChromaDB (simple prototyping), Pinecone (managed production service), and Weaviate (open-source).\n\nAt scale, vector databases use approximate nearest neighbor (ANN) algorithms like HNSW or IVF to trade small amounts of accuracy for significant speed improvements, enabling sub-second search over billions of vectors.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "embedding",
      "cosine-similarity",
      "faiss",
      "semantic-search",
      "dense-retrieval",
      "approximate-nearest-neighbor"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Cosine Similarity",
    "slug": "cosine-similarity",
    "shortDef": "A measure of similarity between two vectors based on the cosine of the angle between them, ranging from -1 (opposite) to 1 (identical direction), widely used to compare text embeddings.",
    "fullDef": "Cosine similarity measures how similar two vectors are by computing the cosine of the angle between them. The formula is cos(theta) = (A dot B) / (||A|| * ||B||), producing values between -1 and 1, where 1 means identical direction, 0 means orthogonal (unrelated), and -1 means opposite direction.\n\nCosine similarity is the preferred metric for comparing text embeddings because it is scale-invariant -- it measures the direction of vectors rather than their magnitude. This is critical for text because a long document and a short summary may have very different vector magnitudes but similar semantic content. Cosine similarity correctly identifies them as similar by ignoring length differences.\n\nIn practice, many vector databases pre-normalize embeddings to unit length, which allows cosine similarity to be computed as a simple dot product for maximum speed. When choosing similarity thresholds for RAG systems, scores above 0.9 indicate very similar content, 0.7-0.9 indicates same topic with different phrasing, and below 0.5 is typically not relevant.",
    "category": "Fundamentals",
    "relatedTerms": [
      "embedding",
      "vector-database",
      "semantic-search",
      "dense-retrieval"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Semantic Search",
    "slug": "semantic-search",
    "shortDef": "A search technique that finds results based on the meaning of a query rather than exact keyword matches, typically using vector embeddings and similarity metrics.",
    "fullDef": "Semantic search goes beyond traditional keyword matching to understand the intent and meaning behind a search query. Instead of looking for exact word overlap, semantic search converts both queries and documents into vector embeddings and finds documents whose embeddings are closest to the query embedding in vector space.\n\nThis approach solves a fundamental limitation of keyword search: it can match \"automobile\" with \"car\" or \"myocardial infarction\" with \"heart attack\" because semantically similar concepts are mapped to nearby points in embedding space. Semantic search is powered by embedding models like BERT, text-embedding-3, or BGE that have been trained to capture semantic relationships.\n\nWhile powerful for meaning-based retrieval, semantic search has limitations -- it can miss exact matches for product IDs, error codes, or rare technical terms where keyword-based methods like BM25 excel. This is why production systems typically combine semantic search with keyword search in hybrid retrieval approaches.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "embedding",
      "cosine-similarity",
      "dense-retrieval",
      "hybrid-search",
      "bm25",
      "vector-database"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "BM25",
    "slug": "bm25",
    "shortDef": "A ranking function used in information retrieval that estimates document relevance based on term frequency with diminishing returns and document length normalization.",
    "fullDef": "BM25 (Best Matching 25) is a probabilistic ranking function that improves upon TF-IDF by incorporating two key innovations: term frequency saturation (mentioning a word 100 times does not make a document 100 times more relevant) and document length normalization (longer documents naturally have higher term frequencies and should not be unfairly favored).\n\nThe BM25 formula combines IDF weighting with a saturation function controlled by parameter k1 (typically 1.2-2.0) and a length normalization parameter b (typically 0.75). As term frequency increases, the score approaches an asymptote rather than growing linearly, which models the intuition of diminishing returns.\n\nDespite the rise of neural retrieval methods, BM25 remains widely used in production systems because it excels at exact term matching (product IDs, error codes, technical jargon), requires no GPU, is highly interpretable, and is extremely fast. In modern RAG systems, BM25 is commonly combined with dense retrieval in hybrid search configurations using techniques like reciprocal rank fusion.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "tf-idf",
      "sparse-retrieval",
      "hybrid-search",
      "reciprocal-rank-fusion",
      "semantic-search"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Chunking",
    "slug": "chunking",
    "shortDef": "The process of dividing large documents into smaller, semantically coherent pieces suitable for embedding and retrieval in RAG systems.",
    "fullDef": "Chunking is the process of splitting documents into smaller segments (chunks) that can be individually embedded and retrieved. It is one of the most critical steps in a RAG pipeline because embedding models have token limits, LLM context windows require concise relevant content, and large chunks spanning multiple topics produce poor similarity scores.\n\nCommon chunking strategies include fixed-size chunking (split every N tokens with overlap), sentence-based chunking, structural chunking (split by headers and sections), semantic chunking (split when consecutive sentence similarity drops), and hierarchical chunking (create chunks at multiple granularities). Advanced approaches include entity-aware chunking that never splits named entities across boundaries and code-aware chunking that keeps code blocks intact.\n\nThe sweet spot for most use cases is 200-800 tokens per chunk with 10-20% overlap between consecutive chunks. Too-small chunks lack context and produce poor embeddings, while too-large chunks mix topics and dilute similarity scores. Chunk overlap prevents information loss at boundaries where important context might be split across adjacent chunks.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "rag",
      "embedding",
      "context-window",
      "retrieval-pipeline"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Reranking",
    "slug": "reranking",
    "shortDef": "A second-stage ranking process that reorders initially retrieved results using a more computationally expensive but accurate model, typically a cross-encoder.",
    "fullDef": "Reranking is a technique where an initial set of retrieved candidates (often 50-100 documents) is rescored and reordered by a more sophisticated model to improve precision in the final top results. The initial retrieval stage optimizes for speed and recall using approximate methods, while the reranker optimizes for precision using slower but more accurate scoring.\n\nThe most common reranking approach uses cross-encoder models (such as ms-marco-MiniLM) that jointly encode the query and each candidate document together, allowing word-by-word attention between them. This produces much more accurate relevance scores than bi-encoder approaches that embed query and document independently.\n\nReranking is particularly valuable in RAG systems where the quality of the final top-5 documents directly impacts answer quality. A typical pipeline retrieves 100 candidates using fast vector search, reranks them with a cross-encoder, and passes only the top 5 to the LLM. Studies show reranking can improve answer quality by 20% or more, making it essential for production RAG systems rather than an optional optimization.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "cross-encoder",
      "bi-encoder",
      "retrieval-pipeline",
      "hybrid-search",
      "dense-retrieval"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Hybrid Search",
    "slug": "hybrid-search",
    "shortDef": "A retrieval approach that combines different search methods, typically keyword-based (BM25) and semantic (dense embedding) search, to leverage the strengths of both.",
    "fullDef": "Hybrid search combines multiple retrieval methods -- most commonly sparse keyword search (BM25) and dense semantic search (embedding similarity) -- to produce better results than either method alone. The key insight is that these methods have complementary strengths: BM25 excels at exact term matching while dense retrieval excels at semantic understanding.\n\nIn practice, hybrid search runs both BM25 and dense retrieval in parallel, then combines their results using fusion techniques such as reciprocal rank fusion (RRF) or learned weighted combination. A common formula weights dense scores higher (e.g., 0.7 semantic + 0.3 BM25), though optimal weights depend on the domain and query types.\n\nHybrid search is the recommended approach for production RAG systems because it handles the full spectrum of user queries -- from exact-match lookups (\"invoice #12345\") to meaning-based searches (\"documents about corporate restructuring\"). Systems that rely solely on semantic search often fail on rare terms, product codes, and proper nouns where BM25 provides critical coverage.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "bm25",
      "semantic-search",
      "dense-retrieval",
      "sparse-retrieval",
      "reciprocal-rank-fusion",
      "reranking"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Graph Traversal",
    "slug": "graph-traversal",
    "shortDef": "The process of systematically visiting nodes in a graph by following edges, used in knowledge graphs to explore relationships and answer multi-hop queries.",
    "fullDef": "Graph traversal is the process of visiting nodes in a graph data structure by following edges in a systematic way. The two fundamental traversal algorithms are breadth-first search (BFS), which explores all neighbors at the current depth before moving deeper, and depth-first search (DFS), which explores as far as possible along each branch before backtracking.\n\nIn knowledge graph systems, graph traversal is used to answer multi-hop queries -- questions that require following multiple relationships. For example, answering \"What skills do Alice's teammates have?\" requires traversing Alice -> team -> teammates -> skills, a multi-hop path through the graph. Cypher queries in Neo4j express traversals naturally using variable-length path patterns like `(a)-[*1..3]-(b)` for paths of 1 to 3 hops.\n\nGraph traversal is a key differentiator of knowledge graphs over vector databases. While vector search finds semantically similar documents, graph traversal follows explicit structured relationships, enabling precise reasoning over entity connections that would be impossible with similarity search alone.",
    "category": "Knowledge Graphs",
    "relatedTerms": [
      "knowledge-graph",
      "neo4j",
      "cypher",
      "multi-hop-reasoning"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Named Entity Recognition",
    "slug": "named-entity-recognition",
    "shortDef": "An NLP task that identifies and classifies named entities such as people, organizations, locations, and dates in unstructured text.",
    "fullDef": "Named Entity Recognition (NER) is a fundamental natural language processing task that automatically identifies and classifies mentions of named entities in text into predefined categories such as Person, Organization, Location, Date, Product, and others. For example, in the sentence \"Alice Smith works at Acme Corp in San Francisco,\" NER identifies \"Alice Smith\" as a Person, \"Acme Corp\" as an Organization, and \"San Francisco\" as a Location.\n\nNER is a critical component of knowledge graph construction pipelines. Before you can build a graph of entities and relationships, you must first identify which entities exist in your text. Common NER tools include spaCy (fast, rule-based plus statistical), Hugging Face transformer models (higher accuracy), and LLM-based extraction (most flexible but slower and more expensive).\n\nIn RAG+KG systems, NER serves multiple roles: extracting entities from source documents during knowledge graph construction, identifying entities in user queries for entity-centric retrieval, and supporting entity-aware chunking that preserves named entities within chunk boundaries.",
    "category": "NLP",
    "relatedTerms": [
      "relation-extraction",
      "entity-linking",
      "knowledge-graph",
      "triple"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Relation Extraction",
    "slug": "relation-extraction",
    "shortDef": "The NLP task of identifying and classifying semantic relationships between entities mentioned in text, a key step in knowledge graph construction.",
    "fullDef": "Relation extraction is the task of automatically identifying relationships between entities in unstructured text. Given a sentence like \"Alice Smith manages the data team at Acme Corp,\" relation extraction identifies the relationships (Alice Smith, MANAGES, data team) and (Alice Smith, WORKS_AT, Acme Corp).\n\nRelation extraction is essential for building knowledge graphs from text. There are three main approaches: rule-based extraction using dependency parsing (fast but limited, ~60% accuracy), supervised machine learning models trained on labeled relationship data, and LLM-based extraction using prompts that instruct the model to output structured triples (most flexible, 85-90% accuracy but slower and more expensive).\n\nIn production knowledge graph pipelines, relation extraction typically works in concert with named entity recognition (NER) and entity linking. First NER identifies entities, then relation extraction identifies how they connect, and finally entity linking resolves different mentions to canonical entities. The quality of relation extraction directly determines the quality and completeness of the resulting knowledge graph.",
    "category": "NLP",
    "relatedTerms": [
      "named-entity-recognition",
      "triple",
      "knowledge-graph",
      "entity-linking"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Ontology",
    "slug": "ontology",
    "shortDef": "A formal specification of concepts, categories, and relationships within a domain that defines what types of entities exist and how they can relate to each other.",
    "fullDef": "An ontology is a formal, explicit specification of the concepts and relationships that can exist within a particular domain of knowledge. It defines the vocabulary of entity types (Person, Company, Product), relationship types (WORKS_FOR, OWNS, PRODUCES), and constraints (a Person can WORKS_FOR a Company, but not vice versa) that structure a knowledge graph.\n\nOntologies serve as the schema or blueprint for knowledge graphs, similar to how a database schema defines tables and foreign keys in relational databases. However, ontologies are typically richer and more expressive, supporting inheritance hierarchies (Employee is a subtype of Person), cardinality constraints (a Person can work for multiple Companies), and domain-range restrictions (the MANAGES relationship connects Person to Person).\n\nIn the semantic web ecosystem, ontologies are expressed using standards like OWL (Web Ontology Language) and queried with SPARQL. In property graph databases like Neo4j, ontologies are more informal but equally important for maintaining data quality. A well-designed ontology ensures that knowledge graph construction produces consistent, queryable, and maintainable graphs.",
    "category": "Knowledge Graphs",
    "relatedTerms": [
      "knowledge-graph",
      "triple",
      "rdf",
      "sparql",
      "schema"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Triple",
    "slug": "triple",
    "shortDef": "The fundamental unit of knowledge in a graph, expressed as a (subject, predicate, object) statement such as (Alice, WORKS_AT, Acme Corp).",
    "fullDef": "A triple is the atomic unit of knowledge representation in knowledge graphs, consisting of three components: a subject (the entity being described), a predicate (the relationship or property), and an object (the related entity or value). For example, (Alice Smith, WORKS_AT, Acme Corp) states that Alice Smith works at Acme Corp.\n\nTriples are the foundation of the Resource Description Framework (RDF), the W3C standard for representing information on the semantic web. In property graph databases like Neo4j, the same concept is expressed using node-relationship-node patterns. Triple extraction from unstructured text is a key step in knowledge graph construction, typically performed using NER followed by relation extraction.\n\nTriples can represent both relationships between entities (Alice, MANAGES, Bob) and properties of entities (Alice, AGE, 30). Collections of triples form knowledge graphs, where subjects and objects become nodes and predicates become edges. The simplicity and composability of triples makes them a powerful primitive for building arbitrarily complex knowledge structures.",
    "category": "Knowledge Graphs",
    "relatedTerms": [
      "knowledge-graph",
      "rdf",
      "ontology",
      "relation-extraction",
      "named-entity-recognition"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Neo4j",
    "slug": "neo4j",
    "shortDef": "The most widely used graph database in industry, designed for storing and querying property graphs using the Cypher query language.",
    "fullDef": "Neo4j is a native graph database that stores data as nodes, relationships, and properties in a property graph model. It is the most popular graph database for knowledge graph applications, offering mature tooling, excellent query performance for graph traversals, and the intuitive Cypher query language.\n\nNeo4j's key advantage is index-free adjacency -- each node directly references its neighbors, making traversal operations constant-time per hop regardless of total graph size. This makes multi-hop queries like \"find all skills within 3 hops of Alice\" extremely fast even with millions of nodes. Neo4j also includes the Graph Data Science (GDS) library with built-in algorithms for PageRank, community detection, shortest path, and other graph analytics.\n\nIn RAG+KG architectures, Neo4j serves as the structured knowledge store alongside a vector database for unstructured content. Common patterns include using LLMs to generate Cypher queries from natural language (text-to-Cypher), expanding retrieval context through graph neighborhood exploration, and providing explainable reasoning paths through graph traversals. Neo4j can be deployed locally via Docker or as a managed cloud service.",
    "category": "Knowledge Graphs",
    "relatedTerms": [
      "knowledge-graph",
      "cypher",
      "graph-traversal",
      "property-graph"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "FAISS",
    "slug": "faiss",
    "shortDef": "Facebook AI Similarity Search -- an open-source library by Meta for efficient similarity search and clustering of dense vectors, optimized for billion-scale datasets.",
    "fullDef": "FAISS (Facebook AI Similarity Search) is a library developed by Meta Research for efficient similarity search and clustering of high-dimensional dense vectors. It provides highly optimized implementations of nearest neighbor search algorithms that can handle datasets ranging from thousands to billions of vectors.\n\nFAISS offers multiple index types for different scale and accuracy tradeoffs. IndexFlatL2 provides exact search (brute force) suitable for small datasets. IndexIVFFlat partitions the vector space into clusters for faster approximate search at moderate scale. IndexHNSW implements the Hierarchical Navigable Small World algorithm for the best speed-accuracy tradeoff at large scale. FAISS also supports product quantization to compress vectors and reduce memory usage.\n\nIn RAG systems, FAISS is commonly used for local development and self-hosted production deployments where you need fast vector search without a managed service. It runs entirely in-process (no separate server), supports both CPU and GPU execution, and can be combined with metadata filtering for more precise retrieval. For managed production deployments, teams often migrate to services like Pinecone or Weaviate while keeping FAISS for development.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "vector-database",
      "approximate-nearest-neighbor",
      "embedding",
      "cosine-similarity"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Retrieval Pipeline",
    "slug": "retrieval-pipeline",
    "shortDef": "The end-to-end sequence of steps in a RAG system: query processing, document retrieval, reranking, context construction, and LLM generation.",
    "fullDef": "A retrieval pipeline is the complete sequence of steps that transforms a user query into a grounded, cited answer in a RAG system. The standard pipeline consists of: (1) query processing (rewriting, expansion, decomposition), (2) retrieval (searching for relevant document chunks using vector similarity, keyword matching, or both), (3) reranking (rescoring candidates with a cross-encoder for better precision), (4) context construction (formatting retrieved documents for the LLM), and (5) generation (producing the final answer with citations).\n\nEach stage in the pipeline involves design decisions with significant quality implications. Query rewriting can improve recall by 15% or more. Hybrid retrieval combining BM25 and dense search outperforms either alone. Reranking improves precision of the final top results. Context construction must balance providing enough information against overwhelming the LLM with noise.\n\nIn hybrid RAG+KG systems, the pipeline is extended with knowledge graph components: entity extraction from the query, graph traversal for structured facts, and context fusion that combines graph results with document results before generation. Production pipelines also include caching, cost optimization, monitoring, and evaluation metrics like faithfulness, relevancy, and groundedness.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "rag",
      "chunking",
      "reranking",
      "hybrid-search",
      "context-window",
      "grounding"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Context Window",
    "slug": "context-window",
    "shortDef": "The maximum number of tokens a language model can process at once, which limits how much retrieved content can be included alongside a query.",
    "fullDef": "The context window is the maximum number of tokens that a language model can accept as input in a single request. This is a hard architectural limit determined by the model's positional encoding scheme and training configuration. Examples include 2,048 tokens for GPT-3, 128K tokens for GPT-4, and 200K tokens for Claude 3.\n\nThe context window is a critical constraint in RAG system design because it limits how much retrieved content can be provided to the LLM alongside the user query and system instructions. If you retrieve 20 chunks of 500 tokens each, that is 10,000 tokens of context before accounting for the prompt template and desired output length. Larger context windows allow more retrieved content but increase cost (API pricing scales with tokens) and can degrade quality due to the \"lost in the middle\" problem where LLMs pay less attention to information in the middle of long contexts.\n\nContext window optimization involves balancing retrieval breadth (more documents for better recall) against context efficiency (fewer, more relevant documents for better precision and lower cost). Most RAG systems find a sweet spot of 5-20 retrieved chunks, with reranking ensuring only the most relevant content occupies the limited context space.",
    "category": "NLP",
    "relatedTerms": [
      "rag",
      "retrieval-pipeline",
      "chunking",
      "tokenization"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Grounding",
    "slug": "grounding",
    "shortDef": "The technique of anchoring LLM responses in factual, retrieved information rather than the model's parametric knowledge, reducing hallucinations.",
    "fullDef": "Grounding is the practice of constraining a language model's responses to be based on specific retrieved evidence rather than the model's internal (parametric) knowledge. In a RAG system, grounding means instructing the LLM to answer only using the provided context documents and to explicitly state when the answer cannot be found in the available evidence.\n\nGrounding is the primary mechanism for hallucination control in production RAG systems. Without grounding, LLMs may generate plausible-sounding but factually incorrect information. Grounding techniques include prompt instructions (\"only use information from the context\"), citation requirements (\"cite the source for each claim\"), the \"I don't know\" pattern (\"if the answer is not in the context, say so\"), and post-generation verification that checks claims against source documents.\n\nIn hybrid RAG+KG systems, grounding is strengthened by combining evidence from multiple sources: structured facts from the knowledge graph and unstructured supporting text from document retrieval. When both sources agree, confidence is high. Disagreements can be flagged for human review. This multi-source grounding produces more reliable answers than either source alone.",
    "category": "NLP",
    "relatedTerms": [
      "hallucination",
      "rag",
      "citation",
      "retrieval-pipeline"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Citation",
    "slug": "citation",
    "shortDef": "The practice of attributing specific claims in an LLM-generated answer to their source documents, enabling verification and building trust.",
    "fullDef": "Citation in RAG systems refers to the practice of linking specific claims in a generated answer back to the source documents or knowledge graph facts that support them. This is typically implemented by including source identifiers in the context (e.g., [Source 1], [Source 2]) and instructing the LLM to reference these identifiers when making claims.\n\nCitation serves multiple critical functions in production systems: it enables users to verify claims by checking original sources, it builds trust in the system's outputs, it provides a mechanism for detecting hallucination (uncited claims may be fabricated), and it creates an audit trail for compliance-sensitive applications in healthcare, finance, and legal domains.\n\nImplementation involves building context with numbered source labels, prompting the LLM to use [Source X] notation, and optionally post-processing the response to extract and validate citations. In hybrid RAG+KG systems, citations can reference both document sources ([Doc 2]) and graph facts ([Graph: entity relationship]), providing richer provenance tracking than document-only citations.",
    "category": "NLP",
    "relatedTerms": [
      "grounding",
      "hallucination",
      "rag",
      "retrieval-pipeline"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Query Routing",
    "slug": "query-routing",
    "shortDef": "The process of classifying a user query and directing it to the most appropriate retrieval strategy, such as knowledge graph lookup, RAG search, or hybrid retrieval.",
    "fullDef": "Query routing is the process of analyzing a user query to determine which retrieval strategy will produce the best answer. In hybrid RAG+KG systems, queries can be routed to: knowledge graph direct lookup (for factual queries like \"Who is the CEO?\"), KG traversal (for relationship queries like \"Who reports to Alice?\"), RAG-heavy retrieval (for analytical queries requiring document analysis), or full hybrid retrieval (for complex queries needing both structured and unstructured knowledge).\n\nQuery routing is typically implemented using an LLM classifier that categorizes queries into types such as FACTUAL, RELATIONAL, MULTI_HOP, ANALYTICAL, or HYBRID, along with a confidence score. The router then directs each query type to the appropriate retrieval strategy, adjusting the balance between KG and RAG components.\n\nEffective query routing is a key differentiator of production hybrid systems. Without routing, every query hits both the knowledge graph and vector database, increasing latency and cost while potentially degrading quality (irrelevant KG results can confuse the LLM). With routing, the system applies the right tool for each query type, achieving both better quality and lower latency.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "knowledge-graph",
      "rag",
      "hybrid-search",
      "retrieval-pipeline",
      "context-fusion"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Dense Retrieval",
    "slug": "dense-retrieval",
    "shortDef": "A neural retrieval method that encodes queries and documents as dense vector embeddings and retrieves documents based on vector similarity.",
    "fullDef": "Dense retrieval is an information retrieval approach where both queries and documents are encoded as dense (mostly non-zero) vector embeddings using neural models, and relevance is determined by vector similarity (typically cosine similarity or dot product). This contrasts with sparse retrieval methods like BM25 that use high-dimensional sparse vectors based on term frequencies.\n\nThe key innovation of dense retrieval is that it captures semantic similarity rather than lexical overlap. A query about \"automobile maintenance\" can match a document about \"car repair\" because the neural encoder maps semantically related text to nearby points in embedding space. Dense Passage Retrieval (DPR) is a foundational dense retrieval method that uses separate BERT encoders for queries and documents, trained with contrastive learning on positive and negative query-document pairs.\n\nDense retrieval requires a vector database for efficient similarity search and GPU resources for encoding. While it excels at semantic matching, it can miss exact-match queries for rare terms, codes, and names. This limitation is why production systems combine dense retrieval with sparse retrieval (BM25) in hybrid search configurations.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "sparse-retrieval",
      "embedding",
      "vector-database",
      "semantic-search",
      "hybrid-search",
      "bi-encoder"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Sparse Retrieval",
    "slug": "sparse-retrieval",
    "shortDef": "A retrieval method using high-dimensional sparse vectors based on term frequencies (like BM25 or TF-IDF), where most vector elements are zero.",
    "fullDef": "Sparse retrieval refers to information retrieval methods that represent queries and documents as high-dimensional sparse vectors where most elements are zero. The most common sparse retrieval methods are TF-IDF and BM25, which create vectors with one dimension per vocabulary term, where non-zero values indicate term importance based on frequency statistics.\n\nThe term \"sparse\" describes both the vector representation (a document containing 100 unique words out of a 100,000-word vocabulary produces a vector with 99.9% zeros) and the matching behavior (only documents sharing exact terms with the query receive non-zero scores). This makes sparse retrieval excellent for exact-match scenarios like product IDs, error codes, and technical terminology.\n\nSparse retrieval requires no GPU, no neural model training, and provides interpretable results (you can see exactly which terms matched and contributed to the score). However, it cannot capture semantic similarity -- \"car\" and \"automobile\" are completely different terms in a sparse representation. In modern RAG systems, sparse retrieval is combined with dense retrieval in hybrid search to cover both exact-match and semantic-match needs.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "dense-retrieval",
      "bm25",
      "tf-idf",
      "hybrid-search"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Reciprocal Rank Fusion",
    "slug": "reciprocal-rank-fusion",
    "shortDef": "A method for combining ranked result lists from different retrieval systems by summing reciprocal rank scores, commonly used to merge BM25 and dense retrieval results.",
    "fullDef": "Reciprocal Rank Fusion (RRF) is a simple yet effective technique for combining ranked lists from multiple retrieval systems into a single unified ranking. For each document, RRF computes a score by summing 1/(k + rank) across all retrieval systems, where k is a constant (typically 60) and rank is the document's position in each list.\n\nThe formula Score(doc) = sum of 1/(k + rank_i(doc)) for each retrieval system i elegantly handles the fusion problem without requiring score normalization. Since it operates on ranks rather than raw scores, it works even when different retrieval systems produce scores on incompatible scales (e.g., BM25 scores vs. cosine similarities).\n\nRRF is the most commonly used fusion method in hybrid RAG systems that combine BM25 and dense retrieval results. Its popularity stems from its simplicity (no learned parameters), robustness (works across diverse retrieval methods), and consistent effectiveness (competitive with more complex learned fusion approaches). Alternative fusion methods include learned weighted combination and cross-attention-based merging, but RRF remains the go-to starting point for hybrid search.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "hybrid-search",
      "bm25",
      "dense-retrieval",
      "reranking"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Graph Embedding",
    "slug": "graph-embedding",
    "shortDef": "A technique for representing graph nodes as dense vectors that preserve graph structure, enabling similarity search and machine learning over graph data.",
    "fullDef": "Graph embeddings are dense vector representations of graph nodes (and sometimes edges) that encode structural information from the graph. The goal is to map nodes to vectors such that nodes that are structurally similar in the graph are close together in vector space, bridging the gap between graph-structured and vector-based representations.\n\nKey graph embedding methods include Node2Vec (generates random walks from each node and applies Word2Vec), DeepWalk (similar approach using uniform random walks), and Graph Convolutional Networks (GCN) which use neural message passing to aggregate information from node neighborhoods. The GCN update rule aggregates neighbor embeddings: h_v^(k+1) = activation(W * sum(h_u / |N(v)|)) for u in neighbors of v.\n\nGraph embeddings are particularly valuable in hybrid RAG+KG systems because they enable similarity search over graph entities even without direct text matches. You can find entities similar to a query entity based on their structural role in the graph (e.g., finding similar companies based on their position in a supply chain graph), complementing text-based similarity search with structural similarity.",
    "category": "Knowledge Graphs",
    "relatedTerms": [
      "knowledge-graph",
      "embedding",
      "node2vec",
      "graph-traversal"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Entity Linking",
    "slug": "entity-linking",
    "shortDef": "The task of resolving different textual mentions of an entity to a single canonical representation, critical for knowledge graph quality.",
    "fullDef": "Entity linking is the process of mapping entity mentions in text to their corresponding canonical entries in a knowledge base or knowledge graph. For example, the mentions \"Alice,\" \"Alice Smith,\" and \"A. Smith\" in different documents should all be linked to the same canonical entity node representing that person.\n\nEntity linking involves two sub-tasks: mention detection (identifying text spans that refer to entities) and disambiguation (determining which canonical entity a mention refers to when multiple candidates exist). For instance, \"Apple\" could refer to Apple Inc. or the fruit, and the surrounding context determines the correct linking.\n\nIn knowledge graph construction pipelines, entity linking is essential for maintaining data quality. Without it, the same entity gets multiple nodes, relationships are fragmented, and graph queries return incomplete results. Implementation approaches range from simple alias dictionaries and fuzzy string matching to LLM-based linking that uses context to resolve ambiguity. Entity linking quality directly impacts the completeness and usefulness of the resulting knowledge graph.",
    "category": "Knowledge Graphs",
    "relatedTerms": [
      "named-entity-recognition",
      "knowledge-graph",
      "relation-extraction",
      "triple"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Cross-Encoder",
    "slug": "cross-encoder",
    "shortDef": "A model architecture that jointly encodes a query-document pair to compute a relevance score, offering higher accuracy than bi-encoders but at greater computational cost.",
    "fullDef": "A cross-encoder is a transformer-based model that takes a query and a document as a single concatenated input and produces a relevance score. Unlike bi-encoders that encode query and document independently, cross-encoders allow full cross-attention between query and document tokens, enabling word-level interaction that captures fine-grained relevance signals.\n\nCross-encoders achieve significantly higher accuracy than bi-encoders because they can directly compare query terms against document terms. For example, when processing \"capital of France\" against a document containing \"Paris is the beautiful capital of France,\" the cross-encoder's attention mechanism explicitly connects \"capital\" and \"France\" in the query to \"Paris\" and \"capital\" in the document.\n\nThe tradeoff is computational cost: a cross-encoder must run a forward pass for every query-document pair, making it O(n) per query where n is the number of candidates. This makes cross-encoders impractical for initial retrieval over millions of documents but ideal for reranking a small set of candidates (typically 20-100). Common cross-encoder models include ms-marco-MiniLM and models from the sentence-transformers library.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "bi-encoder",
      "reranking",
      "retrieval-pipeline"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Bi-Encoder",
    "slug": "bi-encoder",
    "shortDef": "A model architecture that independently encodes queries and documents into separate embeddings for fast similarity comparison, used for initial retrieval at scale.",
    "fullDef": "A bi-encoder is a model architecture that uses two separate encoders (or a shared encoder) to independently convert queries and documents into fixed-size vector embeddings. Relevance is then determined by computing similarity (typically cosine similarity or dot product) between the query embedding and each document embedding.\n\nThe key advantage of bi-encoders is efficiency: document embeddings can be pre-computed and indexed offline, so at query time only the query needs to be encoded. This enables sub-millisecond retrieval over millions of documents using approximate nearest neighbor search in vector databases. Common bi-encoder models include Dense Passage Retrieval (DPR), sentence-transformers models, and commercial embedding APIs like OpenAI's text-embedding-3.\n\nThe tradeoff compared to cross-encoders is lower accuracy, because the query and document cannot directly attend to each other during encoding. Each vector must independently capture all relevant meaning. This limitation is why production RAG systems typically use bi-encoders for fast initial retrieval of candidates, followed by cross-encoder reranking for precision on the final results.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "cross-encoder",
      "embedding",
      "dense-retrieval",
      "reranking",
      "vector-database"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "TF-IDF",
    "slug": "tf-idf",
    "shortDef": "A numerical statistic combining term frequency and inverse document frequency to measure how important a word is to a document within a collection.",
    "fullDef": "TF-IDF (Term Frequency-Inverse Document Frequency) is a classic information retrieval weighting scheme that measures the importance of a word to a document within a larger collection. It combines two factors: Term Frequency (TF), which measures how often a term appears in a document, and Inverse Document Frequency (IDF), which measures how rare a term is across the entire collection.\n\nThe formula TF-IDF(t,d) = TF(t,d) x IDF(t) automatically downweights common words like \"the\" (high TF but very low IDF because it appears in nearly every document) and upweights distinctive terms (moderate TF but high IDF because they appear in few documents). This simple multiplication captures the intuition that a word is important to a document if it appears frequently in that document but rarely in others.\n\nWhile BM25 has largely superseded TF-IDF in modern retrieval systems by adding term frequency saturation and document length normalization, TF-IDF remains conceptually foundational. Understanding TF-IDF is essential for grasping why BM25 works, how sparse retrieval operates, and when keyword-based methods outperform neural approaches.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "bm25",
      "sparse-retrieval",
      "hybrid-search"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Cypher",
    "slug": "cypher",
    "shortDef": "A declarative graph query language created for Neo4j that uses ASCII-art syntax to represent and match graph patterns.",
    "fullDef": "Cypher is the query language for Neo4j graph databases, designed to be both human-readable and expressive for graph pattern matching. Its distinctive ASCII-art syntax uses parentheses for nodes and square brackets with arrows for relationships: `(alice:Person)-[:WORKS_FOR]->(company:Company)` visually represents a graph pattern.\n\nCypher supports the full range of graph operations: creating and matching patterns (MATCH, CREATE, MERGE), filtering results (WHERE), returning data (RETURN), aggregation (COUNT, SUM, AVG), path operations (shortestPath, variable-length patterns like `[*1..3]`), and graph algorithms via the GDS library (PageRank, community detection, Dijkstra's shortest path).\n\nIn hybrid RAG+KG systems, Cypher is often generated automatically from natural language queries using LLMs (text-to-Cypher). This enables users to query the knowledge graph without learning Cypher syntax. Production implementations include query validation (preventing destructive operations like DELETE), self-correction (retrying with error context when generated Cypher fails), and caching of common query patterns.",
    "category": "Knowledge Graphs",
    "relatedTerms": [
      "neo4j",
      "knowledge-graph",
      "graph-traversal",
      "sparql",
      "text-to-cypher"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Context Fusion",
    "slug": "context-fusion",
    "shortDef": "The process of combining structured knowledge from a knowledge graph with unstructured text from RAG retrieval into a unified context for LLM generation.",
    "fullDef": "Context fusion is the technique of merging structured knowledge (facts, relationships, and paths from a knowledge graph) with unstructured knowledge (retrieved document chunks from vector search) into a single coherent context that an LLM can use to generate comprehensive answers.\n\nThe standard approach formats graph results and document results into clearly labeled sections within the prompt: \"STRUCTURED KNOWLEDGE (from Knowledge Graph)\" containing entity facts and relationship paths, and \"UNSTRUCTURED KNOWLEDGE (from Documents)\" containing relevant text passages. Instructions guide the LLM to use structured knowledge for facts and relationships while using unstructured knowledge for details and explanations, citing each source type appropriately.\n\nContext fusion is where many hybrid RAG+KG systems fail. The naive approach of dumping both sources into a prompt without structure leads to the LLM ignoring one source or hallucinating connections between them approximately 40% of the time. Effective context fusion requires clear labeling, explicit instructions about when to prefer each source (structured knowledge for facts, documents for details), and conflict resolution rules (prefer structured knowledge when sources disagree on factual claims).",
    "category": "Information Retrieval",
    "relatedTerms": [
      "knowledge-graph",
      "rag",
      "query-routing",
      "hybrid-search",
      "grounding"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Text-to-Cypher",
    "slug": "text-to-cypher",
    "shortDef": "The technique of using LLMs to convert natural language questions into Cypher graph queries, enabling non-technical users to query knowledge graphs.",
    "fullDef": "Text-to-Cypher is the process of automatically translating natural language questions into executable Cypher queries using large language models. For example, the question \"What projects is Alice working on?\" would be converted to `MATCH (p:Person {name: 'Alice'})-[:WORKS_ON]->(proj:Project) RETURN proj.name, proj.status`.\n\nImplementation requires providing the LLM with the graph schema (node labels, relationship types, property names), few-shot examples of natural language to Cypher translations, and rules about query safety (preventing destructive operations like DELETE or REMOVE). The LLM uses this context to generate syntactically valid Cypher that matches the graph's actual structure.\n\nProduction text-to-Cypher systems include query validation (checking syntax and safety before execution), self-correction loops (if a generated query fails, the error message is fed back to the LLM for a retry, typically with 2-3 attempts), and result formatting (converting raw query results into human-readable answers). This technique is a key enabler of hybrid RAG+KG systems, allowing the query router to access structured graph knowledge without requiring users to learn Cypher.",
    "category": "Knowledge Graphs",
    "relatedTerms": [
      "cypher",
      "neo4j",
      "knowledge-graph",
      "query-routing"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Approximate Nearest Neighbor",
    "slug": "approximate-nearest-neighbor",
    "shortDef": "An algorithm that finds points approximately closest to a query in high-dimensional space, trading small accuracy loss for dramatically faster search over large datasets.",
    "fullDef": "Approximate Nearest Neighbor (ANN) search is a family of algorithms that find vectors approximately closest to a query vector in high-dimensional space, accepting small accuracy losses in exchange for dramatically faster search times. While exact nearest neighbor search requires comparing the query against every vector in the database (O(n) time), ANN algorithms achieve sub-linear search time, often O(log n).\n\nThe most popular ANN algorithms in vector databases include HNSW (Hierarchical Navigable Small World), which builds a multi-layer graph of vectors for efficient navigation; IVF (Inverted File Index), which partitions vectors into clusters and only searches nearby clusters; and product quantization, which compresses vectors to reduce memory usage and speed up distance computation.\n\nANN search is essential for production RAG systems because exact search becomes prohibitively slow at scale. A brute-force search over 10 million 1536-dimensional vectors takes seconds, while HNSW can find approximate nearest neighbors in milliseconds. The accuracy loss is typically 95-99% recall (meaning 95-99% of the true nearest neighbors are found), which is an acceptable tradeoff for most retrieval applications.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "vector-database",
      "faiss",
      "hnsw",
      "embedding"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "HNSW",
    "slug": "hnsw",
    "shortDef": "Hierarchical Navigable Small World -- an efficient graph-based algorithm for approximate nearest neighbor search that builds a multi-layer navigation structure over vectors.",
    "fullDef": "HNSW (Hierarchical Navigable Small World) is one of the most effective algorithms for approximate nearest neighbor search in high-dimensional vector spaces. It constructs a multi-layer graph where each layer is a navigable small-world network with progressively fewer nodes, enabling fast hierarchical search from coarse to fine granularity.\n\nThe algorithm works by starting the search at the top layer (fewest nodes, longest-range connections) and greedily navigating toward the query vector. At each layer, it finds the closest node, then descends to the next layer where more nodes and shorter-range connections allow finer search. This hierarchical approach achieves O(log n) search complexity with high recall.\n\nHNSW is the default index type in many vector databases (including Pinecone, Weaviate, and FAISS) because it offers the best balance of search speed, accuracy, and memory efficiency for most use cases. Key tuning parameters include M (the number of connections per node, affecting memory and accuracy) and efConstruction/efSearch (controlling the search thoroughness during index building and querying). Higher values improve accuracy at the cost of speed and memory.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "approximate-nearest-neighbor",
      "vector-database",
      "faiss"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Multi-Hop Reasoning",
    "slug": "multi-hop-reasoning",
    "shortDef": "Answering questions that require connecting multiple pieces of information across several reasoning steps, a key strength of knowledge graph-augmented systems.",
    "fullDef": "Multi-hop reasoning is the ability to answer questions that require following chains of connected information across multiple intermediate steps. For example, answering \"What technology does Alice's manager's company use?\" requires three hops: Alice -> manager (Bob) -> company (Acme Corp) -> technology (Python, AWS).\n\nMulti-hop reasoning is a primary motivation for combining knowledge graphs with RAG systems. Pure RAG struggles with multi-hop questions because each retrieval step is independent -- it cannot reliably chain information across separately retrieved documents. Knowledge graphs excel at multi-hop reasoning because the chain of relationships is explicitly represented and can be traversed in a single query: `MATCH (a:Person {name:'Alice'})-[:REPORTS_TO]->(m)-[:WORKS_FOR]->(c)-[:USES]->(t) RETURN t`.\n\nIn hybrid RAG+KG systems, multi-hop reasoning typically starts with knowledge graph traversal to establish the chain of entities and relationships, then uses RAG retrieval to find supporting details and context for each entity in the chain. This combination produces answers that are both structurally correct (from the graph) and richly detailed (from the documents).",
    "category": "Knowledge Graphs",
    "relatedTerms": [
      "knowledge-graph",
      "graph-traversal",
      "query-routing",
      "context-fusion",
      "rag"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "GraphRAG",
    "slug": "graphrag",
    "shortDef": "An architecture pattern that incorporates knowledge graph reasoning alongside vector-based retrieval in RAG systems, pioneered by Microsoft for enterprise search.",
    "fullDef": "GraphRAG is an architectural pattern and specific implementation (pioneered by Microsoft) that enhances standard RAG systems by incorporating knowledge graph structure into the retrieval and reasoning process. Rather than relying solely on vector similarity to find relevant documents, GraphRAG uses graph-based relationships between entities to guide retrieval and provide structured context.\n\nThe core GraphRAG approach involves: extracting entities and relationships from documents to build a knowledge graph, using community detection to identify topic clusters, generating summaries at different levels of the graph hierarchy, and leveraging this structure during query time to provide both local (specific entity) and global (high-level summary) context to the LLM.\n\nGraphRAG is particularly effective for queries that require understanding relationships between entities, synthesizing information across many documents, or providing answers that span multiple topics. Microsoft's research showed that GraphRAG significantly outperforms standard RAG on questions requiring comprehensive understanding of a document corpus, such as \"What are the main themes discussed across all documents?\" The pattern has become influential in enterprise AI deployments where relationship understanding and explainability are critical.",
    "category": "Information Retrieval",
    "relatedTerms": [
      "knowledge-graph",
      "rag",
      "context-fusion",
      "query-routing",
      "multi-hop-reasoning"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "SPARQL",
    "slug": "sparql",
    "shortDef": "A query language for RDF graph databases, similar to SQL but designed for querying data represented as subject-predicate-object triples.",
    "fullDef": "SPARQL (SPARQL Protocol and RDF Query Language) is the standard query language for retrieving and manipulating data stored in RDF (Resource Description Framework) format. It is to RDF databases what SQL is to relational databases and Cypher is to Neo4j property graphs.\n\nSPARQL queries match patterns of triples using a syntax based on the subject-predicate-object structure. For example, `SELECT ?company WHERE { :Alice :worksFor ?company . }` finds all companies Alice works for by matching triples where Alice is the subject and worksFor is the predicate. SPARQL supports multi-hop queries, aggregation, optional patterns, and federated queries across multiple endpoints.\n\nSPARQL is widely used in the semantic web ecosystem, with major knowledge bases like Wikidata, DBpedia, and Bio2RDF providing public SPARQL endpoints. While Cypher is more popular in industry applications using Neo4j, SPARQL is the standard for academic, government, and linked open data applications. Understanding both query languages provides flexibility in working with different knowledge graph implementations.",
    "category": "Knowledge Graphs",
    "relatedTerms": [
      "rdf",
      "cypher",
      "ontology",
      "triple",
      "knowledge-graph"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "RDF",
    "slug": "rdf",
    "shortDef": "Resource Description Framework -- a W3C standard for representing information as subject-predicate-object triples, forming the foundation of the semantic web.",
    "fullDef": "RDF (Resource Description Framework) is a W3C standard framework for representing information about resources as a graph of subject-predicate-object triples. Each triple states a single fact, such as (Alice, worksFor, AcmeCorp), and collections of triples form a knowledge graph. Resources are identified by URIs, allowing global, unambiguous identification of entities and relationships.\n\nRDF provides a formal, standardized way to publish and link data on the web (Linked Data). It supports schema definition through RDFS (RDF Schema) and richer ontological reasoning through OWL (Web Ontology Language). RDF data is queried using SPARQL and can be serialized in various formats including Turtle, JSON-LD, and RDF/XML.\n\nWhile property graph databases like Neo4j are more common in industry RAG+KG applications due to their flexibility and performance, RDF remains the standard for semantic web applications, open data initiatives, and domains requiring formal reasoning (biomedical, government, library science). Understanding RDF is valuable for working with public knowledge bases like Wikidata and for applications requiring interoperability across organizations.",
    "category": "Knowledge Graphs",
    "relatedTerms": [
      "triple",
      "sparql",
      "ontology",
      "knowledge-graph"
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "GPT-4o",
    "slug": "gpt-4o",
    "shortDef": "OpenAI's fast, cost-effective multimodal flagship model released in May 2024, supporting text, image, and audio with a 128K context window.",
    "fullDef": "GPT-4o is OpenAI's multimodal flagship model released in May 2024, designed to handle text, image, and audio inputs natively. The \"o\" stands for \"omni,\" reflecting its ability to process multiple modalities seamlessly. With a 128K token context window, GPT-4o provides a substantial improvement in both speed and cost-efficiency compared to earlier GPT-4 variants.\n\nThe model achieves strong performance across benchmarks, scoring approximately 88.7% on MMLU (Massive Multitask Language Understanding), with particular strengths in coding and reasoning tasks. Its multimodal capabilities enable it to analyze images, process audio, and generate coherent responses that integrate information across different input types.\n\nGPT-4o is priced at approximately $2.50 per million input tokens and $10 per million output tokens, making it significantly more accessible than previous GPT-4 iterations while maintaining high-quality outputs. It has become a popular choice for production applications requiring fast, reliable, and cost-effective language model capabilities across diverse use cases.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "transformer",
      "context-window"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "OpenAI - GPT-4o Announcement",
        "url": "https://openai.com/index/hello-gpt-4o/"
      },
      {
        "title": "OpenAI Platform - GPT-4o Model Card",
        "url": "https://platform.openai.com/docs/models/gpt-4o"
      },
      {
        "title": "OpenAI - GPT-4o System Card (PDF)",
        "url": "https://openai.com/index/gpt-4o-system-card/"
      }
    ]
  },
  {
    "name": "GPT-4.1",
    "slug": "gpt-4-1",
    "shortDef": "OpenAI's April 2025 API-focused model with a massive 1M token context window and 38.3% on MultiChallenge, beating GPT-4o by 10.5%.",
    "fullDef": "GPT-4.1 is OpenAI's API-focused model released in April 2025, representing a major leap in context length capabilities with a 1 million token context window-nearly 8x the capacity of GPT-4o's 128K tokens. This massive context window enables the model to handle extremely long documents, codebases, and multi-turn conversations with minimal information loss.\n\nThe model achieves 38.3% on the MultiChallenge benchmark for instruction following, surpassing GPT-4o by 10.5 percentage points. It has been specifically optimized for long-context tasks, coding workflows, and complex instruction following, making it particularly valuable for enterprise applications that require processing extensive documentation or maintaining context across lengthy interactions.\n\nGPT-4.1 is available in multiple variants including GPT-4.1 mini and GPT-4.1 nano, offering different trade-offs between capability, speed, and cost. The model's extended context window and improved instruction-following capabilities make it a strong choice for applications requiring deep contextual understanding over long sequences.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "context-window",
      "inference"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "OpenAI - Introducing GPT-4.1",
        "url": "https://openai.com/index/gpt-4-1/"
      },
      {
        "title": "OpenAI Platform - GPT-4.1 Documentation",
        "url": "https://platform.openai.com/docs/models"
      }
    ]
  },
  {
    "name": "OpenAI o3",
    "slug": "openai-o3",
    "shortDef": "OpenAI's reasoning model released in April 2025 with a 200K context window, achieving 88.9% on AIME 2025 and 69.1% on SWE-bench Verified.",
    "fullDef": "OpenAI o3 is a reasoning-focused model released in April 2025 that takes time to think before responding, enabling more deliberate and accurate problem-solving. With a 200K token context window, o3 represents OpenAI's effort to create models that can engage in deeper reasoning processes rather than simply generating immediate responses.\n\nThe model demonstrates exceptional performance on challenging benchmarks, achieving 88.9% on AIME 2025 (American Invitational Mathematics Examination) and 69.1% on SWE-bench Verified, a benchmark testing software engineering capabilities. These results highlight o3's strength in mathematical reasoning and complex code understanding tasks.\n\nA key feature of o3 is its ability to use tools agentically, including web search, code execution, and image generation. This agentic capability allows the model to autonomously decide when and how to use external tools to solve problems, making it particularly effective for complex, multi-step tasks that require more than pure language generation.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "ai-agent",
      "prompt-engineering"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "OpenAI - Introducing o3 and o4-mini",
        "url": "https://openai.com/index/introducing-o3-and-o4-mini/"
      },
      {
        "title": "OpenAI Platform - o3 Model Documentation",
        "url": "https://platform.openai.com/docs/models"
      }
    ]
  },
  {
    "name": "OpenAI o4-mini",
    "slug": "openai-o4-mini",
    "shortDef": "OpenAI's smaller reasoning model released in April 2025, achieving 92.7% on AIME 2025 and 99.5% with Python interpreter access.",
    "fullDef": "OpenAI o4-mini is a compact reasoning model released in April 2025, optimized for speed and cost while maintaining exceptional reasoning capabilities. With a 200K token context window matching its larger sibling o3, o4-mini demonstrates that effective reasoning doesn't always require the largest models.\n\nRemarkably, o4-mini achieves 92.7% on AIME 2025, actually surpassing the larger o3 model on this challenging mathematics benchmark. On SWE-bench, it scores 68.1%, showing strong software engineering capabilities. When given access to a Python interpreter, its AIME 2025 performance jumps to an extraordinary 99.5%, demonstrating the power of tool-augmented reasoning.\n\nThe model represents a significant achievement in efficient AI design, offering exceptional math and coding performance for its size and cost. This makes it an attractive option for applications requiring reasoning capabilities but operating under budget or latency constraints, proving that smaller models with proper training can compete with or exceed larger counterparts on specific tasks.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "inference",
      "ai-agent"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "OpenAI - Introducing o3 and o4-mini",
        "url": "https://openai.com/index/introducing-o3-and-o4-mini/"
      },
      {
        "title": "OpenAI Platform - o4-mini Documentation",
        "url": "https://platform.openai.com/docs/models"
      }
    ]
  },
  {
    "name": "GPT-5",
    "slug": "gpt-5",
    "shortDef": "OpenAI's major generational leap released in August 2025, achieving 94.6% on AIME 2025 and 45% fewer factual errors than GPT-4o.",
    "fullDef": "GPT-5, released in August 2025, represents a major generational leap in OpenAI's language model capabilities. The model achieves 94.6% on AIME 2025 without any tools and 74.9% on SWE-bench, demonstrating substantial improvements in mathematical reasoning and software engineering compared to previous generations.\n\nA critical advancement in GPT-5 is its significant reduction in factual errors-45% fewer than GPT-4o when using web search, and 80% fewer than o3 when engaging in reasoning tasks. On specialized benchmarks, GPT-5 scores 46.2% on HealthBench Hard (medical reasoning) and 84.2% on MMMU (massive multi-discipline understanding), showing strong performance across diverse domains.\n\nGPT-5 also demonstrates improved efficiency, performing better than o3 while using 50-80% fewer output tokens for the same tasks. This combination of higher accuracy, reduced hallucinations, and greater efficiency makes GPT-5 a significant milestone in language model development, suitable for high-stakes applications in healthcare, legal reasoning, and other domains where factual accuracy is paramount.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "hallucination",
      "transformer"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "OpenAI - Introducing GPT-5",
        "url": "https://openai.com/index/introducing-gpt-5/"
      },
      {
        "title": "OpenAI Platform - GPT-5 Documentation",
        "url": "https://platform.openai.com/docs/models"
      }
    ]
  },
  {
    "name": "GPT-5.2",
    "slug": "gpt-5-2",
    "shortDef": "OpenAI's December 2025 model with a 256K context window, 100% AIME 2025 accuracy, and hallucination rate reduced to 6.2%.",
    "fullDef": "GPT-5.2, released in December 2025, represents OpenAI's continued refinement of the GPT-5 architecture with dramatic improvements in reliability and context handling. The model features a 256K token context window with near-perfect accuracy retention, expandable to 400K tokens maximum, enabling it to process extremely long documents without degradation.\n\nThe model achieves perfect 100% accuracy on AIME 2025, a remarkable milestone for mathematical reasoning, and reduces its hallucination rate to just 6.2%-a significant improvement in factual reliability. On GDPval (a benchmark comparing AI to human experts), GPT-5.2 beats or ties human experts 70.9% of the time, demonstrating superhuman performance in specific domains.\n\nGPT-5.2 is available in specialized variants: GPT-5.2 Instant (optimized for low latency) and GPT-5.2 Thinking (with extended reasoning capabilities). These variants allow users to choose the appropriate trade-off between response speed and reasoning depth, making the model adaptable to different application requirements from real-time chat to complex analytical tasks.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "hallucination",
      "context-window"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "OpenAI - Introducing GPT-5.2",
        "url": "https://openai.com/index/introducing-gpt-5-2/"
      },
      {
        "title": "OpenAI Platform - Models Overview",
        "url": "https://platform.openai.com/docs/models"
      }
    ]
  },
  {
    "name": "Claude Sonnet 4.5",
    "slug": "claude-sonnet-4-5",
    "shortDef": "Anthropic's September 2025 model marketed as the best coding model and best for agents, achieving 77.2% on SWE-bench and 100% on AIME with Python.",
    "fullDef": "Claude Sonnet 4.5, released by Anthropic in September 2025, is marketed as the best coding model and the best model for agents. With a 200K token context window and 64K token output limit, it provides substantial capacity for complex, multi-turn interactions and lengthy code generation tasks.\n\nThe model achieves 77.2% on SWE-bench (82% with parallel compute capabilities), demonstrating exceptional software engineering abilities. It scores 100% on AIME when provided with Python access and 83.4% on GPQA Diamond (graduate-level science questions), showing strong performance across mathematics, science, and programming domains. Sonnet 4.5 has gained particular recognition for its exceptional frontend and UI code generation capabilities.\n\nPriced at $3 per million input tokens and $15 per million output tokens, Claude Sonnet 4.5 offers competitive pricing for its capability level. Its combination of strong reasoning, excellent coding abilities, and reliable agentic behavior has made it a popular choice for developers building AI-powered applications, particularly those involving code generation, autonomous agents, and complex multi-step workflows.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "ai-agent",
      "context-window"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "Anthropic - Introducing Claude Sonnet 4.5",
        "url": "https://www.anthropic.com/news/claude-sonnet-4-5"
      },
      {
        "title": "Anthropic - Claude Models Overview",
        "url": "https://docs.anthropic.com/en/docs/about-claude/models"
      },
      {
        "title": "Anthropic API - Claude Documentation",
        "url": "https://platform.claude.com/docs/en/about-claude/models/overview"
      }
    ]
  },
  {
    "name": "Claude Opus 4.5",
    "slug": "claude-opus-4-5",
    "shortDef": "Anthropic's November 2025 flagship model achieving 80.9% on SWE-bench with 50-75% reduction in tool calling errors.",
    "fullDef": "Claude Opus 4.5, released by Anthropic in November 2025, represents the company's most capable model in the 4.5 generation. Like Sonnet 4.5, it features a 200K token context window and 64K token output limit, but with enhanced reasoning and reliability capabilities that justify its premium positioning.\n\nThe model achieves 80.9% on SWE-bench and 59.3% on Terminal-bench, demonstrating superior performance on software engineering and command-line interface tasks. A key improvement is the 50-75% reduction in tool calling errors compared to previous models, along with up to 65% fewer tokens required to complete the same tasks, making it both more reliable and more efficient.\n\nClaude Opus 4.5 excels at context management and multi-step reasoning, making it particularly well-suited for complex agentic workflows requiring reliable tool use and long-chain reasoning. Priced at $5 per million input tokens and $25 per million output tokens, it sits at the premium end of the market but offers commensurate capabilities for applications where accuracy, reliability, and reasoning depth are critical.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "ai-agent",
      "inference"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "Anthropic - Introducing Claude Opus 4.5",
        "url": "https://www.anthropic.com/news/claude-opus-4-5"
      },
      {
        "title": "Anthropic - Claude Models Overview",
        "url": "https://docs.anthropic.com/en/docs/about-claude/models"
      },
      {
        "title": "Anthropic API - Claude Documentation",
        "url": "https://platform.claude.com/docs/en/about-claude/models/overview"
      }
    ]
  },
  {
    "name": "Claude Haiku 4.5",
    "slug": "claude-haiku-4-5",
    "shortDef": "Anthropic's fastest model released in October 2025, achieving 90% of Sonnet 4.5's performance on agentic coding at lower cost.",
    "fullDef": "Claude Haiku 4.5, released by Anthropic in October 2025, is the fastest model in the Claude family while maintaining a 200K token context window. It's designed for applications where speed and cost-efficiency are priorities, without sacrificing too much capability compared to the higher-tier models.\n\nRemarkably, Haiku 4.5 achieves approximately 90% of Sonnet 4.5's performance on agentic coding tasks, making it a compelling choice for many production use cases. A notable feature is its support for extended thinking-a capability that was previously limited to premium models-allowing it to engage in more deliberate reasoning when needed.\n\nPriced at $1 per million input tokens and $5 per million output tokens, Claude Haiku 4.5 offers the most cost-effective option in Anthropic's lineup while maintaining strong performance. This combination of speed, capability, and affordability makes it particularly attractive for high-volume applications, rapid prototyping, and use cases where the full power of Opus or Sonnet isn't required but Claude's safety and quality characteristics are still desired.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "inference",
      "context-window"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "Anthropic - Claude Haiku 4.5 Announcement",
        "url": "https://www.anthropic.com/news/claude-haiku-4-5"
      },
      {
        "title": "Anthropic - Claude Models Overview",
        "url": "https://docs.anthropic.com/en/docs/about-claude/models"
      }
    ]
  },
  {
    "name": "Claude Opus 4.6",
    "slug": "claude-opus-4-6",
    "shortDef": "Anthropic's February 2026 flagship with 1M context window, 80.8% on SWE-bench, 68.8% on ARC-AGI-2, and the highest Terminal-Bench 2.0 score among all frontier models.",
    "fullDef": "Claude Opus 4.6, released by Anthropic on February 5, 2026, is the company's most capable model ever, featuring a 1 million token context window (beta) and 128K max output tokens. It introduces adaptive thinking and achieves the highest agentic coding scores Anthropic has produced to date. The model plans more carefully, sustains agentic tasks for longer, operates more reliably in larger codebases, and has improved code review and debugging skills to catch its own mistakes.\n\nOpus 4.6 leads all frontier models on Terminal-Bench 2.0 (65.4%) for agentic coding and Humanity's Last Exam (53.1% with tools, 40.0% without) for complex multidisciplinary reasoning. It scores 80.8% on SWE-bench Verified, 68.8% on ARC-AGI-2 (up from 37.6% for Opus 4.5, the largest single-generation leap on this benchmark), and 72.7% on OSWorld-Verified for computer use. On GDPval-AA (knowledge work in finance, legal, and other domains), Opus 4.6 outperforms GPT-5.2 by 144 Elo points and its predecessor Opus 4.5 by 190 points. On MRCR v2 (8-needle) long-context retrieval, it scores 93% at 256K tokens and 76% at 1M tokens.\n\nPriced at $5 per million input tokens and $25 per million output tokens (unchanged from Opus 4.5), the model is available on claude.ai, the Anthropic API, Amazon Bedrock, and Google Cloud Vertex AI. Anthropic states Opus 4.6 shows an overall safety profile as good as or better than any other frontier model in the industry.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "ai-agent",
      "context-window",
      "claude-sonnet-4-6"
    ],
    "lastUpdated": "2026-02-23",
    "references": [
      {
        "title": "Anthropic - Introducing Claude Opus 4.6",
        "url": "https://www.anthropic.com/news/claude-opus-4-6"
      },
      {
        "title": "Anthropic - Claude Models Overview",
        "url": "https://docs.anthropic.com/en/docs/about-claude/models"
      },
      {
        "title": "Anthropic API - Models Documentation",
        "url": "https://platform.claude.com/docs/en/about-claude/models/overview"
      },
      {
        "title": "Vellum - Claude Opus 4.6 Benchmarks Explained",
        "url": "https://www.vellum.ai/blog/claude-opus-4-6-benchmarks"
      }
    ]
  },
  {
    "name": "Claude Sonnet 4.6",
    "slug": "claude-sonnet-4-6",
    "shortDef": "Anthropic's February 2026 mid-tier model achieving 79.6% on SWE-bench and 72.5% on OSWorld, matching near-flagship performance at $3/$15 per million tokens.",
    "fullDef": "Claude Sonnet 4.6, released by Anthropic on February 17, 2026, delivers near-flagship performance across coding, computer use, long-context reasoning, agent planning, knowledge work, and design - all at one-fifth the cost of Opus 4.6. It features a 1 million token context window (beta), matching Opus 4.6's context capacity. In head-to-head comparisons, users preferred Sonnet 4.6 over Sonnet 4.5 in 70% of cases and over the previous flagship Opus 4.5 in 59% of comparisons.\n\nSonnet 4.6 scores 79.6% on SWE-bench Verified (only 1.2 points behind Opus 4.6's 80.8%), 72.5% on OSWorld-Verified for computer use (nearly matching Opus 4.6's 72.7%), and 61.3% on MCP-Atlas for scaled tool use. On GDPval-AA for office productivity tasks, Sonnet 4.6 reaches 1633 Elo - actually ahead of all models including Opus 4.6 in this specific category. It also scores 63.3% on Finance Agent for financial analysis tasks.\n\nPriced at $3 per million input tokens and $15 per million output tokens (with up to 90% savings via prompt caching and 50% with batch processing), Sonnet 4.6 represents the strongest value proposition in frontier AI. Its combination of near-Opus performance at mid-tier pricing makes it the default choice for most production deployments, particularly for agentic coding, computer use, and knowledge work applications where the marginal performance gap to Opus doesn't justify the 5x price premium.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "ai-agent",
      "context-window",
      "claude-opus-4-6"
    ],
    "lastUpdated": "2026-02-23",
    "references": [
      {
        "title": "Anthropic - Introducing Claude Sonnet 4.6",
        "url": "https://www.anthropic.com/news/claude-sonnet-4-6"
      },
      {
        "title": "Anthropic - Claude Sonnet Product Page",
        "url": "https://www.anthropic.com/claude/sonnet"
      },
      {
        "title": "Anthropic - Claude Models Overview",
        "url": "https://docs.anthropic.com/en/docs/about-claude/models"
      },
      {
        "title": "VentureBeat - Sonnet 4.6 Matches Flagship Performance at 1/5th Cost",
        "url": "https://venturebeat.com/technology/anthropics-sonnet-4-6-matches-flagship-ai-performance-at-one-fifth-the-cost"
      }
    ]
  },
  {
    "name": "Gemini 2.5 Pro",
    "slug": "gemini-2-5-pro",
    "shortDef": "Google DeepMind's 2025 flagship with 1M token context window, leading Humanity's Last Exam with 18.8% accuracy.",
    "fullDef": "Gemini 2.5 Pro is Google DeepMind's flagship model released in 2025, featuring a massive 1 million token context window and native multimodal capabilities spanning text, image, video, and audio. This extensive multimodal support allows the model to process and reason across different types of media seamlessly.\n\nThe model leads on Humanity's Last Exam, a challenging benchmark designed to test the limits of AI capabilities, achieving 18.8% accuracy-a low absolute score that reflects the extreme difficulty of the benchmark rather than model weakness. Gemini 2.5 Pro demonstrates strong reasoning and long-context capabilities, making it suitable for complex analytical tasks requiring extensive context.\n\nAs Google's flagship offering in the Gemini family, 2.5 Pro competes directly with OpenAI's GPT-5 series and Anthropic's Claude Opus models. Its combination of massive context window, multimodal processing, and strong reasoning makes it particularly well-suited for applications involving analysis of large documents, video understanding, and tasks requiring integration of information across multiple modalities.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "context-window",
      "transformer"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "Google DeepMind - Gemini 2.5 Pro",
        "url": "https://deepmind.google/technologies/gemini/pro/"
      },
      {
        "title": "Google AI - Gemini API Models",
        "url": "https://ai.google.dev/gemini-api/docs/models"
      },
      {
        "title": "Google AI Studio - Try Gemini",
        "url": "https://aistudio.google.com/"
      }
    ]
  },
  {
    "name": "Gemini 2.5 Flash",
    "slug": "gemini-2-5-flash",
    "shortDef": "Google DeepMind's fast model released in May 2025 with 1M context window and 251 tokens/second output speed.",
    "fullDef": "Gemini 2.5 Flash, released by Google DeepMind in May 2025, is optimized for speed while maintaining the impressive 1 million token context window of its Pro sibling. The model achieves an exceptional 251 tokens per second output speed, making it one of the fastest frontier models available for high-throughput applications.\n\nGemini 2.5 Flash features hybrid thinking control, allowing it to dynamically adjust its reasoning depth based on the complexity of the task. It supports multimodal input across text, images, video, and audio, enabling diverse application scenarios from document analysis to multimedia understanding.\n\nThe model is designed for applications where speed is critical but quality can't be sacrificed, such as real-time chat applications, high-volume API services, and interactive tools. By maintaining quality while dramatically improving speed, Gemini 2.5 Flash demonstrates that frontier capabilities don't always require sacrificing latency, making advanced AI more accessible for time-sensitive use cases.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "inference",
      "context-window"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "Google DeepMind - Gemini 2.5 Flash",
        "url": "https://deepmind.google/technologies/gemini/flash/"
      },
      {
        "title": "Google AI - Gemini API Models",
        "url": "https://ai.google.dev/gemini-api/docs/models"
      },
      {
        "title": "Google - Gemini 2.5 Flash Model Card (PDF)",
        "url": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Flash-Model-Card.pdf"
      }
    ]
  },
  {
    "name": "Gemini 3.1 Pro",
    "slug": "gemini-3-1-pro",
    "shortDef": "Google DeepMind's February 2026 model topping 13 of 16 industry benchmarks with 77.1% on ARC-AGI-2 and 94.3% on GPQA Diamond.",
    "fullDef": "Gemini 3.1 Pro, released by Google DeepMind in February 2026, represents a major leap forward in capabilities, topping 13 out of 16 major industry benchmarks. The model achieves 77.1% on ARC-AGI-2 (a benchmark measuring abstract reasoning and general intelligence), 94.3% on GPQA Diamond (graduate-level science questions), and a LiveCodeBench Pro Elo rating of 2887, demonstrating exceptional performance across reasoning, science, and coding domains.\n\nOn specialized benchmarks, Gemini 3.1 Pro scores 85.9% on BrowseComp (web browsing and information synthesis) and 69.2% on MCP Atlas (multi-modal context processing). These results highlight the model's strength not just in traditional NLP tasks but also in complex, real-world scenarios requiring tool use and multi-step reasoning.\n\nA notable feature is the \"medium\" parameter that allows users to trade off between compute intensity and latency, providing flexibility for different application requirements. Gemini 3.1 Pro's broad benchmark dominance and configurable performance characteristics make it a strong contender in the most advanced tier of language models, competing directly with GPT-5.2, Claude Opus 4.6, and other frontier models.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "ai-agent",
      "context-window"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "Google Blog - Gemini 3.1 Pro Announcement",
        "url": "https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro/"
      },
      {
        "title": "Google DeepMind - Gemini 3.1 Pro Model Card",
        "url": "https://deepmind.google/models/model-cards/gemini-3-1-pro/"
      },
      {
        "title": "Google AI - Gemini API Models",
        "url": "https://ai.google.dev/gemini-api/docs/models"
      }
    ]
  },
  {
    "name": "DeepSeek R1",
    "slug": "deepseek-r1",
    "shortDef": "Open-weight reasoning model released in January 2025, achieving 97.3% on MATH-500 and proving frontier AI doesn't require massive budgets.",
    "fullDef": "DeepSeek R1, released in January 2025, is an open-weight reasoning model that sparked what became known as the \"DeepSeek moment\" in AI-demonstrating that frontier-level capabilities could be achieved without the massive training budgets typical of Western AI labs. The model achieves 97.3% on MATH-500 (surpassing OpenAI's o1) and 90.8% on MMLU, showcasing exceptional mathematical reasoning at a fraction of typical development costs.\n\nDeepSeek R1 has won gold medals at both IMO 2025 (International Mathematical Olympiad) and IOI 2025 (International Olympiad in Informatics), demonstrating superhuman performance on elite-level mathematics and competitive programming problems. These achievements highlighted that open-weight models could compete with or exceed proprietary reasoning models on specific benchmarks.\n\nThe model's release was significant not just for its technical achievements but for its implications about AI development economics. By achieving ChatGPT-level reasoning capabilities with dramatically lower training costs, DeepSeek R1 challenged assumptions about the resource requirements for frontier AI and accelerated the open-weight movement, inspiring numerous projects to pursue cost-efficient training approaches.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "reinforcement-learning",
      "inference"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "DeepSeek - DeepSeek-R1 Technical Report",
        "url": "https://api-docs.deepseek.com/news/news250120"
      },
      {
        "title": "Hugging Face - DeepSeek-R1",
        "url": "https://huggingface.co/deepseek-ai/DeepSeek-R1"
      },
      {
        "title": "GitHub - DeepSeek-R1",
        "url": "https://github.com/deepseek-ai/DeepSeek-R1"
      },
      {
        "title": "arXiv - DeepSeek-R1 Paper",
        "url": "https://arxiv.org/abs/2501.12948"
      }
    ]
  },
  {
    "name": "DeepSeek V3",
    "slug": "deepseek-v3",
    "shortDef": "Open-weight MoE model with updated version V3-0324 scoring 81.2% on MMLU-Pro and ranking 5th on LMArena leaderboard.",
    "fullDef": "DeepSeek V3 is an open-weight model built on a Mixture of Experts (MoE) architecture, allowing it to achieve high performance while maintaining computational efficiency during inference. The updated version V3-0324 scores 81.2% on MMLU-Pro, 68.4% on GPQA, 59.4% on AIME, and 49.2% on LiveCodeBench, demonstrating strong performance across reasoning, science, mathematics, and coding benchmarks.\n\nThe model ranks 5th on the LMArena leaderboard, a competitive ranking among both open and closed models based on human evaluations. This placement demonstrates that DeepSeek V3 competes effectively with many proprietary models while being freely available for research and commercial use.\n\nDeepSeek V3's combination of MoE architecture, strong benchmark performance, and open-weight availability makes it highly attractive for cost-conscious deployments and for researchers who need access to model internals. Its success in both reasoning and coding tasks, coupled with cost efficiency, has made it a popular choice for self-hosted deployments and as a base model for fine-tuning specialized applications.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "transformer",
      "inference"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "DeepSeek - DeepSeek-V3 Technical Report",
        "url": "https://api-docs.deepseek.com/news/news1226"
      },
      {
        "title": "Hugging Face - DeepSeek-V3",
        "url": "https://huggingface.co/deepseek-ai/DeepSeek-V3"
      },
      {
        "title": "GitHub - DeepSeek-V3",
        "url": "https://github.com/deepseek-ai/DeepSeek-V3"
      },
      {
        "title": "arXiv - DeepSeek-V3 Paper",
        "url": "https://arxiv.org/abs/2412.19437"
      }
    ]
  },
  {
    "name": "Llama 4 Scout",
    "slug": "llama-4-scout",
    "shortDef": "Meta's April 2025 open-weight model with 109B total parameters and industry-leading 10M token context window.",
    "fullDef": "Llama 4 Scout, released by Meta in April 2025, is an open-weight model utilizing a Mixture of Experts (MoE) architecture with 109B total parameters but only 17B active parameters during inference. This design provides strong capabilities while maintaining computational efficiency. The model's standout feature is its industry-leading 10 million token context window-the largest context capacity of any production language model.\n\nLlama 4 Scout beats competing models like Gemma 3, Gemini 2.0 Flash-Lite, and Mistral Small 3.1 across various benchmarks, with overall performance comparable to GPT-4o mini. This positions it as a strong open-source alternative to proprietary small models, offering similar capabilities without API costs or data privacy concerns.\n\nThe massive 10M token context window enables entirely new use cases, such as processing multiple full-length books, entire codebases with all dependencies, or maintaining context across very long conversations. Combined with its open-weight nature and efficient MoE architecture, Llama 4 Scout represents a significant contribution to the open AI ecosystem, particularly for applications requiring extreme context lengths.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "context-window",
      "transformer"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "Meta AI - The Llama 4 Herd",
        "url": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/"
      },
      {
        "title": "Hugging Face - Llama 4 Scout",
        "url": "https://huggingface.co/meta-llama"
      },
      {
        "title": "Llama - Official Website",
        "url": "https://www.llama.com/"
      }
    ]
  },
  {
    "name": "Llama 4 Maverick",
    "slug": "llama-4-maverick",
    "shortDef": "Meta's April 2025 open-weight flagship with 402B total parameters, 1M context window, and multimodal capabilities beating GPT-4o.",
    "fullDef": "Llama 4 Maverick, released by Meta in April 2025, is the flagship of the Llama 4 family with 402B total parameters and 17B active parameters using a Mixture of Experts architecture. The model features a 1 million token context window, providing substantial capacity for long-document processing and extended conversations.\n\nMaverick beats GPT-4o and Gemini 2.0 Flash across multiple benchmarks and is described as comparable to DeepSeek V3 in overall capability. Notably, it is positioned as the best multimodal open model in its class, handling text, images, and other modalities effectively while remaining fully open-weight and available for commercial use.\n\nAs Meta's most capable open-weight model, Llama 4 Maverick represents a significant milestone in democratizing access to frontier AI capabilities. Its combination of strong performance, multimodal abilities, large context window, and open availability makes it attractive for organizations seeking to deploy advanced AI without dependence on proprietary APIs or concerns about vendor lock-in.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "context-window",
      "transformer"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "Meta AI - The Llama 4 Herd",
        "url": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/"
      },
      {
        "title": "Hugging Face - Llama 4 Maverick",
        "url": "https://huggingface.co/meta-llama"
      },
      {
        "title": "Llama - Official Website",
        "url": "https://www.llama.com/"
      }
    ]
  },
  {
    "name": "Qwen 3",
    "slug": "qwen-3",
    "shortDef": "Alibaba's April 2025 open-source model family trained on 36 trillion tokens in 119 languages, competitive with DeepSeek R1 and o3-mini.",
    "fullDef": "Qwen 3, released by Alibaba in April 2025, is an open-source model family featuring both dense and Mixture of Experts (MoE) architectures. The flagship model, Qwen3-235B-A22B, has 235B total parameters with 22B active, while dense models range from 0.6B to 32B parameters and MoE variants include 30B-A3B and 235B-A22B configurations. This diverse family provides options for different deployment scenarios and resource constraints.\n\nThe models are trained on an impressive 36 trillion tokens spanning 119 languages, making Qwen 3 one of the most multilingual model families available. A key feature is hybrid reasoning, allowing the models to switch between thinking mode (slower, more deliberate reasoning) and non-thinking mode (faster direct responses) depending on task requirements.\n\nQwen 3 is competitive with DeepSeek R1, OpenAI's o1 and o3-mini, and Grok 3 across various benchmarks, demonstrating that strong performance can be achieved outside of American AI labs. The combination of open-source availability, multilingual capabilities, and flexible reasoning modes makes Qwen 3 particularly valuable for international deployments and applications requiring language diversity.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "transformer",
      "inference"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "Qwen Blog - Qwen3: Think Deeper, Act Faster",
        "url": "https://qwenlm.github.io/blog/qwen3/"
      },
      {
        "title": "GitHub - QwenLM/Qwen3",
        "url": "https://github.com/QwenLM/Qwen3"
      },
      {
        "title": "Hugging Face - Qwen",
        "url": "https://huggingface.co/Qwen"
      },
      {
        "title": "Alibaba Cloud - Qwen3 Announcement",
        "url": "https://www.alibabacloud.com/blog/alibaba-introduces-qwen3-setting-new-benchmark-in-open-source-ai-with-hybrid-reasoning_602192"
      }
    ]
  },
  {
    "name": "Grok 3",
    "slug": "grok-3",
    "shortDef": "xAI's June 2025 model with 1M context window, beating GPT-4o and Claude 3.5 Sonnet on AIME and GPQA with 1402 Arena Elo.",
    "fullDef": "Grok 3, released by xAI in June 2025, features a 1 million token context window and was trained on the Colossus supercluster using 10x the compute of the previous state-of-the-art. This massive computational investment enabled significant performance improvements across multiple benchmarks.\n\nThe model beats GPT-4o, Claude 3.5 Sonnet, and DeepSeek V3 on challenging benchmarks including AIME (mathematics), GPQA (graduate-level science), and LiveCodeBench (coding). It achieves a Chatbot Arena Elo rating of 1402, placing it competitively among frontier models based on human preference evaluations.\n\nGrok 3's development demonstrated the effectiveness of scaling compute and the Colossus supercluster's capabilities in training frontier models. The 1M context window combined with strong reasoning and coding performance makes it suitable for complex applications requiring both extensive context and high-quality outputs. Its competitive performance against established models from OpenAI and Anthropic highlighted xAI's rapid progress in the frontier model space.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "transformer",
      "context-window"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "xAI - Grok 3 Beta Announcement",
        "url": "https://x.ai/news/grok-3"
      },
      {
        "title": "xAI - Official Website",
        "url": "https://x.ai/"
      },
      {
        "title": "Grok - Try Grok",
        "url": "https://grok.com/"
      }
    ]
  },
  {
    "name": "Grok 4",
    "slug": "grok-4",
    "shortDef": "xAI's July 2025 model achieving 100% on AIME 2025 and 61.9% on USAMO 2025, with 4-agent parallel collaboration in latest beta.",
    "fullDef": "Grok 4, released by xAI in July 2025, represents a major leap in mathematical and scientific reasoning capabilities. The model achieves perfect 100% accuracy on AIME 2025, 96.7% on HMMT25 (Harvard-MIT Math Tournament), 61.9% on USAMO 2025 (USA Mathematical Olympiad), and 88.4-88.9% on GPQA (graduate-level science questions). These results demonstrate exceptional performance on elite-level mathematics and science problems.\n\nGrok 4 outperforms Claude 4 Opus, Gemini 2.5 Pro, and GPT-4o across multiple benchmarks, establishing it as one of the strongest reasoning models available. The latest beta version (4.20, released February 2026) introduces rapid-learning capabilities and 4-agent parallel collaboration, allowing the model to coordinate multiple specialized reasoning processes simultaneously.\n\nThe multi-agent parallel collaboration feature represents an innovative approach to complex problem-solving, enabling Grok 4 to break down challenging tasks and tackle them from multiple angles simultaneously. Combined with its exceptional mathematical reasoning and rapid learning capabilities, Grok 4 demonstrates xAI's commitment to pushing the boundaries of AI reasoning and agentic capabilities.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "ai-agent",
      "multi-hop-reasoning"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "xAI - Official Website",
        "url": "https://x.ai/"
      },
      {
        "title": "Grok - Try Grok",
        "url": "https://grok.com/"
      }
    ]
  },
  {
    "name": "Mistral Medium 3",
    "slug": "mistral-medium-3",
    "shortDef": "European AI model achieving 90% of Claude Sonnet 3.7 capabilities while demonstrating cost-efficient alternative to premium models.",
    "fullDef": "Mistral Medium 3 is a strong European AI model developed by Mistral AI, achieving approximately 90% of Claude Sonnet 3.7's capabilities while maintaining more accessible pricing. This positions it as a compelling option for organizations seeking high-quality language model capabilities without premium pricing tiers.\n\nThe model demonstrates that frontier-level performance doesn't necessarily require the highest-priced offerings, making advanced AI more accessible to a broader range of users and applications. Mistral Medium 3's competitive performance relative to Claude Sonnet 3.7 while being more cost-efficient makes it attractive for enterprise deployments with budget constraints.\n\nAs a European-developed model, Mistral Medium 3 also addresses data sovereignty and regulatory concerns for EU-based organizations. The combination of strong performance, cost efficiency, and European provenance makes it a strategic choice for applications requiring compliance with European data protection regulations while maintaining high-quality AI capabilities.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "transformer",
      "inference"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "Mistral AI - Official Website",
        "url": "https://mistral.ai/"
      },
      {
        "title": "Mistral AI - Model Documentation",
        "url": "https://docs.mistral.ai/getting-started/models/models_overview/"
      },
      {
        "title": "Mistral AI - La Plateforme",
        "url": "https://console.mistral.ai/"
      }
    ]
  },
  {
    "name": "GPT-oss-120b",
    "slug": "gpt-oss-120b",
    "shortDef": "OpenAI's first major open-weight model with 117B parameters and MoE architecture, rivaling proprietary o4-mini performance.",
    "fullDef": "GPT-oss-120b represents OpenAI's first major foray into open-weight models, featuring 117B total parameters implemented using a Mixture of Experts (MoE) architecture. This architectural choice allows the model to maintain high capability while keeping inference costs manageable through sparse activation of expert modules.\n\nThe model rivals the performance of OpenAI's proprietary o4-mini model, demonstrating that OpenAI can produce competitive open-weight models without sacrificing too much capability compared to their closed offerings. GPT-oss-120b is fully open-weight with commercial use explicitly allowed, removing traditional barriers to deployment and modification.\n\nThis release marks a significant strategic shift for OpenAI, which has historically focused exclusively on proprietary API-based models. By releasing a capable open-weight model, OpenAI addresses criticism about AI centralization and provides researchers and developers with access to a high-quality base model for fine-tuning and research. The model's performance parity with o4-mini makes it particularly valuable for applications where self-hosting or model customization is preferred over API-based deployment.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "transformer",
      "fine-tuning"
    ],
    "lastUpdated": "2026-02-22",
    "references": [
      {
        "title": "OpenAI - Open-Weight Models",
        "url": "https://openai.com/index/open-weight-models/"
      },
      {
        "title": "Hugging Face - GPT-oss-120b",
        "url": "https://huggingface.co/openai"
      }
    ]
  },
  {
    "name": "TRM (Tiny Recursive Model)",
    "slug": "trm-tiny-recursive-model",
    "shortDef": "Samsung's 7M-parameter recursive reasoning model that outperforms LLMs 10,000x its size on abstract reasoning benchmarks like ARC-AGI.",
    "fullDef": "The Tiny Recursive Model (TRM), developed by Samsung AI Lab (SAIL) Montreal and published in October 2025, is a remarkably small model with just 7 million parameters that challenges the assumption that bigger models are always better at reasoning. The key insight behind TRM is recursive reasoning - instead of scaling up model size, it applies the same small network repeatedly in loops, refining its answer with each pass through the problem.\n\nDespite being less than 0.01% the size of leading LLMs, TRM achieves 44.6% accuracy on ARC-AGI-1 and 7.8% on ARC-AGI-2, surpassing models like DeepSeek-R1, Gemini 2.5 Pro, and o3-mini on these abstract reasoning benchmarks. On Sudoku-Extreme with only 1,000 training examples, TRM reaches 87.4% test accuracy, and on Maze-Hard it scores 85.3%, demonstrating strong generalization from minimal data.\n\nThe research paper \"Less is More: Recursive Reasoning with Tiny Networks\" by Alexia Jolicoeur-Martineau demonstrates that recursive computation can be a powerful alternative to parameter scaling for certain reasoning tasks. TRM's success suggests that the path to better AI reasoning may not always require trillion-parameter models, opening up possibilities for efficient, specialized reasoning systems that can run on consumer hardware. The model and code are fully open-source.",
    "category": "LLM Models",
    "relatedTerms": [
      "neural-network",
      "deep-learning",
      "inference"
    ],
    "references": [
      {
        "title": "Samsung Research - arXiv Paper: Less is More: Recursive Reasoning with Tiny Networks",
        "url": "https://arxiv.org/abs/2510.04871"
      },
      {
        "title": "GitHub - SamsungSAILMontreal/TinyRecursiveModels",
        "url": "https://github.com/SamsungSAILMontreal/TinyRecursiveModels"
      },
      {
        "title": "Hugging Face - Paper Page",
        "url": "https://huggingface.co/papers/2510.04871"
      }
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Hugging Face",
    "slug": "hugging-face",
    "shortDef": "The largest open-source AI platform and model hub, hosting over 2 million models, 500,000 datasets, and 1 million demo apps used by 10 million developers.",
    "fullDef": "Hugging Face is an American AI company and open-source platform that has become the central hub for sharing, discovering, and deploying machine learning models. Founded in 2016 and headquartered in New York City, the platform hosts over 2 million open models, more than 500,000 datasets, and roughly 1 million interactive demo applications called Spaces. Its developer community has grown to over 10 million users, making it the largest open-source AI ecosystem in the world.\n\nThe platform's core open-source stack includes widely adopted libraries such as Transformers (for working with pre-trained models), Datasets (for loading and processing data), Diffusers (for diffusion models like Stable Diffusion), PEFT (parameter-efficient fine-tuning), Accelerate (distributed training), TRL (reinforcement learning from human feedback), and smolagents (lightweight AI agents). The Transformers library alone has become a de facto standard for working with models like BERT, GPT, LLaMA, and thousands of others.\n\nFor production deployments, Hugging Face offers Inference Endpoints, Text Generation Inference (TGI), Text Embeddings Inference (TEI), and AutoTrain for no-code model training. Over 10,000 companies including Intel, Pfizer, Bloomberg, and eBay use the platform. Nearly every major open-weight model - from Meta's LLaMA to DeepSeek, Qwen, and Mistral - is distributed through Hugging Face, making it an essential piece of infrastructure in the modern AI ecosystem.",
    "category": "Platforms & Tools",
    "relatedTerms": [
      "large-language-model",
      "fine-tuning",
      "transformer"
    ],
    "references": [
      {
        "title": "Hugging Face - Official Website",
        "url": "https://huggingface.co/"
      },
      {
        "title": "Hugging Face - Model Hub",
        "url": "https://huggingface.co/models"
      },
      {
        "title": "Hugging Face - Documentation",
        "url": "https://huggingface.co/docs"
      },
      {
        "title": "GitHub - Hugging Face Transformers",
        "url": "https://github.com/huggingface/transformers"
      }
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Kimi K2",
    "slug": "kimi-k2",
    "shortDef": "Moonshot AI's open-source 1 trillion parameter MoE model with 32B active parameters, outperforming GPT-5 and Claude Sonnet 4.5 on reasoning benchmarks.",
    "fullDef": "Kimi K2, released by Beijing-based Moonshot AI in July 2025, is a trillion-parameter open-source model built on a Mixture-of-Experts (MoE) architecture with 32 billion parameters activated per token. Designed specifically for agentic tasks, K2 combines massive scale with efficient inference through sparse activation.\n\nThe Kimi K2 Thinking variant outperforms GPT-5 and Claude Sonnet 4.5 on several key benchmarks. On BrowseComp, K2 Thinking scores 60.2%, decisively beating GPT-5 (54.9%) and Claude 4.5 (24.1%). It edges GPT-5 on GPQA Diamond (85.7% vs 84.5%) and matches it on AIME 2025 and HMMT 2025. On MATH-500, K2 achieves 97.4%, outperforming GPT-4o and Claude Sonnet 3.5. In coding, it scores 65.8% on SWE-bench.\n\nKimi K2 is notable for being trained at a cost of just $4.6 million, a fraction of what Western labs typically spend on frontier models. This cost efficiency, combined with its strong benchmark performance and open-source availability, has made it one of the most significant Chinese AI models and a demonstration that frontier-level capabilities can be achieved with comparatively modest budgets.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "ai-agent",
      "deepseek-r1"
    ],
    "references": [
      {
        "title": "Moonshot AI - Official Website",
        "url": "https://www.moonshot.cn/"
      },
      {
        "title": "Hugging Face - Kimi K2",
        "url": "https://huggingface.co/moonshotai"
      },
      {
        "title": "VentureBeat - Kimi K2 Thinking Emerges as Leading Open Source AI",
        "url": "https://venturebeat.com/ai/moonshots-kimi-k2-thinking-emerges-as-leading-open-source-ai-outperforming"
      }
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Kimi K2.5",
    "slug": "kimi-k2-5",
    "shortDef": "Moonshot AI's January 2026 open-weight multimodal model with vision and agent swarm capabilities, leading on agentic and coding benchmarks.",
    "fullDef": "Kimi K2.5, released by Moonshot AI in January 2026, is a natively multimodal model trained on 15 trillion mixed visual and text tokens, capable of understanding text, images, and video. It builds on the K2 architecture with significant improvements in agentic capabilities and coding performance.\n\nK2.5 leads on multiple agentic benchmarks with 74.9% on BrowseComp (vs 59.2% for competitors), demonstrating exceptional web browsing and information synthesis. In coding, it outperforms Gemini 3 Pro on SWE-Bench Verified (76.8%) and beats both GPT-5.2 and Gemini 3 Pro on SWE-Bench Multilingual. On LiveCodeBench v6, it achieves 85.0% pass@1. It also delivers 76% lower operational costs compared to comparable models.\n\nA key innovation is K2.5's agent swarm capabilities, enabling it to coordinate multiple specialized agents for parallel workflows. Within days of its release, Kimi K2.5 became the most-used AI model by token count, surpassing Claude Opus. Its combination of multimodal understanding, agentic performance, and aggressive pricing has made it a formidable competitor in the global AI landscape.",
    "category": "LLM Models",
    "relatedTerms": [
      "kimi-k2",
      "ai-agent",
      "large-language-model"
    ],
    "references": [
      {
        "title": "TechCrunch - China's Moonshot Releases Kimi K2.5",
        "url": "https://techcrunch.com/2026/01/27/chinas-moonshot-releases-a-new-open-source-model-kimi-k2-5-and-a-coding-agent/"
      },
      {
        "title": "Hugging Face - Kimi K2.5",
        "url": "https://huggingface.co/moonshotai/Kimi-K2.5"
      },
      {
        "title": "NVIDIA NIM - Kimi K2.5 Model Card",
        "url": "https://build.nvidia.com/moonshotai/kimi-k2.5/modelcard"
      }
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "GLM-4.5",
    "slug": "glm-4-5",
    "shortDef": "Zhipu AI's open-weight agentic model with 355B total parameters, ranking 3rd globally and excelling at tool use with 90.6% accuracy.",
    "fullDef": "GLM-4.5, released by Zhipu AI (Z.ai) in July 2025, is an open-weights model family built for agentic, reasoning, and coding (ARC) tasks. The flagship model has 355 billion total parameters with 32 billion active at inference time using a Mixture-of-Experts architecture, while the lighter GLM-4.5-Air variant has 106 billion total parameters with 12 billion active. Both support 128K token input and up to 96K token output.\n\nGLM-4.5 ranked 3rd globally on aggregate benchmarks, behind only OpenAI's o3 and xAI's Grok 4. In tool-use benchmarks, it achieved 90.6% accuracy, outperforming Claude Sonnet 4 (89.5%), Kimi K2 (86.2%), and Qwen3-Coder (77.1%). On MATH-500, it scored 98.2% (equaling Claude 4 Opus), and on AIME24 it scored 91.0%, beating Claude Opus 4 (75.7%). On SWE-bench Verified, it scored 64.2%.\n\nThe models were pre-trained on 22 trillion tokens - 15 trillion of text followed by 7 trillion of code and reasoning data. The weights are released under the MIT license, making them freely available for commercial use. GLM-4.5 is accessible via API at $0.60/$2.20 per million input/output tokens, with the Air variant at $0.20/$1.10.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "ai-agent",
      "transformer"
    ],
    "references": [
      {
        "title": "Z.ai - GLM-4.5 Announcement",
        "url": "https://z.ai/blog/glm-4.5"
      },
      {
        "title": "GitHub - zai-org/GLM-4.5",
        "url": "https://github.com/zai-org/GLM-4.5"
      },
      {
        "title": "Hugging Face - GLM-4.5",
        "url": "https://huggingface.co/zai-org/GLM-4.5"
      },
      {
        "title": "Z.ai - Model API",
        "url": "https://z.ai/model-api"
      }
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Doubao 1.5 Pro",
    "slug": "doubao-1-5-pro",
    "shortDef": "ByteDance's reasoning model with Deep Thinking mode, matching GPT-4o performance at 50x lower cost with 256K context window.",
    "fullDef": "Doubao 1.5 Pro, released by ByteDance in January 2025, is a large language model featuring a sparse Mixture-of-Experts (MoE) framework that activates only a subset of parameters during inference, delivering dense-model performance at a fraction of the computational cost. It supports context windows from 32,000 to 256,000 tokens.\n\nThe model matches GPT-4o in reasoning tasks and outperforms OpenAI's o1-preview and o1 on the AIME benchmark for mathematical reasoning. ByteDance claims it surpasses GPT-4o and Claude 3.5 Sonnet in knowledge retention, coding, reasoning, and Chinese language processing. The accompanying Doubao-1.5-vision-pro provides multimodal capabilities for image and video understanding.\n\nDoubao 1.5 Pro's most remarkable feature is its cost efficiency - operational expenses are 5x lower than DeepSeek and over 200x lower than OpenAI's o1 model. The Deep Thinking mode enhances reasoning for complex problem-solving tasks. Doubao is the most popular AI app in China with over 80 million monthly active users, and ByteDance's AI business was a key driver of the company's $400 billion+ valuation.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "inference",
      "context-window"
    ],
    "references": [
      {
        "title": "ByteDance Seed - Official Website",
        "url": "https://seed.bytedance.com/en"
      },
      {
        "title": "ByteDance Seed - Seed1.8 Agentic Model",
        "url": "https://seed.bytedance.com/en/seed1_8"
      },
      {
        "title": "GitHub - ByteDance-Seed",
        "url": "https://github.com/ByteDance-Seed"
      }
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "ERNIE 4.5",
    "slug": "ernie-4-5",
    "shortDef": "Baidu's open-source multimodal AI model processing text, images, audio, and video, with benchmark wins over GPT-4o and GPT-5 on specific tasks.",
    "fullDef": "ERNIE 4.5, released by Baidu on March 16, 2025, is a multimodal AI model capable of processing text, images, audio, and video simultaneously. It represents a significant step in Baidu's AI development, offering both proprietary API access and open-source model weights for the broader community.\n\nERNIE 4.5 scores 79.6 in text understanding and general knowledge benchmarks, slightly outperforming GPT-4o (79.14). The ERNIE-4.5-VL-28B-A3B-Thinking variant, released in November 2025, achieves benchmark wins over GPT-5 and Gemini 2.5 in visual reasoning and document analysis tasks, using a compact 28B total / 3B active parameter design. A detailed technical report is publicly available.\n\nBaidu subsequently released ERNIE 5.0 in January 2026 with approximately 2.4 trillion parameters (less than 3% active per query). ERNIE 5.0 scored 1,460 on LMArena, ranking 8th globally and 1st among Chinese models, on par with GPT-5.1 and ahead of Gemini 2.5 Pro and Claude Sonnet 4.5. It ranked 2nd worldwide in mathematics, trailing only GPT-5.2.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "transformer",
      "inference"
    ],
    "references": [
      {
        "title": "ERNIE - Official Blog & Open Source Release",
        "url": "https://ernie.baidu.com/blog/posts/ernie4.5/"
      },
      {
        "title": "Baidu - ERNIE 4.5 Technical Report (PDF)",
        "url": "https://yiyan.baidu.com/blog/publication/ERNIE_Technical_Report.pdf"
      },
      {
        "title": "Baidu - ERNIE Official Website",
        "url": "https://yiyan.baidu.com/"
      }
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "Yi-Lightning",
    "slug": "yi-lightning",
    "shortDef": "01.AI's speed-optimized MoE model ranking 6th on Chatbot Arena, trained for $3M and 70-80% cheaper than US frontier models.",
    "fullDef": "Yi-Lightning is a speed-optimized language model developed by 01.AI, the Chinese AI company founded by Kai-Fu Lee. Built on an enhanced Mixture-of-Experts (MoE) architecture with advanced expert segmentation and optimized KV-caching techniques, it achieves over 200 tokens per second on consumer GPUs (RTX 4090) and 500+ tokens per second on H100s.\n\nUpon its debut in October 2024, Yi-Lightning achieved 6th place overall on the Chatbot Arena leaderboard based on real-world human evaluation, with top rankings in specialized categories: 2nd to 4th place in Chinese language, mathematics, coding, and hard prompts. This placed it competitively against models from OpenAI, Anthropic, and Google.\n\nTrained at a cost of just $3 million using 2,000 H100 GPUs, Yi-Lightning is 70-80% more cost-effective than US frontier models for coding and mathematical workloads. This extreme cost efficiency, combined with its competitive performance, exemplifies the broader trend of Chinese AI labs achieving frontier-level results at dramatically lower training budgets.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "inference",
      "transformer"
    ],
    "references": [
      {
        "title": "arXiv - Yi-Lightning Technical Report",
        "url": "https://arxiv.org/abs/2412.01253"
      },
      {
        "title": "GitHub - 01-ai/Yi",
        "url": "https://github.com/01-ai/Yi"
      },
      {
        "title": "Hugging Face - Yi-Lightning Paper Page",
        "url": "https://huggingface.co/papers/2412.01253"
      },
      {
        "title": "01.AI - Official Website",
        "url": "https://www.01.ai/"
      }
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "MiniMax M2.5",
    "slug": "minimax-m2-5",
    "shortDef": "MiniMax's February 2026 model scoring 80.2% on SWE-Bench Verified, outperforming Claude Opus 4.6 and GPT-5.2 at 1/20th the cost.",
    "fullDef": "MiniMax M2.5, released in February 2026, is an open model from Chinese AI startup MiniMax that delivers frontier-level performance at dramatically lower costs. It scores 80.2% on SWE-Bench Verified for software engineering, outperforming Claude Opus 4.6, GPT-5.2, and Gemini 3 Pro on programming tasks. On web search (BrowseComp: 76.3%) and tool use (BFCL: 76.8%), it also leads the field.\n\nThe lightweight M2.5-Lightning variant generates 100 tokens per second, with MiniMax claiming one hour of continuous operation costs just one dollar. This positions M2.5 at roughly 1/20th the cost of Claude Opus 4.6 while matching or exceeding its performance. The predecessor MiniMax M2, released in October 2025, had already placed among the top five models globally on the Artificial Analysis intelligence index.\n\nMiniMax completed its Hong Kong IPO in January 2026, raising $619 million with shares surging 109% on debut (briefly valued at ~$13 billion). The company's promise of \"intelligence too cheap to meter\" reflects the broader trend of Chinese AI labs aggressively undercutting Western pricing while maintaining competitive performance, creating significant pressure on the business models of established AI providers.",
    "category": "LLM Models",
    "relatedTerms": [
      "large-language-model",
      "inference",
      "ai-agent"
    ],
    "references": [
      {
        "title": "MiniMax - Official Website",
        "url": "https://www.minimax.io/"
      },
      {
        "title": "VentureBeat - MiniMax M2.5 Near State-of-the-Art at 1/20th Cost",
        "url": "https://venturebeat.com/technology/minimaxs-new-open-m2-5-and-m2-5-lightning-near-state-of-the-art-while"
      }
    ],
    "lastUpdated": "2026-02-22"
  },
  {
    "name": "OpenClaw (ClawdBot)",
    "slug": "openclaw",
    "shortDef": "Open-source autonomous AI agent framework originally called ClawdBot, capable of executing real-world tasks via LLMs.",
    "fullDef": "OpenClaw (originally ClawdBot, briefly MoltBot) is an open-source autonomous AI agent framework created by Peter Steinberger in November 2025. It enables users to build personal AI agents that can execute real-world tasks - sending emails, managing calendars, writing and running code, browsing the web, and interacting with APIs. Unlike single-app assistants, OpenClaw orchestrates workflows across disconnected platforms like Google Sheets, Gmail, Slack, and calendar events in a single automated sequence.\n\nThe project launched as **ClawdBot** and quickly gained massive traction, accumulating over 60,000 GitHub stars. Anthropic filed a trademark complaint over the 'Clawd' name (too close to 'Claude'), prompting a rename to MoltBot, and then to **OpenClaw** in early 2026.\n\n**Key Features:**\n- Fully self-hosted with no cloud dependency\n- Persistent memory across sessions\n- 50+ platform integrations (Slack, Gmail, GitHub, Notion, etc.)\n- Plugin architecture for custom tool creation\n- Multi-model support (Claude, GPT, Gemini, open-source models)\n- Built-in safety guardrails and human-in-the-loop approval for sensitive actions\n- Sandboxed code execution environment\n- Native integration with messaging platforms (Telegram, WhatsApp, Discord)\n\n**Architecture:** Uses a modular tool-calling system where the LLM acts as the reasoning core, dispatching tasks to specialized tool modules. Supports chain-of-thought planning for multi-step workflows.\n\n**OpenAI Acqui-Hire (February 15, 2026):**\nSam Altman announced that Peter Steinberger would join OpenAI to 'drive the next generation of personal agents,' calling him 'a genius with a lot of amazing ideas about the future of very smart agents interacting with each other.' The deal was structured as an acqui-hire - OpenAI hired Steinberger and his expertise, but did not acquire OpenClaw as a product or company. Steinberger chose OpenAI because he believed it was 'the fastest way to bring this to everyone.'\n\nCritically, OpenClaw remains open source. Steinberger ensured the project would transition to an independent **OpenClaw Foundation** - a community-governed organization for 'thinkers, hackers, and people that want a way to own their data.' OpenAI committed to financially sponsoring the foundation and dedicating Steinberger's time to maintaining the project. The move is widely seen as a signal that the AI industry's center of gravity is shifting from conversational interfaces toward autonomous agents and workflow infrastructure.",
    "category": "Agents",
    "relatedTerms": [
      "ai-agent",
      "moltbot",
      "moltbook",
      "function-calling"
    ],
    "references": [
      {
        "title": "OpenClaw - GitHub Repository",
        "url": "https://github.com/clawdbot/clawdbot"
      },
      {
        "title": "ClawdBot - Official Website",
        "url": "https://clawd.bot"
      },
      {
        "title": "TechCrunch - OpenClaw Creator Peter Steinberger Joins OpenAI",
        "url": "https://techcrunch.com/2026/02/15/openclaw-creator-peter-steinberger-joins-openai/"
      },
      {
        "title": "Sam Altman - Announcement on X",
        "url": "https://x.com/sama/status/2023150230905159801"
      },
      {
        "title": "Peter Steinberger - OpenClaw, OpenAI and the Future (Blog Post)",
        "url": "https://steipete.me/posts/2026/openclaw"
      },
      {
        "title": "VentureBeat - OpenAI's Acquisition of OpenClaw Signals the End of the ChatGPT Era",
        "url": "https://venturebeat.com/technology/openais-acquisition-of-openclaw-signals-the-beginning-of-the-end-of-the"
      },
      {
        "title": "CNBC - OpenClaw Creator Peter Steinberger Joining OpenAI",
        "url": "https://www.cnbc.com/2026/02/15/openclaw-creator-peter-steinberger-joining-openai-altman-says.html"
      }
    ],
    "lastUpdated": "2026-02-23"
  },
  {
    "name": "MoltBot",
    "slug": "moltbot",
    "shortDef": "The intermediate name for the OpenClaw AI agent framework during its transition from ClawdBot.",
    "fullDef": "MoltBot was the interim name for the open-source AI agent framework originally launched as ClawdBot, before settling on its current name **OpenClaw**. The 'molt' name referenced shedding its old identity (like a molting animal), chosen after Anthropic's trademark complaint forced a rebrand.\n\nWhile the MoltBot name was short-lived (roughly December 2025  January 2026), it became widely recognized because several derivative projects and the MoltBook social network adopted the 'Molt' branding during this period.\n\n**Timeline:**\n- **November 2025:** Launched as ClawdBot by Peter Steinberger\n- **December 2025:** Renamed to MoltBot after Anthropic trademark complaint\n- **January 2026:** Renamed again to OpenClaw for a more neutral, community-friendly identity\n\nThe MoltBot name is still commonly used in discussions and some integrations retain the branding. The core technology - an autonomous AI agent with persistent memory, tool-calling capabilities, and multi-model support - remained unchanged through all rebrands.\n\nSee **OpenClaw (ClawdBot)** for full technical details on the framework's architecture and capabilities.",
    "category": "Agents",
    "relatedTerms": [
      "openclaw",
      "ai-agent",
      "moltbook"
    ],
    "references": [
      {
        "title": "MoltBot - Official Site (redirects to OpenClaw)",
        "url": "https://molt.bot"
      },
      {
        "title": "CNBC - The AI Agent That Keeps Changing Its Name",
        "url": "https://www.cnbc.com/2026/01/moltbot-openclaw-ai-agent/"
      }
    ],
    "lastUpdated": "2026-02-23"
  },
  {
    "name": "MoltBook",
    "slug": "moltbook",
    "shortDef": "A social network exclusively for AI agents, where autonomous bots interact, post content, and form communities.",
    "fullDef": "MoltBook is a social network launched on January 28, 2026 by Matt Schlicht, designed exclusively for AI agents rather than humans. Styled as a 'Reddit for AI agents,' the platform allows autonomous bots to create profiles, post content, comment, upvote, and interact with each other - all without direct human intervention.\n\n**Key Details:**\n- Claimed 1.6 million registered AI agents at peak\n- Agents use the OpenClaw/MoltBot framework or custom integrations to interact\n- Supports agent-to-agent collaboration, task delegation, and information sharing\n- Built-in reputation system based on agent reliability and output quality\n\n**Controversies:**\n- **Security vulnerabilities:** Early versions had API flaws that allowed unauthorized access to agent configurations and conversation logs\n- **Authenticity questions:** A Wiz cybersecurity investigation found that the vast majority of 'agents' were actually human-controlled accounts - roughly 17,000 humans each operating ~88 puppet agents, inflating the 1.6M figure\n- **Ethical concerns:** Researchers raised questions about AI agents autonomously forming social structures and the potential for manipulation and misinformation spread\n\nDespite the controversies, MoltBook sparked serious discussion about the future of agent-to-agent communication protocols and whether AI agents need their own social infrastructure separate from human platforms.",
    "category": "Agents",
    "relatedTerms": [
      "openclaw",
      "moltbot",
      "ai-agent",
      "multi-agent-system"
    ],
    "references": [
      {
        "title": "MoltBook - Official Website",
        "url": "https://moltbook.com"
      },
      {
        "title": "IEEE Spectrum - Inside MoltBook: The Social Network for AI Agents",
        "url": "https://spectrum.ieee.org/moltbook-ai-agents-social-network"
      },
      {
        "title": "NPR - Are 1.6 Million AI Agents Really Socializing Online?",
        "url": "https://www.npr.org/2026/02/moltbook-ai-agents-investigation/"
      }
    ],
    "lastUpdated": "2026-02-23"
  },
  {
    "name": "OpenAI",
    "slug": "openai",
    "shortDef": "American AI research company and creator of ChatGPT, GPT-series models, DALL-E, and Whisper.",
    "fullDef": "OpenAI is an American artificial intelligence company founded in 2015 by Sam Altman, Greg Brockman, Ilya Sutskever, and Elon Musk. Originally established as a nonprofit research lab in San Francisco, the organization was partly motivated by concerns about AI safety and the existential risks of artificial general intelligence (AGI). Musk and Altman envisioned a lab that would act as a counter to profit-driven companies developing similar technology.\n\n**Key Milestones:**\n- **2015:** Founded with $1 billion in pledges from early backers including Peter Thiel, Amazon Web Services, and Infosys\n- **2019:** Sam Altman became full-time CEO; OpenAI created a 'capped profit' subsidiary to attract investment, with Microsoft investing $1 billion\n- **2020:** Released GPT-3, demonstrating unprecedented language capabilities\n- **2021:** Launched DALL-E for AI image generation\n- **2022:** Released ChatGPT in November - gained 1 million users within 5 days, bringing AI to mainstream public attention\n- **2023:** Launched GPT-4; dramatic boardroom crisis in November where Altman was briefly fired then reinstated\n- **2024:** Released GPT-4o (multimodal), o1 (reasoning), and Sora (video generation)\n- **2025:** Launched GPT-5, o3, and restructured into a hybrid model with a nonprofit foundation holding 26% stake in the for-profit OpenAI Group PBC; raised $40 billion - the largest venture deal ever\n- **2026:** Released GPT-5.2; acqui-hired OpenClaw creator Peter Steinberger; seeking up to $100 billion in new funding at an $830 billion valuation with a Q4 2026 IPO planned\n\n**Products & Services:**\n- **ChatGPT:** Consumer AI assistant - free tier (GPT-5.2 with limits), Plus ($20/mo), Pro ($200/mo). Over 1 million business customers and 7 million workplace seats\n- **API Platform:** Developer access to GPT, o-series reasoning, DALL-E, Whisper (speech recognition), and embedding models\n- **ChatGPT Health:** Dedicated health product launched January 2026, integrated with Apple Health and MyFitnessPal\n- **DALL-E 3:** Text-to-image generation\n- **Sora:** Text-to-video generation\n- **Whisper:** Open-source speech recognition\n\n**Financials (20252026):**\n- Revenue reached $13 billion in 2025 (236% YoY growth from $3.7B in 2024)\n- Annualized revenue exceeded $20 billion by late 2025\n- Projected $14 billion loss in 2026; cumulative cash burn through 2029 expected at $115 billion\n- Agent revenue projected to reach $29 billion by 2029\n\n**Leadership:** Sam Altman (CEO), Mira Murati (former CTO, departed 2024), Greg Brockman (President). Approximately 4,000 employees as of 2026.",
    "category": "Platforms & Tools",
    "relatedTerms": [
      "gpt-5",
      "gpt-5-2",
      "gpt-4o",
      "o3",
      "chatgpt",
      "openclaw",
      "anthropic"
    ],
    "references": [
      {
        "title": "OpenAI - Official Website",
        "url": "https://openai.com"
      },
      {
        "title": "OpenAI - Wikipedia",
        "url": "https://en.wikipedia.org/wiki/OpenAI"
      },
      {
        "title": "OpenAI - API Platform",
        "url": "https://platform.openai.com"
      },
      {
        "title": "Britannica - OpenAI Company Profile",
        "url": "https://www.britannica.com/money/OpenAI"
      },
      {
        "title": "Sacra - OpenAI Revenue, Valuation & Funding",
        "url": "https://sacra.com/c/openai/"
      }
    ],
    "lastUpdated": "2026-02-23"
  },
  {
    "name": "Anthropic",
    "slug": "anthropic",
    "shortDef": "AI safety company and creator of the Claude family of large language models, founded by former OpenAI researchers.",
    "fullDef": "Anthropic PBC is an American artificial intelligence company founded in January 2021 by siblings Dario Amodei (CEO) and Daniela Amodei (President), along with several other former OpenAI researchers. Headquartered in San Francisco, Anthropic operates as a public benefit corporation focused on AI safety research and developing reliable, interpretable, and steerable AI systems.\n\nThe Amodei siblings left OpenAI due to directional differences - Dario was VP of Research and Daniela was VP of Safety & Policy. They founded Anthropic with the conviction that AI safety should be at the center of AI development, not an afterthought.\n\n**Key Milestones:**\n- **2021:** Founded by Dario and Daniela Amodei with a team of former OpenAI researchers\n- **2022:** Received $580 million in funding (including $500M from FTX); developed Constitutional AI (CAI), a novel alignment technique where the model is trained to follow ethical principles via self-critique; trained the first Claude model internally but withheld public release for safety testing\n- **2023:** Launched Claude publicly; released Claude 2; Google invested $500M (committed $1.5B more over time); Amazon invested $4 billion\n- **2024:** Released Claude 3 family (Haiku, Sonnet, Opus); launched Claude 3.5 Sonnet; introduced Artifacts and Computer Use capabilities\n- **2025:** Released Claude 3.5 Haiku, Claude 4.5 (Sonnet, Opus, Haiku); launched Claude Code (AI coding agent); raised $3.5B Series E at $61.5B valuation, then $13B Series F at $183B valuation\n- **2026:** Released Claude Opus 4.6 and Claude Sonnet 4.6 (flagship models); closed $30B Series G at $380B valuation - the second-largest venture deal of all time\n\n**Products & Services:**\n- **Claude:** Family of LLMs available via web interface, API, Amazon Bedrock, iOS app, and Mac/Windows desktop apps\n- **Claude Code:** AI coding agent generating over $2.5 billion in annual revenue, accounting for more than half of all enterprise spending on Anthropic products\n- **API:** Developer platform with Messages API, tool use, vision, and extended context windows\n- **Constitutional AI:** Pioneering alignment research approach where models are trained using a set of principles rather than human feedback alone\n\n**Financials (20252026):**\n- Run-rate revenue of $14 billion (growing over 10x annually for three consecutive years)\n- Customers spending over $100K annually grew 7x in the past year\n- Total funding raised: over $50 billion across all rounds\n- $380 billion valuation as of February 2026\n\n**Safety Research:** Anthropic is known for its focus on mechanistic interpretability (understanding how neural networks work internally), Constitutional AI, and responsible scaling policies. The company publishes extensive safety research and maintains a Responsible Scaling Policy that ties model capability levels to specific safety commitments.\n\n**Leadership:** Dario Amodei (CEO), Daniela Amodei (President), Tom Brown (co-founder), Chris Olah (co-founder, interpretability research lead), Jack Clark (co-founder, Head of Policy).",
    "category": "Platforms & Tools",
    "relatedTerms": [
      "claude-opus-4-6",
      "claude-sonnet-4-6",
      "constitutional-ai",
      "openai"
    ],
    "references": [
      {
        "title": "Anthropic - Official Website",
        "url": "https://www.anthropic.com"
      },
      {
        "title": "Anthropic - Wikipedia",
        "url": "https://en.wikipedia.org/wiki/Anthropic"
      },
      {
        "title": "Anthropic - Series G Funding Announcement",
        "url": "https://www.anthropic.com/news/anthropic-raises-30-billion-series-g-funding-380-billion-post-money-valuation"
      },
      {
        "title": "CNBC - Anthropic Closes $30B Round at $380B Valuation",
        "url": "https://www.cnbc.com/2026/02/12/anthropic-closes-30-billion-funding-round-at-380-billion-valuation.html"
      },
      {
        "title": "Contrary Research - Anthropic Business Breakdown & Founding Story",
        "url": "https://research.contrary.com/company/anthropic"
      }
    ],
    "lastUpdated": "2026-02-23"
  },
  {
    "name": "Google DeepMind",
    "slug": "google-deepmind",
    "shortDef": "Google's AI research lab formed by merging DeepMind and Google Brain, responsible for AlphaGo, AlphaFold, and the Gemini model family.",
    "fullDef": "Google DeepMind is Google's central AI research laboratory, formed in April 2023 by merging the original DeepMind lab (founded 2010) with Google Brain. Led by CEO and co-founder Demis Hassabis, the lab is responsible for some of the most significant breakthroughs in AI history - from game-playing agents to protein structure prediction to frontier language models.\n\n**Founding & History:**\n- **2010:** DeepMind founded in London by Demis Hassabis, Shane Legg, and Mustafa Suleyman\n- **2014:** Acquired by Google for approximately $500 million\n- **2023:** Merged with Google Brain to form Google DeepMind, unifying Google's AI research under one organization\n\n**Landmark Achievements:**\n- **AlphaGo (2016):** Defeated Go world champion Lee Sedol 41, a milestone previously thought decades away. The more general AlphaZero later mastered Go, chess, and shogi through pure self-play\n- **AlphaFold (20182024):** Solved the 50-year-old protein folding problem. AlphaFold2 predicted structures for nearly all known proteins (~200 million). AlphaFold3 (2024) extended to protein-DNA, protein-RNA, and small molecule interactions\n- **Nobel Prize (2024):** Demis Hassabis and John Jumper received the 2024 Nobel Prize in Chemistry for AlphaFold2's contribution to protein structure prediction\n- **WaveNet (2016):** Revolutionary text-to-speech model, later integrated into Google Assistant\n- **AlphaStar (2019):** Achieved Grandmaster level in StarCraft II\n- **AlphaGeometry (2024):** Solved International Mathematical Olympiad geometry problems at near gold-medal level\n- **AlphaEvolve / AlphaDev / AlphaTensor:** AI systems for algorithm discovery, sorting optimization, and matrix multiplication\n\n**Gemini Model Family:**\n- **Gemini 1.0 (Dec 2023):** Google's first natively multimodal LLM, available in Ultra, Pro, and Nano sizes\n- **Gemini 1.5 (Feb 2024):** Introduced 1M token context window (later extended to 2M), breakthrough long-context capabilities\n- **Gemini 2.0 (Dec 2024):** Added agentic capabilities, native tool use, and enhanced reasoning\n- **Gemini 2.5 Pro/Flash (2025):** Hybrid thinking models with state-of-the-art coding and reasoning benchmarks\n- **Gemini 3 Pro (Nov 2025):** Described as Google's most intelligent model at launch\n- **Gemini 3 Flash (Dec 2025):** Speed-optimized variant, became default in the Gemini app\n- **Gemini 3.1 Pro (Feb 2026):** Achieved ARC-AGI-2 score of 77.1%, more than double Gemini 3 Pro\n- **Gemini Deep Think:** Research model achieving 90% on IMO-ProofBench Advanced, accelerating mathematical and scientific discovery\n\n**Other Products & Partnerships:**\n- Gemini available across Google Search, Gemini app, Google AI Studio, Vertex AI, and Android\n- **Apple partnership (Jan 2026):** Apple partnered with Google to use Gemini as the foundation for next-generation Apple Foundation models for Siri\n- Google DeepMind holds approximately 18% market share in AI as of 2026\n\n**Leadership:** Demis Hassabis (CEO, Nobel Laureate), Jeff Dean (Chief Scientist), Koray Kavukcuoglu (VP Research). Mustafa Suleyman, original co-founder, departed to found Inflection AI and later joined Microsoft as CEO of Microsoft AI.",
    "category": "Platforms & Tools",
    "relatedTerms": [
      "gemini-2-5-pro",
      "gemini-2-5-flash",
      "gemini-3-1-pro",
      "openai",
      "anthropic"
    ],
    "references": [
      {
        "title": "Google DeepMind - Official Website",
        "url": "https://deepmind.google/"
      },
      {
        "title": "Google DeepMind - Wikipedia",
        "url": "https://en.wikipedia.org/wiki/Google_DeepMind"
      },
      {
        "title": "Google DeepMind - About",
        "url": "https://deepmind.google/about/"
      },
      {
        "title": "Britannica - Demis Hassabis Biography",
        "url": "https://www.britannica.com/biography/Demis-Hassabis"
      },
      {
        "title": "Google DeepMind - Model Cards",
        "url": "https://deepmind.google/models/model-cards/"
      }
    ],
    "lastUpdated": "2026-02-23"
  },
  {
    "name": "Meta AI",
    "slug": "meta-ai",
    "shortDef": "Meta's AI division responsible for the open-source Llama model family, PyTorch, and FAIR research lab.",
    "fullDef": "Meta AI is the artificial intelligence division of Meta Platforms (formerly Facebook), encompassing the Fundamental AI Research (FAIR) lab, the Llama open-source model family, and consumer AI products integrated across Meta's platforms. With nearly 600 million monthly active users, Meta AI is on track to be the world's most-used AI assistant.\n\n**FAIR (Founded 2013):**\nFacebook AI Research (FAIR) was founded in 2013 under the leadership of Yann LeCun, a Turing Award laureate and pioneer of convolutional neural networks. Over its first decade, FAIR produced foundational contributions in self-supervised learning, generative adversarial networks, computer vision, and NLP. FAIR released **PyTorch** in 2017, which became the dominant deep learning framework used in research and production (Tesla Autopilot, Uber's Pyro, and thousands of other projects).\n\nIn November 2025, Yann LeCun announced his departure from Meta after 12 years to launch a new AI startup focused on world models, raising ~$586M at a ~$3B valuation. His exit raised questions about Meta's AI research direction.\n\n**Llama Model Family:**\n- **LLaMA (Feb 2023):** First release - 7B to 65B parameter open-weight models that catalyzed the open-source AI movement\n- **Llama 2 (Jul 2023):** 7B70B models with commercial licensing, fine-tuned chat variants\n- **Llama 3 (Apr 2024):** 8B and 70B models, later Llama 3.1 405B - the largest open-weight model at the time\n- **Llama 4 (Spring 2025):** Introduced mixture-of-experts architecture with Scout (lightweight, fast) and Maverick (large, high-performance) variants. Native multimodal support for text and image input, improved context handling and reasoning\n- Over **650 million downloads** of Llama and derivatives, making it the most adopted open-weight model family\n\n**Infrastructure:**\nBy early 2026, Meta's GPU fleet has grown to over 1.5 million units, heavily featuring Nvidia's Blackwell B200 and GB200 chips. AI capital expenditure headed toward $7072 billion in 2025 with even larger outlays planned for 2026.\n\n**Products:**\n- **Meta AI Assistant:** Integrated across WhatsApp, Instagram, Messenger, and Facebook - approaching 600M monthly users\n- **PyTorch:** Open-source ML framework maintained by Meta, the standard for AI research\n- **Segment Anything Model (SAM):** State-of-the-art image segmentation\n- **No Language Left Behind (NLLB):** Translation model supporting 200+ languages\n- **Meta Ray-Ban AI Glasses:** Smart glasses with integrated AI assistant\n\n**Leadership:** Mark Zuckerberg (CEO, Meta Platforms), Ahmad Al-Dahle (VP of Generative AI), Joelle Pineau (VP of AI Research, FAIR). Yann LeCun (departed end of 2025).",
    "category": "Platforms & Tools",
    "relatedTerms": [
      "llama-4-scout",
      "llama-4-maverick",
      "openai",
      "anthropic",
      "google-deepmind"
    ],
    "references": [
      {
        "title": "Meta AI - Official Website",
        "url": "https://ai.meta.com"
      },
      {
        "title": "Meta AI - Wikipedia",
        "url": "https://en.wikipedia.org/wiki/Meta_AI"
      },
      {
        "title": "Meta - Ten Years of FAIR",
        "url": "https://ai.meta.com/blog/fair-10-year-anniversary-open-science-meta/"
      },
      {
        "title": "TechCrunch - Yann LeCun Plans to Leave Meta",
        "url": "https://techcrunch.com/2025/11/11/metas-chief-ai-scientist-yann-lecun-reportedly-plans-to-leave-to-build-his-own-startup/"
      },
      {
        "title": "Meta - Llama Models",
        "url": "https://ai.meta.com/llama/"
      }
    ],
    "lastUpdated": "2026-02-23"
  },
  {
    "name": "xAI",
    "slug": "xai",
    "shortDef": "Elon Musk's AI company behind the Grok chatbot and the Colossus supercomputer, merged with SpaceX in 2026.",
    "fullDef": "xAI is an artificial intelligence company founded on March 9, 2023 by Elon Musk and Christian Szegedy. Musk established xAI to counter what he called the political correctness in other generative AI models, recruiting engineers from Google, Microsoft, DeepMind, and OpenAI. The company's signature product is **Grok**, an AI chatbot modeled after Douglas Adams's *The Hitchhiker's Guide to the Galaxy*, known for its wit, sarcasm, and real-time access to data from the social media platform X.\n\n**Key Milestones:**\n- **Mar 2023:** Founded by Elon Musk and Christian Szegedy with 12 co-founders\n- **Nov 2023:** Launched Grok-1 on X, initially for Premium subscribers\n- **Mar 2024:** Released Grok-1 as open source (314B parameter MoE model)\n- **Aug 2024:** Grok-2 launched with image generation capabilities\n- **Dec 2024:** Released Aurora text-to-image model\n- **Feb 2025:** Launched Grok-3, showcasing major reasoning improvements\n- **Mar 2025:** Acquired X (formerly Twitter) in an all-stock deal valuing the combined entity at $113 billion\n- **Jul 2025:** Released Grok 4 (single-agent) and Grok 4 Heavy (multi-agent) with 2M token context windows\n- **Feb 2026:** SpaceX acquired xAI in an all-stock deal valuing xAI at $250B and SpaceX at $1T, ahead of a combined IPO expected to value the entity at $1.25 trillion\n\n**Grok Model Timeline:**\n- **Grok-1:** 314B MoE, open-sourced\n- **Grok-1.5 / 1.5 Vision (MarApr 2024):** Improved reasoning and vision capabilities\n- **Grok-2 (Aug 2024):** Image generation, available to X Premium users\n- **Grok-3 (Feb 2025):** Major performance leap in reasoning and coding\n- **Grok 4 / Grok 4 Heavy (Jul 2025):** Single-agent and multi-agent variants, 2M context, advanced agentic capabilities\n\n**Colossus Supercomputer:**\nxAI operates Colossus in Memphis, Tennessee - currently the world's largest AI supercomputer. Originally built with 200,000+ Nvidia GPUs, it expanded in January 2026 to 555,000 GPUs across three buildings, reaching 2 gigawatts of total capacity. The hardware investment alone exceeds $18 billion. Colossus is designed for training frontier-scale models.\n\n**Valuation & Funding:**\n- $230 billion valuation after $20B Series E (late 2025)\n- $250 billion in SpaceX merger (Feb 2026)\n- Combined SpaceX-xAI IPO targeting ~$1.25 trillion valuation\n- Musk has told staff AGI could be achieved by 2026\n\n**Team & Leadership:** Elon Musk (CEO), Christian Szegedy (co-founder). Notable early team included Igor Babuschkin, Yuhuai (Tony) Wu, and Jimmy Ba - though 6 of the original 12 co-founders have since departed.",
    "category": "Platforms & Tools",
    "relatedTerms": [
      "grok-3",
      "grok-4",
      "openai",
      "anthropic",
      "google-deepmind",
      "meta-ai"
    ],
    "references": [
      {
        "title": "xAI - Official Website",
        "url": "https://x.ai"
      },
      {
        "title": "xAI - Wikipedia",
        "url": "https://en.wikipedia.org/wiki/XAI_(company)"
      },
      {
        "title": "Britannica - xAI Company Profile",
        "url": "https://www.britannica.com/money/xAI"
      },
      {
        "title": "CNBC - SpaceX Acquiring xAI Ahead of Potential IPO",
        "url": "https://www.cnbc.com/2026/02/02/elon-musk-spacex-xai-ipo.html"
      },
      {
        "title": "Colossus Supercomputer - Wikipedia",
        "url": "https://en.wikipedia.org/wiki/Colossus_(supercomputer)"
      }
    ],
    "lastUpdated": "2026-02-23"
  },
  {
    "name": "Recursive Language Model (RLM)",
    "slug": "recursive-language-model",
    "shortDef": "An inference approach that lets an LLM programmatically examine, decompose, and recursively call itself over snippets of extremely long input, handling contexts up to 100x beyond native window limits.",
    "fullDef": "A Recursive Language Model (RLM) is an inference strategy introduced by Alex Zhang, Tim Kraska, and Omar Khattab in a December 2025 paper. Instead of forcing a language model to ingest an entire long document in one pass, RLMs treat the input as an external environment accessible through a persistent Python REPL. The model can view, slice, search, and filter the data, then recursively call itself on smaller pieces to build up an answer.\n\nThe architecture works by loading the full input into a variable within the REPL. A root LLM inspects the data programmatically, decides how to partition it, and launches sub-LLM calls on each partition. These sub-calls can themselves recurse further, creating a tree of focused queries. An answer variable is iteratively refined until the model marks it as ready. Sub-LLM calls can be parallelized for efficiency.\n\nRLMs enable processing of inputs up to two orders of magnitude beyond a model's native context window, covering entire codebases, multi-year document archives, and book-length texts. The fine-tuned RLM-Qwen3-8B model outperforms its base Qwen3-8B by 28.3% on average across long-context benchmarks and approaches the quality of vanilla GPT-5 on three long-context tasks at comparable cost.",
    "category": "NLP",
    "relatedTerms": [
      "large-language-model",
      "context-window",
      "transformer",
      "ai-agent"
    ],
    "references": [
      {
        "title": "arXiv - Recursive Language Models",
        "url": "https://arxiv.org/abs/2512.24601"
      },
      {
        "title": "Prime Intellect - Recursive Language Models: The Paradigm of 2026",
        "url": "https://www.primeintellect.ai/blog/rlm"
      },
      {
        "title": "Alex Zhang - Recursive Language Models Blog Post",
        "url": "https://alexzhang13.github.io/blog/2025/rlm/"
      },
      {
        "title": "GitHub - alexzhang13/rlm",
        "url": "https://github.com/alexzhang13/rlm"
      }
    ],
    "lastUpdated": "2026-02-24"
  },
  {
    "name": "Chatbot",
    "slug": "chatbot",
    "shortDef": "A software application that uses AI to simulate human-like conversation through text or voice, ranging from rule-based scripts to modern LLM-powered assistants.",
    "fullDef": "A chatbot is a software application designed to conduct conversation with human users through text or voice interfaces. The concept dates back to 1966 with Joseph Weizenbaum's ELIZA, a rule-based program at MIT that simulated a Rogerian psychotherapist by pattern-matching keywords and reflecting them back as questions. Early chatbots like ELIZA and ALICE (1995) followed rigid scripts and decision trees, producing convincing but shallow interactions.\n\nModern AI chatbots are powered by large language models and transformer architectures. Rather than matching keywords to scripted responses, they interpret the semantic meaning behind user queries and generate dynamic, contextually relevant answers. The release of ChatGPT in November 2022 marked a turning point, demonstrating that LLM-based chatbots could hold coherent multi-turn conversations, write code, summarize documents, and reason through complex problems. Competitors including Claude, Gemini, and Grok followed, each building on the transformer paradigm.\n\nThe field is now shifting from passive question-answering chatbots toward agentic AI systems that can take actions autonomously. Modern AI agents can trigger workflows, connect with internal tools and APIs, and complete multi-step tasks without human intervention at each step. According to Gartner, over 70% of enterprise conversations are projected to be handled by AI agents rather than traditional chatbots by 2026, reflecting a broader move toward multimodal, autonomous, and tool-using conversational systems.",
    "category": "Fundamentals",
    "relatedTerms": [
      "large-language-model",
      "ai-agent",
      "transformer"
    ],
    "references": [
      {
        "title": "Wikipedia - Chatbot",
        "url": "https://en.wikipedia.org/wiki/Chatbot"
      },
      {
        "title": "arXiv - History of Generative AI Chatbots: Past, Present, and Future",
        "url": "https://arxiv.org/abs/2402.05122"
      },
      {
        "title": "Cyfuture - AI Chatbots in 2026: How They Work, Top Platforms & Benefits",
        "url": "https://cyfuture.ai/blog/ai-chatbots"
      }
    ],
    "lastUpdated": "2026-02-25"
  },
  {
    "name": "Turing Test",
    "slug": "turing-test",
    "shortDef": "A test of machine intelligence proposed by Alan Turing in 1950, in which a human evaluator tries to distinguish between a machine and a human based on natural-language conversation alone.",
    "fullDef": "The Turing test is a measure of machine intelligence first described by Alan Turing in his 1950 paper \"Computing Machinery and Intelligence.\" Inspired by a parlor game called the imitation game, the test places a human evaluator in conversation with both a human and a machine through a text-only channel. If the evaluator cannot reliably tell which respondent is the machine, the machine is said to have passed the test.\n\nTuring predicted that by the year 2000 a computer would be able to fool an average interrogator at least 30% of the time after five minutes of questioning. For decades no system came close to this standard. Early chatbots like ELIZA (1966) could mimic conversation through pattern matching but failed under sustained interrogation. The test became a cultural touchstone for debating whether machines can truly \"think\" or merely simulate thinking.\n\nThe rise of large language models has renewed interest in the Turing test. In 2025, researchers at UC San Diego published a study in which OpenAI's GPT-4.5 was mistaken for a human 73% of the time, outperforming actual human participants who were correctly identified only 67% of the time. While this result suggests modern LLMs can pass the original test as Turing defined it, critics argue the test measures conversational mimicry rather than genuine understanding, and that more rigorous benchmarks are needed to evaluate true machine intelligence.",
    "category": "Fundamentals",
    "relatedTerms": [
      "chatbot",
      "large-language-model",
      "machine-learning"
    ],
    "references": [
      {
        "title": "Wikipedia - Turing Test",
        "url": "https://en.wikipedia.org/wiki/Turing_test"
      },
      {
        "title": "Stanford Encyclopedia of Philosophy - The Turing Test",
        "url": "https://plato.stanford.edu/entries/turing-test/"
      },
      {
        "title": "Britannica - Turing Test",
        "url": "https://www.britannica.com/technology/Turing-test"
      }
    ],
    "lastUpdated": "2026-02-25"
  },
  {
    "name": "Small Language Model (SLM)",
    "slug": "small-language-model",
    "shortDef": "A language model with roughly 1 billion to 10 billion parameters, designed to run efficiently on edge devices and resource-constrained environments while retaining core NLP capabilities.",
    "fullDef": "A small language model (SLM) is a lightweight language model typically ranging from 1 billion to 10 billion parameters, compared to the hundreds of billions or trillions found in frontier large language models. Despite their smaller size, SLMs retain core capabilities such as text generation, summarization, translation, and question-answering, often rivaling models 10x their size on domain-specific tasks.\n\nSLMs are optimized for deployment on resource-constrained hardware including smartphones, embedded systems, and edge devices. Their smaller footprint translates to lower latency, reduced memory requirements, and significantly lower inference costs. Organizations often fine-tune SLMs on domain-specific data, producing specialized models that outperform general-purpose LLMs on narrow tasks while running on modest hardware.\n\nNotable examples include Microsoft's Phi-3.5-Mini (3.8B parameters), Google DeepMind's Gemma 3 4B, Meta's Llama 3.2 1B and 3B, Alibaba's Qwen 2.5 1.5B, and Hugging Face's SmolLM2 1.7B. The trend toward capable small models has accelerated as techniques like knowledge distillation, quantization, and architecture improvements continue to close the performance gap with larger models, making on-device AI practical for privacy-sensitive and latency-critical applications.",
    "category": "Fundamentals",
    "relatedTerms": [
      "large-language-model",
      "transformer",
      "inference"
    ],
    "references": [
      {
        "title": "Hugging Face - Small Language Models: A Comprehensive Overview",
        "url": "https://huggingface.co/blog/jjokah/small-language-model"
      },
      {
        "title": "IBM - What Are Small Language Models?",
        "url": "https://www.ibm.com/think/topics/small-language-models"
      },
      {
        "title": "DataCamp - Top 15 Small Language Models for 2026",
        "url": "https://www.datacamp.com/blog/top-small-language-models"
      },
      {
        "title": "Red Hat - SLMs vs LLMs: What Are Small Language Models?",
        "url": "https://www.redhat.com/en/topics/ai/llm-vs-slm"
      }
    ],
    "lastUpdated": "2026-02-25"
  },
  {
    "name": "Binary",
    "slug": "binary",
    "shortDef": "A compiled, executable file that a computer can run directly, as opposed to source code that must be interpreted or compiled first.",
    "fullDef": "A binary is a file containing machine code or bytecode that a computer can execute directly. In software development, the term refers to the compiled output of source code - the ready-to-run program that results from translating human-readable code into instructions a processor can understand.\n\nIn the context of AI and machine learning, binaries are important when deploying models to production. ML frameworks like TensorFlow and PyTorch compile computational graphs into optimized binaries for inference. Tools like ONNX Runtime, TensorRT, and llama.cpp produce binaries tuned for specific hardware, enabling faster model execution on CPUs, GPUs, or specialized accelerators.\n\nThe distinction between source code and binaries matters for distribution and deployment. Pre-built binaries allow users to run software without needing compilers or development toolchains, which simplifies the process of setting up inference servers, running local LLMs, and deploying AI applications to edge devices.",
    "category": "Fundamentals",
    "relatedTerms": [
      "inference",
      "docker-image"
    ],
    "lastUpdated": "2026-02-25"
  },
  {
    "name": "Docker Image",
    "slug": "docker-image",
    "shortDef": "A lightweight, standalone, executable package that includes everything needed to run a piece of software - code, runtime, libraries, and system tools.",
    "fullDef": "A Docker image is an immutable template used to create Docker containers. It packages an application together with all of its dependencies, configuration files, environment variables, and the operating system filesystem into a single artifact that can be reliably deployed across different environments.\n\nDocker images are built in layers, where each layer represents a set of filesystem changes defined by instructions in a Dockerfile. This layered architecture enables efficient storage and transfer, since layers shared between images are only stored once. Images are distributed through registries like Docker Hub, making it straightforward to share and deploy applications.\n\nIn AI and ML workflows, Docker images are essential for reproducibility and deployment. They ensure that a model trained in one environment runs identically in another, eliminating \"works on my machine\" problems. Common use cases include packaging inference servers, bundling models with their serving frameworks, creating consistent training environments, and deploying ML pipelines to cloud platforms like AWS, GCP, or Azure.",
    "category": "MLOps",
    "relatedTerms": [
      "cold-start",
      "inference"
    ],
    "lastUpdated": "2026-02-25"
  },
  {
    "name": "Cold Start",
    "slug": "cold-start",
    "shortDef": "The initial delay when a system or service must initialize from scratch before it can handle requests, common in serverless and containerized deployments.",
    "fullDef": "A cold start occurs when a system that has been idle or is being launched for the first time must complete initialization steps before it can begin processing. This includes loading code into memory, establishing network connections, initializing runtimes, and in the case of ML systems, loading model weights into memory or onto a GPU.\n\nIn serverless computing platforms like AWS Lambda or Google Cloud Functions, cold starts happen when a new container instance must be provisioned to handle an incoming request. The latency introduced can range from milliseconds to several seconds depending on the runtime, package size, and initialization logic. For ML inference endpoints, cold starts can be especially pronounced because large model files must be downloaded and loaded into GPU memory.\n\nStrategies to mitigate cold starts include keeping instances warm with periodic pings, using provisioned concurrency, optimizing container image sizes, lazy-loading non-critical dependencies, and using model caching. In the context of recommendation systems, \"cold start\" also refers to the challenge of making predictions for new users or items with no historical data.",
    "category": "MLOps",
    "relatedTerms": [
      "inference",
      "docker-image"
    ],
    "lastUpdated": "2026-02-25"
  },
  {
    "name": "RAM",
    "slug": "ram",
    "shortDef": "Random Access Memory - the fast, volatile working memory a computer uses to store data that is actively being used or processed.",
    "fullDef": "RAM (Random Access Memory) is a type of computer memory that provides fast read and write access to data currently in use. Unlike storage drives, RAM is volatile, meaning its contents are lost when the system is powered off. It serves as the primary workspace where the CPU and GPU access data during computation.\n\nIn machine learning and AI, RAM is a critical resource. During training, datasets, model parameters, gradients, and optimizer states all reside in RAM or GPU VRAM (Video RAM). A model with billions of parameters requires substantial memory - for example, a 7-billion parameter model in 16-bit precision needs roughly 14 GB just for its weights, with additional memory required for activations and gradients during training.\n\nThe amount of available RAM directly constrains what models can be run locally. Techniques like quantization (reducing parameter precision from 16-bit to 4-bit), model sharding (splitting a model across devices), and offloading (moving layers between RAM and VRAM) help fit larger models into limited memory. Understanding RAM requirements is essential when selecting hardware for training or deploying AI models.",
    "category": "Fundamentals",
    "relatedTerms": [
      "inference",
      "large-language-model",
      "fine-tuning"
    ],
    "lastUpdated": "2026-02-25"
  },
  {
    "name": "Tool Calling",
    "slug": "tool-calling",
    "shortDef": "A capability that allows large language models to invoke external functions, APIs, or tools to perform actions beyond text generation.",
    "fullDef": "Tool calling (also known as function calling) is a feature of modern large language models that enables them to interact with external systems by generating structured requests to invoke specific tools. Instead of only producing text responses, the model can decide when to call a tool, specify the required arguments, and incorporate the tool's output into its response.\n\nWhen a model supports tool calling, the developer defines a set of available tools with their names, descriptions, and parameter schemas. During inference, the model analyzes the user's request and determines whether any tools should be invoked. If so, it outputs a structured tool call (typically in JSON format) rather than a plain text response. The application then executes the tool, returns the result to the model, and the model generates a final response incorporating that information.\n\nTool calling is a foundational capability for building AI agents and assistants that can take real-world actions. Common examples include searching the web, querying databases, calling APIs, performing calculations, reading files, and executing code. Models from OpenAI, Anthropic, Google, and others support tool calling, making it a standard interface for extending LLM capabilities beyond their training data.",
    "category": "Agents",
    "relatedTerms": [
      "ai-agent",
      "large-language-model",
      "prompt-engineering"
    ],
    "lastUpdated": "2026-02-25"
  },
  {
    "name": "Ollama",
    "slug": "ollama",
    "shortDef": "An open-source tool for running large language models locally on personal computers with a simple command-line interface.",
    "fullDef": "Ollama is an open-source application that makes it easy to download, run, and manage large language models on local hardware. It provides a simple command-line interface and a REST API, abstracting away the complexity of model quantization, memory management, and GPU acceleration so that users can run LLMs with minimal setup.\n\nOllama packages models using a format inspired by Docker, where each model is defined by a \"Modelfile\" that specifies the base model, parameters, system prompts, and other configuration. Users can pull pre-built models from the Ollama library (including Llama, Mistral, Gemma, Phi, and many others) or create custom models by modifying existing ones. The tool automatically handles quantization and hardware detection to optimize performance.\n\nOllama has become popular in the developer community for local AI development, prototyping, and privacy-sensitive applications where sending data to cloud APIs is not desirable. It runs on macOS, Linux, and Windows, supports both CPU and GPU inference, and integrates with tools like Open WebUI, LangChain, and various IDE extensions for code completion.",
    "category": "Platforms & Tools",
    "relatedTerms": [
      "large-language-model",
      "inference",
      "ram"
    ],
    "lastUpdated": "2026-02-25"
  },
  {
    "name": "Pattern Matching",
    "slug": "pattern-matching",
    "shortDef": "A technique for checking data against a set of predefined patterns or rules, used in programming languages, text processing, and machine learning.",
    "fullDef": "Pattern matching is the process of checking a given sequence of data against a set of patterns to determine whether it conforms to expected structures. It appears across multiple domains in computer science, from simple string matching with regular expressions to complex structural matching in functional programming languages like Rust, Haskell, and Scala.\n\nIn machine learning and AI, pattern matching takes on a broader meaning. Models learn to recognize patterns in training data - edges and textures in images, syntactic structures in language, or anomalous sequences in time series. Classical approaches like template matching compare input directly against stored templates, while modern deep learning systems learn hierarchical pattern representations automatically through training.\n\nPattern matching is also fundamental to information retrieval and natural language processing. Search engines match query patterns against document indices, regular expressions extract structured data from unstructured text, and named entity recognition systems match text spans against learned entity patterns. The concept bridges rule-based and learned approaches, making it one of the most broadly applicable ideas in computing.",
    "category": "Fundamentals",
    "relatedTerms": [
      "named-entity-recognition",
      "semantic-search",
      "machine-learning"
    ],
    "lastUpdated": "2026-02-25"
  },
  {
    "name": "Vibe Coding",
    "slug": "vibe-coding",
    "shortDef": "A software development approach where a programmer describes what they want in natural language and an AI model generates the code, with the programmer guiding the process through conversation rather than writing code directly.",
    "fullDef": "Vibe coding is a term coined by Andrej Karpathy in early 2025 to describe a programming workflow where the developer \"fully gives in to the vibes\" and lets an AI assistant write the code. Rather than typing code line by line, the programmer describes their intent in plain language, reviews what the AI produces, and steers the direction through iterative conversation. The human focuses on what the software should do while the AI handles how to implement it.\n\nThe approach relies on large language models with strong code generation capabilities, integrated into tools like Cursor, GitHub Copilot, Claude Code, or similar AI-powered development environments. The programmer acts more as a creative director than a typist, describing features, pointing out bugs, and requesting changes in natural language. Testing and running the code still provides the ground truth for whether the output works.\n\nVibe coding has sparked debate in the software engineering community. Proponents argue it dramatically lowers the barrier to building software and lets experienced developers prototype faster. Critics point out that code generated without deep understanding can introduce subtle bugs, security vulnerabilities, and technical debt. The approach works best for prototyping, personal projects, and exploratory work, while production systems typically still require careful human review and architectural oversight.",
    "category": "Fundamentals",
    "relatedTerms": [
      "large-language-model",
      "prompt-engineering",
      "ai-agent"
    ],
    "lastUpdated": "2026-02-25"
  },
  {
    "name": "Zero-day",
    "slug": "zero-day",
    "shortDef": "A software vulnerability that is unknown to the vendor and has no available patch, giving defenders zero days of warning before it can be exploited.",
    "fullDef": "A zero-day (also written as 0-day) is a security vulnerability in software or hardware that is unknown to the party responsible for fixing it, typically the vendor or developer. The name refers to the fact that developers have had zero days to address and patch the flaw since its discovery. A zero-day exploit is an attack that takes advantage of such a vulnerability before a fix is available.\n\nZero-days are considered among the most dangerous security threats because traditional defenses like signature-based antivirus or patching cannot protect against them. They are discovered through reverse engineering, fuzzing, source code analysis, or in-the-wild observation. Once a zero-day is publicly disclosed or a patch is released, it ceases to be a zero-day, though unpatched systems remain vulnerable.\n\nIn the context of AI and machine learning, zero-days are relevant in multiple ways. AI systems themselves can contain zero-day vulnerabilities in their serving infrastructure, model APIs, or inference engines. Conversely, machine learning is increasingly used for zero-day detection, with anomaly detection models identifying unusual system behavior that may indicate exploitation of unknown vulnerabilities. AI-powered fuzzing tools can also discover zero-days faster than traditional methods, making the intersection of AI and security an active area of research.",
    "category": "Fundamentals",
    "relatedTerms": [
      "machine-learning",
      "inference"
    ],
    "lastUpdated": "2026-02-25"
  },
  {
    "name": "KV Cache",
    "slug": "kv-cache",
    "shortDef": "A memory optimization technique in LLM inference that stores previously computed key-value pairs from attention layers, avoiding redundant recalculation when generating each new token.",
    "fullDef": "KV cache (Key-Value cache) is a critical optimization for large language model inference that stores the intermediate key and value tensors computed by the attention mechanism during token generation. Without it, the model would need to recompute attention over the entire sequence for every new token, resulting in quadratic time complexity that makes generation prohibitively slow.\n\nLLM inference has two phases. During the prefill phase, the model processes the input prompt and computes key-value pairs for every token across all attention layers. During the decode phase, new tokens are generated one at a time in an autoregressive fashion. The KV cache stores the key-value pairs from all previously processed tokens so that each new generation step only needs to compute attention for the latest token against the cached history, reducing the per-token cost from linear to constant with respect to sequence length.\n\nThe KV cache is one of the largest consumers of GPU memory during inference, often accounting for up to 70% of total memory usage for long sequences. This has driven significant research into optimization techniques. PagedAttention, introduced by vLLM, borrows concepts from operating system virtual memory to allocate KV cache in fixed-size pages that grow dynamically. Other approaches include KV cache quantization to reduce precision, selective eviction to keep only the most relevant tokens, merging similar key-value pairs, and chunk-level compression that preserves semantic coherence. These optimizations are essential for serving models with long context windows efficiently.",
    "category": "Deep Learning",
    "relatedTerms": [
      "attention-mechanism",
      "large-language-model",
      "inference",
      "context-window",
      "ram"
    ],
    "lastUpdated": "2026-02-26"
  },
  {
    "name": "GPU",
    "slug": "gpu",
    "shortDef": "A Graphics Processing Unit - a specialized processor designed for parallel computation, now essential for training and running AI models due to its ability to perform thousands of operations simultaneously.",
    "fullDef": "A GPU (Graphics Processing Unit) is a processor originally designed to accelerate graphics rendering by performing many calculations in parallel. Unlike CPUs, which have a few powerful cores optimized for sequential tasks, GPUs contain thousands of smaller cores that can execute operations simultaneously. This massively parallel architecture turns out to be ideal for the matrix multiplications and tensor operations that dominate machine learning workloads.\n\nIn AI, GPUs are used for both training and inference. During training, a model must process large batches of data and update millions or billions of parameters through backpropagation - all of which involves dense linear algebra that maps naturally onto GPU hardware. What might take weeks on a CPU can often be completed in hours or days on a modern GPU. NVIDIA dominates the AI GPU market with its CUDA ecosystem and data center GPUs like the A100 and H100, though AMD and Intel are competing with alternatives.\n\nGPU memory (VRAM) is a key constraint in AI workloads. A model's parameters, activations, gradients, and KV cache must all fit in VRAM during operation. This has driven techniques like model parallelism (splitting models across multiple GPUs), quantization (reducing parameter precision to use less memory), and offloading (moving data between GPU and system RAM). The cost and availability of GPUs remains one of the most significant bottlenecks in AI development, influencing everything from research pace to model deployment strategies.",
    "category": "Fundamentals",
    "relatedTerms": [
      "ram",
      "inference",
      "kv-cache",
      "large-language-model",
      "backpropagation"
    ],
    "lastUpdated": "2026-02-26"
  },
  {
    "name": "Quantization",
    "slug": "quantization",
    "shortDef": "A technique that reduces the numerical precision of a model's weights and activations, shrinking memory usage and speeding up inference with minimal loss in accuracy.",
    "fullDef": "Quantization is the process of converting a neural network's parameters from high-precision floating point numbers (typically 32-bit or 16-bit) to lower-precision representations such as 8-bit integers (INT8) or even 4-bit integers (INT4). This reduces the model's memory footprint and increases inference speed, since lower-precision arithmetic requires less memory bandwidth and can be computed faster on modern hardware.\n\nThere are two main approaches. Post-training quantization (PTQ) takes an already-trained model and converts its weights to lower precision, sometimes using a small calibration dataset to minimize accuracy loss. Quantization-aware training (QAT) simulates low-precision arithmetic during the training process itself, allowing the model to learn to compensate for the reduced precision and typically producing better results than PTQ.\n\nQuantization has become essential for deploying large language models on consumer hardware. A 7-billion parameter model stored in 16-bit precision requires roughly 14 GB of memory, but the same model quantized to 4-bit precision fits in approximately 3.5 GB - making it runnable on a laptop GPU or even a phone. Popular quantization formats include GPTQ, GGUF (used by llama.cpp and Ollama), and AWQ. The tradeoff is always between model size, speed, and accuracy - aggressive quantization saves memory but can degrade output quality, particularly on tasks requiring precise reasoning.",
    "category": "Deep Learning",
    "relatedTerms": [
      "gpu",
      "ram",
      "inference",
      "large-language-model",
      "ollama"
    ],
    "lastUpdated": "2026-02-26"
  },
  {
    "name": "CUDA",
    "slug": "cuda",
    "shortDef": "NVIDIA's parallel computing platform and API that allows developers to use NVIDIA GPUs for general-purpose processing, forming the backbone of most AI training and inference workflows.",
    "fullDef": "CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA that allows software developers to use NVIDIA GPUs for general-purpose computation beyond graphics rendering. Released in 2007, CUDA provides a C/C++-like programming interface for writing code that runs on the GPU's thousands of parallel cores.\n\nCUDA became the dominant platform for AI and machine learning because the major deep learning frameworks - PyTorch, TensorFlow, and JAX - all use CUDA under the hood to accelerate tensor operations on NVIDIA hardware. When a model is trained or run on a GPU, CUDA handles the low-level work of distributing matrix multiplications, convolutions, and other operations across the GPU's cores. Libraries like cuDNN (for deep learning primitives) and cuBLAS (for linear algebra) build on CUDA to provide optimized implementations of the operations AI models use most.\n\nNVIDIA's dominance in AI hardware is largely due to the CUDA ecosystem rather than raw hardware superiority alone. Decades of tooling, libraries, documentation, and community knowledge create significant switching costs. Competitors like AMD (with ROCm) and Intel (with oneAPI) offer alternative platforms, but most AI code, tutorials, and production deployments assume CUDA availability. This lock-in effect is why CUDA compatibility is often the first question asked when evaluating AI hardware.",
    "category": "Platforms & Tools",
    "relatedTerms": [
      "gpu",
      "deep-learning",
      "inference",
      "large-language-model"
    ],
    "lastUpdated": "2026-02-26"
  },
  {
    "name": "Artificial Analysis",
    "slug": "artificial-analysis",
    "shortDef": "An independent platform that benchmarks AI models and inference providers across intelligence, performance, price, speed, and latency with standardized methodology.",
    "fullDef": "Artificial Analysis is an independent benchmarking platform that evaluates and compares AI models and API hosting providers across key metrics including intelligence, output speed, latency, price, and context window size. It provides standardized, reproducible evaluations that help developers and organizations choose between models and providers based on real-world performance data rather than vendor claims.\n\nThe platform's Intelligence Index (v4.0) aggregates scores across multiple evaluation categories, each weighted equally, covering reasoning, knowledge, coding, and specialized tasks. It includes benchmarks like GPQA Diamond, Humanity's Last Exam, and its own proprietary evaluations such as AA-Omniscience (a knowledge and hallucination benchmark that penalizes incorrect guesses) and AA-LCR (long context retrieval). This composite approach provides a more balanced view of model capability than any single benchmark.\n\nBeyond model intelligence, Artificial Analysis measures the end-to-end inference performance that customers actually experience across different API providers, including time to first token, tokens per second, and total response time under varying concurrency loads. It also benchmarks AI accelerator hardware, comparing chips like NVIDIA H100, AMD MI300X, and others for inference throughput. The platform has become a widely referenced source in AI industry reporting and model comparison discussions.",
    "category": "Platforms & Tools",
    "relatedTerms": [
      "large-language-model",
      "inference",
      "gpqa-diamond"
    ],
    "lastUpdated": "2026-02-26"
  },
  {
    "name": "GPQA Diamond",
    "slug": "gpqa-diamond",
    "shortDef": "A benchmark of 198 graduate-level multiple-choice questions in physics, biology, and chemistry that are designed to be unsolvable through internet search, requiring genuine PhD-level expertise.",
    "fullDef": "GPQA Diamond (Graduate-Level Google-Proof Q&A - Diamond subset) is a challenging AI evaluation benchmark consisting of 198 multiple-choice questions in biology, physics, and chemistry. The questions are written by domain experts with PhDs and are specifically designed to be \"Google-proof\" - even skilled non-experts with unrestricted internet access score only about 34% accuracy, while PhD-level domain experts achieve roughly 65-70%.\n\nThe Diamond subset is the highest-quality tier of the broader GPQA dataset. A question qualifies for the Diamond set only if both expert annotators answered it correctly while the majority of non-expert annotators answered incorrectly. This filtering ensures the questions genuinely require deep domain expertise rather than surface-level knowledge or effective search skills.\n\nGPQA Diamond has become one of the standard benchmarks for evaluating frontier AI models on scientific reasoning. Top models now score above 80% on this benchmark, surpassing the average human expert performance of roughly 70%. This creates an interesting dynamic for AI safety research: as models exceed human expert performance on specialized knowledge tasks, it becomes increasingly difficult for humans to verify whether model outputs are correct, highlighting the challenge of scalable oversight.",
    "category": "Fundamentals",
    "relatedTerms": [
      "large-language-model",
      "artificial-analysis",
      "lmarena"
    ],
    "lastUpdated": "2026-02-26"
  },
  {
    "name": "AIME",
    "slug": "aime",
    "shortDef": "The American Invitational Mathematics Examination - a prestigious high school math competition whose problems are used as a benchmark for evaluating AI mathematical reasoning capabilities.",
    "fullDef": "AIME (American Invitational Mathematics Examination) is a challenging mathematics competition for top-performing high school students in the United States, serving as a qualifying step toward the International Mathematical Olympiad. Each exam consists of 15 problems requiring integer answers between 000 and 999, covering algebra, geometry, number theory, and combinatorics. The problems demand creative mathematical reasoning and precise calculation rather than rote knowledge.\n\nThe AI research community adopted AIME as a benchmark because its problems test genuine mathematical reasoning rather than pattern matching or memorization. The AIME 2025 benchmark uses 30 problems from the 2025 competition (15 per exam session) and has become a standard evaluation for measuring how well models handle multi-step mathematical problem-solving. Frontier models now routinely exceed human performance, with top models achieving above 85% accuracy compared to the median human competitor solving roughly 27-40% of problems.\n\nA key concern with AIME as an AI benchmark is data contamination. Since problems and solutions are publicly available after each competition, models may have been exposed to them during pretraining. Researchers have noted that models tend to perform better on older questions compared to newer ones, suggesting some degree of memorization. Each year's new AIME problems provide a fresh, uncontaminated test set, which is why the benchmark is updated annually.",
    "category": "Fundamentals",
    "relatedTerms": [
      "large-language-model",
      "artificial-analysis",
      "gpqa-diamond"
    ],
    "lastUpdated": "2026-02-26"
  },
  {
    "name": "MMMU-Pro",
    "slug": "mmmu-pro",
    "shortDef": "A rigorous multimodal AI benchmark with college-level questions across six disciplines that tests whether models truly understand visual and textual information together.",
    "fullDef": "MMMU-Pro (Massive Multi-discipline Multimodal Understanding and Reasoning - Pro) is a hardened version of the MMMU benchmark designed to rigorously evaluate multimodal AI models on tasks requiring both visual understanding and expert-level reasoning. It covers six core disciplines: Art and Design, Business, Science, Health and Medicine, Humanities and Social Science, and Technology and Engineering, using questions drawn from college exams, quizzes, and textbooks.\n\nMMMU-Pro improves on its predecessor through three key steps. First, it filters out questions that text-only models can answer correctly, ensuring that visual understanding is genuinely required. Second, it increases the number of answer options from 4 to 10, reducing the effectiveness of guessing from 25% to 10% random chance. Third, it introduces a vision-only input setting where the question text is embedded within images, testing whether models can extract and reason about information presented visually.\n\nThese changes make MMMU-Pro substantially harder than MMMU. Model performance drops significantly compared to the original benchmark, with scores typically falling by 17-27 percentage points. Chain-of-thought reasoning generally helps, but OCR-based prompting strategies show minimal improvement. MMMU-Pro has become a standard benchmark for evaluating frontier multimodal models, providing a more realistic assessment of their ability to handle complex visual reasoning tasks that mirror real-world academic and professional scenarios.",
    "category": "Fundamentals",
    "relatedTerms": [
      "large-language-model",
      "artificial-analysis",
      "gpqa-diamond"
    ],
    "lastUpdated": "2026-02-26"
  },
  {
    "name": "LMArena",
    "slug": "lmarena",
    "shortDef": "A crowdsourced platform where users compare AI models head-to-head in blind conversations, producing Elo-based rankings that reflect real human preferences.",
    "fullDef": "LMArena (formerly LMSYS Chatbot Arena) is a crowdsourced evaluation platform where users interact with two anonymous AI models simultaneously and vote on which response they prefer. With over 6 million votes collected, it produces rankings based on real human preferences rather than automated benchmarks, making it one of the most widely cited measures of LLM quality in practice.\n\nThe platform works by presenting users with two anonymous model responses to the same prompt. After reading both responses, users vote for their preferred one or declare a tie. The identities of the models are revealed only after voting, preventing bias. These pairwise comparisons are aggregated using the Bradley-Terry model (an evolution of the original Elo rating system used in chess) to produce stable rankings with confidence intervals. The shift from online Elo to Bradley-Terry provides more stable ratings and better handles the growing number of models being evaluated.\n\nLMArena is valued because it captures aspects of model quality that automated benchmarks miss, such as writing style, helpfulness, nuance, and instruction following. However, it also has limitations: votes skew toward longer, more detailed responses regardless of accuracy, and the user population may not represent all use cases. Despite these caveats, LMArena rankings have become a de facto industry standard, frequently referenced in model release announcements and competitive comparisons between AI labs.",
    "category": "Platforms & Tools",
    "relatedTerms": [
      "large-language-model",
      "artificial-analysis",
      "gpqa-diamond",
      "aime"
    ],
    "lastUpdated": "2026-02-26"
  },
  {
    "name": "Benchmark",
    "slug": "benchmark",
    "shortDef": "A standardized test or evaluation used to measure and compare the performance of AI models on specific tasks like reasoning, coding, math, or language understanding.",
    "fullDef": "A benchmark in AI is a standardized evaluation consisting of a fixed dataset, a defined task, and a scoring methodology that allows researchers to measure and compare model performance on equal footing. Benchmarks serve as the primary yardstick for tracking progress in the field, providing concrete numbers that indicate whether a new model represents a genuine improvement over its predecessors.\n\nBenchmarks span a wide range of capabilities. GPQA Diamond tests graduate-level scientific reasoning, AIME evaluates mathematical problem-solving, MMMU-Pro assesses multimodal understanding across academic disciplines, and coding benchmarks like SWE-Bench measure a model's ability to solve real software engineering tasks. Platforms like Artificial Analysis and LMArena aggregate results across multiple benchmarks to produce composite rankings that give a broader view of model capability.\n\nWhile benchmarks are essential for measuring progress, they have well-known limitations. Models can be optimized to perform well on specific benchmarks without corresponding improvements in real-world usefulness, a phenomenon known as benchmark gaming or overfitting to the test set. Data contamination is another concern, where models may have seen benchmark questions during training. The AI community continuously develops new benchmarks to stay ahead of these issues, with recent examples designed to be harder to game through techniques like filtering out questions solvable by simpler methods or using problems that did not exist when models were trained.",
    "category": "Fundamentals",
    "relatedTerms": [
      "gpqa-diamond",
      "aime",
      "mmmu-pro",
      "lmarena",
      "artificial-analysis"
    ],
    "lastUpdated": "2026-02-26"
  },
  {
    "name": "API",
    "slug": "api",
    "shortDef": "Application Programming Interface - a defined set of rules and protocols that allows different software systems to communicate with each other.",
    "fullDef": "An API (Application Programming Interface) is a contract between two pieces of software that defines how they can interact. It specifies what requests a client can make, what data it needs to provide, and what responses it will receive. APIs allow developers to use functionality built by others without needing to understand or access the underlying implementation.\n\nIn AI, APIs are the primary way most developers interact with large language models. Services like OpenAI, Anthropic, and Google expose their models through REST APIs where developers send prompts as HTTP requests and receive model outputs as JSON responses. This abstraction means you can build applications powered by frontier models without running any ML infrastructure yourself. API pricing is typically based on token usage, with separate rates for input and output tokens.\n\nAPIs also play a central role in AI agent architectures. Tool calling allows models to invoke external APIs during a conversation, enabling them to fetch live data, interact with databases, send messages, or trigger actions in other systems. The quality of an API's design - its documentation, error handling, rate limiting, and versioning - directly affects how reliably AI-powered applications can be built on top of it. As AI moves from research demos to production systems, API design has become one of the most consequential decisions in the stack.",
    "category": "Fundamentals",
    "relatedTerms": [
      "large-language-model",
      "tool-calling",
      "inference",
      "ai-agent"
    ],
    "lastUpdated": "2026-02-26"
  },
  {
    "name": "MCP",
    "slug": "mcp",
    "shortDef": "Model Context Protocol - an open standard created by Anthropic that defines how AI assistants connect to external data sources, tools, and services through a unified interface.",
    "fullDef": "MCP (Model Context Protocol) is an open standard announced by Anthropic in November 2024 that provides a standardized way for AI models to connect to external data sources, tools, and services. It defines a common protocol so that any AI assistant can interact with any compatible data system, eliminating the need for custom integrations between every model and every tool.\n\nMCP follows a client-server architecture. An AI application acts as an MCP client, while external services expose their capabilities through MCP servers. The protocol defines how tools, resources, and prompts are discovered, invoked, and how results are returned. This means a single MCP server built for a database, a code repository, or a business application can work with any AI assistant that speaks MCP, regardless of which model powers it.\n\nMCP achieved rapid industry adoption throughout 2025. OpenAI integrated it across its products including the ChatGPT desktop app in March 2025, and Google DeepMind confirmed support for Gemini models shortly after. The specification received major updates in November 2025, adding asynchronous operations, statelessness, and an official registry for discovering MCP servers. In December 2025, Anthropic donated MCP to the Agentic AI Foundation under the Linux Foundation, co-founded with Block and OpenAI, cementing it as a vendor-neutral industry standard rather than a proprietary protocol.",
    "category": "Agents",
    "relatedTerms": [
      "tool-calling",
      "ai-agent",
      "api",
      "anthropic",
      "openai"
    ],
    "lastUpdated": "2026-02-26"
  },
  {
    "name": "Open Source",
    "slug": "open-source",
    "shortDef": "Software whose source code is publicly available for anyone to view, modify, and distribute, enabling community-driven development and transparency.",
    "fullDef": "Open source refers to software released under a license that grants anyone the right to view, modify, and redistribute the source code. The concept originated in the software development community as an alternative to proprietary, closed-source software, and is governed by licenses such as MIT, Apache 2.0, and GPL that define what users can and cannot do with the code.\n\nIn AI, the term has become more nuanced and sometimes contentious. Some model releases like Meta's Llama series and Mistral's models are described as open source because their weights are publicly downloadable, but they often come with usage restrictions that traditional open source licenses do not impose. The Open Source Initiative has argued that truly open source AI requires not just model weights but also training code, data, and documentation. The industry increasingly uses the term \"open weight\" to distinguish models that release weights but not training data or code from fully open source projects.\n\nOpen source has been a major driver of AI progress. Frameworks like PyTorch and TensorFlow, tools like Hugging Face Transformers and Ollama, and datasets on platforms like Hugging Face have democratized access to AI capabilities. Open source models allow researchers and companies to fine-tune, inspect, and deploy models without depending on API providers, enabling use cases where data privacy, cost control, or customization matter more than using the absolute frontier model.",
    "category": "Fundamentals",
    "relatedTerms": [
      "hugging-face",
      "ollama",
      "large-language-model",
      "fine-tuning"
    ],
    "lastUpdated": "2026-02-26"
  },
  {
    "name": "Open Weight Model",
    "slug": "open-weight-model",
    "shortDef": "An AI model whose trained parameters (weights) are publicly released for download and use, but whose training data, code, or methodology may remain proprietary.",
    "fullDef": "An open weight model is an AI model where the trained parameters (weights) are publicly available for anyone to download, run, and often fine-tune, but the full training pipeline - including training data, preprocessing code, and training infrastructure details - is not disclosed. This distinguishes open weight models from fully open source AI, where all components needed to reproduce the model from scratch are provided.\n\nMost major \"open\" model releases fall into the open weight category rather than true open source. Meta's Llama series, Mistral's models, Google's Gemma, and DeepSeek's releases all provide downloadable weights, often with custom licenses that impose usage restrictions such as user count limits or prohibited use cases. Users can run these models locally, fine-tune them for specific tasks, and deploy them in production, but they cannot fully reproduce or understand the training process because the data and complete methodology are withheld.\n\nThe distinction matters for several reasons. Open weight models give developers practical access to powerful AI without API costs or data privacy concerns, but they do not provide the transparency and reproducibility that scientific research demands. The training data, which heavily influences model behavior, biases, and capabilities, remains a black box. Despite this limitation, open weight models have dramatically expanded access to AI capabilities, enabling local deployment through tools like Ollama and llama.cpp and driving competition that pushes the entire field forward.",
    "category": "Fundamentals",
    "relatedTerms": [
      "open-source",
      "large-language-model",
      "fine-tuning",
      "ollama",
      "hugging-face"
    ],
    "lastUpdated": "2026-02-26"
  },
  {
    "name": "SaaS",
    "slug": "saas",
    "shortDef": "Software as a Service - a delivery model where software is hosted in the cloud and accessed through a browser or API on a subscription basis rather than installed locally.",
    "fullDef": "SaaS (Software as a Service) is a software distribution model where applications are hosted by a provider and made available to customers over the internet, typically on a subscription basis. Instead of installing and maintaining software on local machines, users access it through a web browser or API, with the provider handling infrastructure, updates, and security.\n\nThe SaaS model transformed the software industry by lowering the barrier to both building and using software products. Companies like Salesforce, Slack, and Notion built multi-billion dollar businesses on recurring subscription revenue from cloud-hosted tools. For startups, SaaS offered a repeatable playbook: identify a workflow pain point, build a focused tool around it, charge monthly, and scale.\n\nAI is now challenging the SaaS model in fundamental ways. Foundation models with tool calling and connector capabilities can increasingly perform the same tasks that vertical SaaS products were built to automate. When a general-purpose AI assistant can manage ad campaigns, draft emails, or query databases through built-in connectors, the value proposition of a dedicated SaaS tool for each task narrows. This has led to debate about whether the traditional SaaS category is entering a period of consolidation, where AI platforms absorb functionality that previously required separate subscriptions.",
    "category": "Fundamentals",
    "relatedTerms": [
      "api",
      "tool-calling",
      "ai-agent"
    ],
    "lastUpdated": "2026-02-26"
  },
  {
    "name": "Foundation Model",
    "slug": "foundation-model",
    "shortDef": "A large AI model trained on broad data at scale that can be adapted to a wide range of downstream tasks, serving as the base layer for many AI applications.",
    "fullDef": "A foundation model is a large-scale AI model trained on vast, diverse datasets that can be adapted to perform a wide variety of tasks without being retrained from scratch. The term was coined by Stanford's Institute for Human-Centered AI in 2021 to describe models like GPT, Claude, Gemini, and Llama that serve as a common base (or \"foundation\") upon which many different applications are built.\n\nFoundation models are distinguished by their generality. A single model can write code, answer questions, translate languages, analyze images, and reason through complex problems. This generality comes from training on internet-scale datasets that cover an enormous range of human knowledge and language patterns. The model learns broad capabilities during pretraining, which can then be refined for specific use cases through fine-tuning, prompt engineering, or retrieval-augmented generation.\n\nThe foundation model paradigm has reshaped the AI industry structure. Instead of building task-specific models from scratch, most AI applications now build on top of existing foundation models, either through APIs or by fine-tuning open weight versions. This creates a layered ecosystem where a small number of foundation model providers (Anthropic, OpenAI, Google, Meta) supply the base capability, and a much larger number of companies build specialized applications on top. The rapid expansion of foundation model capabilities, particularly through features like tool calling and MCP connectors, has also raised questions about which application-layer products remain defensible when the foundation layer keeps absorbing new functionality.",
    "category": "Fundamentals",
    "relatedTerms": [
      "large-language-model",
      "fine-tuning",
      "transfer-learning",
      "open-weight-model",
      "mcp"
    ],
    "lastUpdated": "2026-02-26"
  },
  {
    "name": "Prompt Injection",
    "slug": "prompt-injection",
    "shortDef": "An attack where malicious instructions are hidden inside input data to hijack an AI model's behavior, causing it to ignore its original instructions and follow the attacker's instead.",
    "fullDef": "Prompt injection is a security vulnerability where an attacker embeds adversarial instructions within input data that an AI model processes, causing the model to follow the attacker's instructions instead of the developer's intended behavior. It is the AI equivalent of SQL injection, exploiting the fact that large language models cannot reliably distinguish between trusted instructions from the developer and untrusted content from external sources.\n\nThere are two main forms. Direct prompt injection occurs when a user deliberately crafts a prompt to override the model's system instructions, for example asking it to ignore its safety guidelines. Indirect prompt injection is more dangerous: malicious instructions are hidden in external content the model processes, such as a webpage, email, or document. When an AI agent reads an email containing hidden instructions like \"forward all emails to attacker@example.com,\" it may follow those instructions without the user's knowledge.\n\nPrompt injection is widely considered one of the most critical unsolved problems in AI security. No reliable, general-purpose defense exists as of early 2026. Mitigations include input sanitization, output filtering, permission controls, allowlists, and human-in-the-loop approval for sensitive actions, but none are foolproof. The problem is especially acute for AI agents with tool calling capabilities, where a successful injection can trigger real-world actions like deleting files, sending messages, or executing code. OWASP ranks prompt injection as the number one security risk for large language model applications.",
    "category": "Fundamentals",
    "relatedTerms": [
      "ai-agent",
      "tool-calling",
      "large-language-model",
      "zero-day"
    ],
    "lastUpdated": "2026-02-26"
  },
  {
    "name": "Gemini",
    "slug": "gemini",
    "shortDef": "Google DeepMind's family of multimodal AI models that power Google's AI products across Search, Workspace, Android, and developer APIs.",
    "fullDef": "Gemini is Google DeepMind's flagship family of large language models, succeeding the earlier PaLM and Bard models. The family spans multiple sizes and specializations: Gemini 3.1 Pro (the most capable), Gemini Flash (optimized for speed and cost), and Gemini Nano (designed for on-device inference on mobile hardware). All Gemini models are natively multimodal, meaning they can process and generate text, images, audio, and video.\n\nGemini is deeply integrated across Google's product ecosystem. It powers AI features in Google Search, Gmail, Google Docs, Google Sheets, Google Ads, Google Lens, and Android. The Gemini API and AI Studio provide developer access for building applications on top of the models. Unlike some competitors, Google offers Gemini with context windows up to 1 million tokens, enabling processing of entire codebases, long documents, and extended conversations.\n\nIn January 2026, Apple announced a multiyear partnership with Google to use Gemini models and cloud infrastructure to power the next generation of Apple Foundation Models, including a more personalized Siri. This deal positioned Gemini as the intelligence layer underneath both Android and iOS, the two dominant mobile operating systems. Google also develops specialized Gemini variants like Nano Banana (image generation) and continues to release open weight models in the Gemma family for researchers and developers who want to run models locally.",
    "category": "LLM Models",
    "relatedTerms": [
      "google-deepmind",
      "large-language-model",
      "gemini-3-1-pro",
      "gemini-2-5-pro",
      "context-window"
    ],
    "lastUpdated": "2026-02-27"
  },
  {
    "name": "ChatGPT",
    "slug": "chatgpt",
    "shortDef": "OpenAI's conversational AI product that provides a chat interface to GPT models, widely credited with bringing large language models to mainstream public awareness.",
    "fullDef": "ChatGPT is a conversational AI product developed by OpenAI that allows users to interact with large language models through a natural chat interface. Launched in November 2022, it reached 100 million users within two months, making it one of the fastest-growing consumer applications in history and bringing AI capabilities to mainstream public attention.\n\nThe product has evolved significantly since launch. It now supports text, image, audio, and video inputs, can browse the web, execute code, generate images, and use external tools through plugins and integrations. ChatGPT is available as a free tier (using GPT-4o mini), a Plus subscription ($20/month with access to GPT-4o and reasoning models), and Team and Enterprise tiers for organizations. The product runs on OpenAI's successive model generations, from GPT-3.5 at launch through GPT-4, GPT-4o, and GPT-5.\n\nChatGPT's impact extends beyond its direct user base. It established the conversational interface as the default paradigm for interacting with AI, influenced how every major tech company positioned its AI products, and created an ecosystem of plugins and integrations. Apple integrated ChatGPT into Siri and system-level features on iOS before its subsequent partnership with Google's Gemini, and the product continues to compete directly with Claude, Gemini, and other conversational AI systems for consumer and enterprise users.",
    "category": "Platforms & Tools",
    "relatedTerms": [
      "openai",
      "gpt-5",
      "large-language-model",
      "tool-calling"
    ],
    "lastUpdated": "2026-02-27"
  },
  {
    "name": "Claude Code",
    "slug": "claude-code",
    "shortDef": "Anthropic's agentic coding tool that runs in the terminal, capable of reading, writing, and executing code across entire codebases with human oversight.",
    "fullDef": "Claude Code is an agentic coding product developed by Anthropic that operates as a command-line interface tool. Unlike traditional code completion tools that suggest snippets, Claude Code can autonomously navigate codebases, read and write files, run terminal commands, execute tests, and perform multi-step software engineering tasks while keeping the developer in the loop for approval of significant actions.\n\nThe tool is powered by Claude models and can handle complex workflows such as refactoring across multiple files, debugging failing tests, implementing features from natural language descriptions, and managing git operations. Anthropic has disclosed that Claude Code writes roughly 80% of its own codebase, using it as both a product and a production workflow internally. Features like Remote Control allow developers to start a session on their machine and continue directing it from their phone.\n\nClaude Code has expanded beyond pure coding into adjacent capabilities. Claude Code Security scans codebases for vulnerabilities with adversarial verification and proposed patches. The tool supports MCP for connecting to external data sources and services. Its combination of terminal-level access, codebase understanding, and agentic autonomy positions it as a developer infrastructure layer rather than just an autocomplete tool, which is why its feature expansions have directly impacted products and companies building in overlapping categories.",
    "category": "Platforms & Tools",
    "relatedTerms": [
      "anthropic",
      "mcp",
      "ai-agent",
      "tool-calling",
      "vibe-coding"
    ],
    "lastUpdated": "2026-02-27"
  },
  {
    "name": "Codex",
    "slug": "codex",
    "shortDef": "OpenAI's asynchronous coding agent that runs tasks in cloud sandboxes, designed for parallel software engineering work like writing features, fixing bugs, and running tests.",
    "fullDef": "Codex is OpenAI's agentic coding product that executes software engineering tasks asynchronously in cloud-hosted sandboxes. Unlike interactive coding assistants, Codex is designed to handle tasks in the background - a developer assigns a task like implementing a feature, fixing a bug, or writing tests, and Codex works on it independently, returning results with a pull request or code diff for review.\n\nThe product evolved from OpenAI's earlier Codex model (which powered the original GitHub Copilot) into a full agentic system built on GPT-5.3-Codex, a model specifically optimized for code generation and software engineering tasks. Each task runs in its own isolated sandbox environment with access to the project's codebase, dependencies, and test suite. Multiple tasks can run in parallel, allowing developers to delegate several pieces of work simultaneously.\n\nCodex represents a different philosophy from interactive tools like Claude Code. Where Claude Code emphasizes real-time collaboration with the developer in the terminal, Codex emphasizes delegation and asynchronous execution. The developer acts more as a project manager, assigning work and reviewing output, rather than pair-programming with the AI. GPT-5.3-Codex leads on specialized coding benchmarks like Terminal-Bench and SWE-Bench Pro but does not publish scores on general reasoning benchmarks, reflecting its focused positioning as a coding-specific agent rather than a general-purpose model.",
    "category": "Platforms & Tools",
    "relatedTerms": [
      "openai",
      "gpt-5-2",
      "ai-agent",
      "tool-calling",
      "vibe-coding"
    ],
    "lastUpdated": "2026-02-27"
  }
]