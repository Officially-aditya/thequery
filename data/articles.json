[
  {
    "title": "OpenClaw Had 210,000 GitHub Stars. Then Anthropic Shipped One Feature.",
    "slug": "openclaw-had-210000-github-stars",
    "date": "2026-02-26",
    "summary": "OpenClaw was the closest thing to JARVIS anyone had shipped. 210,000 GitHub stars in two months. Then Anthropic quietly updated Claude Code and the category collapsed.",
    "content": "Two months ago, I hacked together a weekend project. What started as WhatsApp Relay now has over 100,000 GitHub stars and drew 2 million visitors in a single week.\n\nThat is how Peter Steinberger described [OpenClaw](/glossary/openclaw) in January 2026. By February, the project had crossed 210,000 stars. Developers called it the closest thing to JARVIS anyone had shipped. Then Anthropic quietly updated Claude Code.\n\nThis is the third article in an accidental series. We wrote about [Ryze](/articles/claude-killed-my-startup), a vertical SaaS product that disappeared overnight when Anthropic added a native connector. We wrote about [Claude Code Security](/articles/claude-code-security-the-argument-for-human-in-the-loop-just-got-harder) and what happens when AI systems get too much access without enough oversight. OpenClaw is both stories happening at the same time.\n\n---\n\n## What OpenClaw Actually Was\n\nStrip away the GitHub star count and the JARVIS comparisons and OpenClaw is a pragmatic control plane with a tool surface. Not a mystical AI brain. Not a multi-agent swarm. A single process that owns your messaging connections and lets an LLM call tools on your behalf.\n\nIt runs on your own machine or server and connects directly to tools people already use: email, calendars, files, browsers, and popular messaging platforms like Slack, Telegram, WhatsApp, Discord, and more. Instead of opening a browser tab to talk to an AI, you just send a WhatsApp message. Your assistant is always on, always local, always yours.\n\nThe pitch was privacy-first and genuinely different: your machine, your keys, your data. No cloud subscription, no data leaving your device, no vendor lock-in. For developers who had grown skeptical of cloud-everything, this hit a nerve.\n\nThe naming history alone became a meme. The project launched as Clawd, a pun on Claude with a lobster claw. Anthropic's legal team politely asked for a rename. It became Moltbot briefly, then finally OpenClaw. Each rebrand pulled more attention. By the time it stabilized, it had more GitHub stars than most projects accumulate in years.\n\n---\n\n## The Email Incident\n\nThe enthusiasm had a shadow.\n\nOpenClaw's power came from the same thing that made it dangerous. A developer who gives it access to Gmail is giving an autonomous AI agent the ability to read, write, and delete emails without confirming each action. For experienced developers who set appropriate guardrails, this is manageable. For everyone else, it is a loaded gun.\n\nThe incident that circulated most widely: a user asked OpenClaw to clear an inbox. It started deleting everything. Not archiving. Not sorting. Deleting, at speed, across years of correspondence.\n\nOpenClaw's own documentation acknowledged the problem plainly. Prompt injection, where a malicious instruction hidden inside an email or document hijacks the agent's behavior, is an industry-wide unsolved problem. The project published security best practices. It added allowlists and permission controls. But the fundamental tension between \"agent that acts\" and \"agent that asks permission first\" was never fully resolved.\n\nThis is exactly the argument we made in the [Claude Code Security article](/articles/claude-code-security-the-argument-for-human-in-the-loop-just-got-harder). The human-in-the-loop problem does not disappear when the software is open source. It gets harder because the user is now responsible for their own guardrails with no safety net.\n\n---\n\n## What Claude Code Remote Control Actually Changed\n\nIn February 2026, Anthropic shipped Claude Code Remote Control. The feature allows a developer to start a coding session in their terminal and continue controlling it from their phone while the session keeps running locally.\n\nRead that again slowly.\n\nYou start Claude Code on your machine. You step away. You send instructions from your phone. Claude executes them on your local environment. Your files, your terminal, your codebase, all accessible from a messaging interface.\n\nThat is OpenClaw's core value proposition. Not a similar product. The same idea, built natively into a tool that developers already have installed, already trust, already pay for, with Anthropic's safety infrastructure underneath it.\n\nOpenClaw required setup. It required configuration. It required understanding allowlists and permission models and security best practices. Claude Code Remote Control required nothing new. If you already had Claude Code, you already had this.\n\nThe moat that OpenClaw built over two months, the GitHub stars, the community, the plugins, the documentation, did not matter. Distribution won. Not product.\n\n---\n\n## The Pattern Is Not Subtle Anymore\n\nThis is the third time in one week we have watched the same thing happen.\n\nRyze built a product that connected AI to Google and Meta ad accounts. Anthropic added a native connector. Close rate went from 70% to 20% overnight.\n\nOpenClaw built a product that connected AI to your messaging platforms and local environment. Anthropic shipped Remote Control. The category collapsed.\n\nCrowdStrike and Palo Alto saw their stock prices fall the same week Anthropic shipped Claude Code Security, a tool for scanning codebases for vulnerabilities that overlapped directly with their enterprise offerings.\n\nThe pattern is not that Anthropic is deliberately targeting specific products. The pattern is that foundation model companies are expanding the definition of what a foundation model does. Every expansion absorbs categories that startups and open source projects had claimed.\n\nThis is the Indian IT parallel we drew in the [Ryze article](/articles/claude-killed-my-startup), playing out in real time across different product categories. The Indian IT industry built a $250 billion business on repeatable, well-defined tasks. Those tasks are exactly what AI absorbs fastest because they are well-specified enough to automate. OpenClaw automated well-defined messaging and local execution tasks. Claude Remote Control does the same thing with less friction.\n\nThe developers who saw this coming are already asking the right question: what is left that foundation models cannot absorb with a single feature update?\n\n---\n\n## What Actually Survives\n\nSteinberger's own answer, embedded in OpenClaw's Vision document, is worth reading. The project explicitly rejects agent-hierarchy frameworks and heavy orchestration layers. It chose simple, serialized, debuggable architecture on purpose. Not because complex orchestration is impossible but because it creates systems that are hard to trust.\n\nThat design philosophy, prioritizing human understanding over autonomous capability, is the thing Claude Code Remote Control does not replace. Anthropic can ship a feature that controls your terminal from your phone. It cannot ship a project that teaches you why that should make you nervous.\n\nOpenClaw's community, the developers who understood its internals, who contributed security improvements, who wrote the allowlist documentation, those people are not replaced by Claude Code Remote Control. They are the ones who know why it matters.\n\nThe products that survive this consolidation are the ones that encode knowledge Claude cannot surface with a connector. Not automation. Not integration. Understanding.\n\n---\n\n## One More Thing\n\nOpenClaw started as a weekend hack. Crossed 100,000 GitHub stars in two months. Its creator got hired by OpenAI. The project moved to a foundation. The lobster mascot survived every rebrand.\n\nThat is not a failure story. That is what happens when you build something real fast enough for the right people to notice.\n\nThe question for every developer building on top of AI infrastructure right now is not whether this will happen to them. It is whether they will be the person who gets hired by OpenAI or the person still maintaining the codebase after the category disappears.\n\nBuild things Claude cannot replace. Or build them fast enough that someone hires you before it does.\n\n---\n\n*This is the third article in an unplanned series on how AI is reshaping what developers build and who gets to build it. The first was [Claude Code Security: The Argument for Human-in-the-Loop Just Got Harder](/articles/claude-code-security-the-argument-for-human-in-the-loop-just-got-harder). The second was [Claude Killed My Startup](/articles/claude-killed-my-startup). We did not plan a series. The news planned it for us.*"
  },
  {
    "title": "Claude Killed My Startup. Now What Does That Mean for the Rest of Us?",
    "slug": "claude-killed-my-startup",
    "date": "2026-02-26",
    "summary": "Ira Bodnar woke up one morning and found her startup was gone. Not bankrupt. Not outcompeted. Just quietly made irrelevant by a feature update. The Ryze story is not about Ryze.",
    "content": "Ira Bodnar woke up one morning and found her startup was gone. Not bankrupt. Not outcompeted. Just quietly made irrelevant by a feature update.\n\nBodnar founded Ryze, a San Francisco-based tool that automated ad management across Google and Meta. Clients gave it access to their accounts, and it handled the rest. Clean problem, clean solution, several hundred paying customers in two months, a 70% deal close rate. By any early metric, a success story.\n\nThen Anthropic shipped a Meta Ads connector for Claude. Close rate dropped from 70% to 20%. Overnight.\n\n\"I woke up today and Claude killed my startup,\" she wrote on X. Not dramatic exaggeration. Just the math.\n\n---\n\n## The Ryze Story Is Not About Ryze\n\nEvery major publication covered it as a cautionary tale about one founder. That is the wrong frame.\n\nThe Ryze story is about what happens when a vertical SaaS product solves a problem that a foundation model decides to solve instead, for free, as a feature update, with no announcement and no warning.\n\nThis is not a new risk. Microsoft bundled features into Windows for decades. Apple absorbed entire app categories into iOS. What is new is the speed. Those consolidations happened over years. This happened overnight. And the mechanism is different. It is not a competing product. It is the same tool the customer was already using simply expanding what it does.\n\nThe week Ryze's numbers collapsed, Anthropic also shipped Claude Code Security, a tool for scanning codebases for vulnerabilities. Cybersecurity stocks including CrowdStrike and Palo Alto fell the same day. The pattern repeated itself in a different category within the same week.\n\nThis is not coincidence. This is the shape of how AI expands now.\n\n---\n\n## The Indian IT Parallel\n\nIndia built a $250 billion IT industry on a simple thesis: take work that Western companies need done, do it faster, do it cheaper, do it reliably. For three decades this worked. Entire cities built their economies around it. Families planned their futures around it.\n\nThe disruption is not theoretical anymore. It is in the quarterly earnings calls. It is in the hiring freezes. It is in the internal memos about automation targets. The work that sustained the Indian IT industry is increasingly the work that large language models are being trained to do.\n\nThe timing matters. The Indian IT industry consolidated around repeatable, well-defined tasks because those tasks had stable demand. That stability is what made the model work. It is also exactly what makes those tasks easy for AI to absorb.\n\nRyze was a startup version of the same structure. A well-defined task, done reliably, with a clear value proposition. It worked until the foundation model underneath it expanded its scope.\n\nThe parallel is uncomfortable and accurate.\n\n---\n\n## Claude Code Writes 80% of Itself Now\n\nAnthropic has disclosed that Claude Code, its agentic coding product, is now responsible for writing roughly 80% of its own codebase. Not as a test. As a production workflow.\n\nThis was framed as a capability demonstration. It is also a data point about where the labor requirement in software development is heading.\n\nThe standard response is that developers will move up the stack. Less implementation, more architecture. Less writing code, more directing systems that write code. This is probably true for senior engineers at well-resourced companies. It is less obvious for the junior developers who would have learned by writing the implementation work first.\n\nThe Indian IT pipeline depends on entry level demand. Companies hire junior engineers, train them on real projects, and build capability over time. If the entry level work gets automated, the pipeline breaks at the start.\n\nThis is not a distant forecast. It is a structural question that hiring data is already beginning to answer.\n\n---\n\n## What Is Actually Left for Small Developers\n\nBodnar's analysis of which product categories survive is worth reading carefully. Her framework: anything a foundation model can do with a connector will eventually be absorbed. Anything requiring persistent data, complex orchestration, or relationships that exist outside the model is safer.\n\nMCP, the protocol that lets Claude connect to external tools and services, is her most pointed observation. She argues it functions like a new app store, with one significant difference. In the current app economy, users browse and choose. In an MCP-dominated world, the model selects the tool. The user never sees the alternatives.\n\nThat reframes the competitive landscape entirely. If Claude picks which tool to use for a given task, the question of distribution stops being about reaching users directly. It becomes about being the tool Claude recommends, or being embedded in the workflow before Claude gets involved.\n\nFor small developers, this narrows the defensible space but does not eliminate it. The categories that hold:\n\nComplexity that foundation models do not yet handle. Ryze's pivot targets agencies managing hundreds of ad accounts with lean teams. That is not a job Claude does well today. It may be in two years. But two years is real runway.\n\nRelationships and trust that exist outside the model. Claude does not have your client history, your brand voice, your institutional knowledge of a specific domain. Products that encode that specificity are harder to absorb than products that solve generic problems.\n\nDistribution that predates the automation. Bodnar said it directly: \"When building is free, distribution becomes everything.\" The product moat is collapsing. The audience moat is not.\n\n---\n\n## The Positive Note You Were Waiting For\n\nThe honest version of the optimistic case is not that AI will create more jobs than it eliminates. Nobody can say that with certainty yet. The honest version is narrower and more specific.\n\nThe developers who survive this transition are not the ones who fought it. They are the ones who understood it early enough to position themselves on the right side of the line. Ryze saw the disruption coming and started pivoting weeks before it arrived. The company is not gone. It is smaller, more focused, and building in a space Claude does not yet reach.\n\nThe Indian IT industry is not going to zero. It is going to bifurcate. The commodity work gets automated. The work that requires judgment, context, relationships, and domain depth does not. The question is which side of that line you are building toward.\n\nFor small developers, the frame that holds up is this: AI is the most powerful tool in the history of building software. The developers who treat it as a threat will compete with it. The developers who treat it as infrastructure will build on top of it.\n\nRyze was not killed by Claude. Ryze was killed by building something Claude could replace. The lesson is not to stop building. The lesson is to build things Claude cannot.\n\n---\n\n*We covered the human-in-the-loop argument in our [Claude Code Security article](/articles/claude-code-security-the-argument-for-human-in-the-loop-just-got-harder). The Ryze story is why that argument matters beyond security.*"
  },
  {
    "title": "Gemini 3.1 Pro Scored 13 Out of 16. Here Is What That Actually Means.",
    "slug": "gemini-3-1-pro-scored-13-out-of-16",
    "date": "2026-02-26",
    "summary": "Google published a benchmark table. Gemini 3.1 Pro won 13 out of 16 comparisons. We looked at what those wins are actually against.",
    "content": "Google published a benchmark table. Gemini 3.1 Pro won 13 out of 16 comparisons. The headlines wrote themselves. \"Google is back.\" \"Gemini finally beats everyone.\" The usual cycle.\n\nWe are not writing that article.\n\n## The Setup\n\nWhen Google released Gemini 3.1 Pro, it published an official comparison table on deepmind.google covering six models: Gemini 3.1 Pro, Gemini 3 Pro, Claude Sonnet 4.6, Claude Opus 4.6, GPT-5.2, and GPT-5.3-Codex. Sixteen benchmarks. Bold text indicating the highest score per row. Clean, confident, shareable.\n\n13 out of 16 wins looks like a landslide. The problem is what those wins are actually against.\n\n## The GPT-5.3-Codex Problem\n\nGPT-5.3-Codex, OpenAI's coding-specialized model, appears in the comparison table across all 16 rows. Scores appear for only 2 of them: Terminal-Bench 2.0 and SWE-Bench Pro.\n\nFor the remaining 14 benchmarks, the Codex column reads a dash. Unpublished. Absent.\n\nGemini 3.1 Pro \"won\" those 14 comparisons against a competitor that did not show up. That is not a benchmark victory. That is a walkover.\n\nOpenAI positioned GPT-5.3-Codex specifically as a coding model, so its absence from general reasoning benchmarks is not entirely surprising. But Google chose to include it in the table anyway, across all 16 rows, while knowing most of its scores would be missing. The visual effect is a column full of dashes next to a column full of Gemini wins.\n\nThat is not deception exactly. It is framing. The kind that holds up technically and misleads practically.\n\n## What the Third-Party Data Actually Shows\n\nStrip away the absent competitor and look at the benchmarks where real comparisons exist.\n\nOn the [Artificial Analysis](/glossary/artificial-analysis) Intelligence Index, a composite benchmark covering reasoning, knowledge, mathematics, and coding evaluated independently, Gemini 3.1 Pro scored 57. Claude Opus 4.6 scored 53. Claude Sonnet 4.6 scored 51. Gemini leads here, and Artificial Analysis confirmed it after receiving pre-release access from Google.\n\nOn LMArena, which measures human preference through direct voting rather than automated scoring, Claude Opus 4.6 leads Gemini 3.1 Pro by 4 points. Essentially tied. The difference is within noise.\n\nOn GDPval-AA, which benchmarks enterprise task performance across finance, legal, and business applications, the gap is not close. Claude Sonnet 4.6 scored 1633. Claude Opus 4.6 scored 1606. Gemini 3.1 Pro scored 1317. That is nearly 300 points behind on the benchmark most relevant to production use cases. Artificial Analysis independently confirmed Gemini \"improved but did not take the lead\" in this category.\n\nThere is also a detail worth noting on multimodal understanding specifically. MMMU-Pro, which tests visual reasoning, shows Gemini 3 Pro scoring 81.0% and Gemini 3.1 Pro scoring 80.5%. The newer model underperforms its predecessor on one of Google's signature strengths. Newer does not always mean better across every dimension.\n\n## What Gemini 3.1 Pro Is Actually Good At\n\nThis is not an anti-Google article. The model is genuinely strong in the areas where it claims to be.\n\nOn agentic tasks, Gemini 3.1 Pro leads the benchmarks that exist. BrowseComp at 85.9%, MCP Atlas at 69.2%, terminal and coding-adjacent tasks where long context and reasoning combine. The 1 million token context window, now standard in the Gemini 2.X and 3.X line, is a real advantage for codebase-level understanding and complex multi-document tasks.\n\nOn scientific reasoning, the GPQA Diamond scores are competitive. On mathematics, AIME results hold up. On multilingual tasks, Gemini leads clearly.\n\nThe model is not pretending to be something it is not. The benchmarks where it wins are real wins. The issue is the selective presentation of where it competes and against whom.\n\n## The Broader Pattern\n\nGoogle is not unique here. Every major lab publishes benchmark tables designed to tell a particular story. OpenAI does it. Anthropic does it. The benchmark selection, the comparison set, the metrics chosen, all of these decisions shape the narrative before a single number appears.\n\nThe honest read of the AI benchmark landscape in early 2026 is that no single model dominates across everything. Gemini 3.1 Pro leads on automated composite intelligence metrics. Claude models lead on enterprise task performance and human preference voting. GPT-5.3-Codex leads on specialized coding benchmarks where it bothered to publish scores. These are not contradictory facts. They are a picture of a genuinely competitive field where the right model depends entirely on what you are building.\n\n## The Real Story About Google\n\nHere is the thing about Gemini that the \"Google is back\" framing keeps getting wrong.\n\nGoogle was never fully gone. Gemini has been deeply embedded in Search, Workspace, NotebookLM, Android, and a dozen other products touching billions of people daily. When Gemini improves, the effect is not a headline win over OpenAI. It is a slightly smarter autocomplete suggestion when drafting an email. A better summary in NotebookLM. A more useful response when someone Googles a question they would have asked a person ten years ago.\n\nThe reason a Gemini benchmark win feels different from an OpenAI benchmark win is not because Google made a better model. It is because Google is so deeply woven into how people already use technology that its improvements feel personal. When Gemini gets better, something you already use daily gets better without you doing anything.\n\nThat is not a comeback story. That is infrastructure quietly getting smarter.\n\nThe benchmark game is worth understanding. So is knowing when to stop playing it."
  },
  {
    "title": "Claude Code Security: The Argument for Human-in-the-Loop Just Got Harder",
    "slug": "claude-code-security-the-argument-for-human-in-the-loop-just-got-harder",
    "date": "2026-02-25",
    "summary": "Security was the last comfortable argument against AI replacing developers. Anthropic just made that argument significantly harder to make.",
    "content": "Security was the last comfortable argument against AI replacing developers. AI can write code, sure. But can it write secure code? Can it catch what it missed? Can it reason about vulnerabilities the way a seasoned security researcher does?\n\nAnthropic just made that argument significantly harder to make.\n\n## What It Actually Is\n\nClaude Code Security does three things: scans your entire codebase for vulnerabilities, validates each finding to minimise false positives, and suggests patches you can review and approve. Currently available in research preview for Claude Code Enterprise and Team customers.\n\nThe full details are on the official page: [claude.com/solutions/claude-code-security](https://claude.com/solutions/claude-code-security)\n\nWhat makes it different from existing tools is not the scanning part. Tools like Snyk, SonarQube, and Semgrep have been scanning codebases for years. The difference is in how it scans.\n\nTraditional security tools use pattern matching - they look for known vulnerability signatures. Fast, cheap, but limited. They miss context-dependent issues and produce high false positive rates that gradually train developers to ignore alerts.\n\nClaude Code Security reasons through your code like a security researcher. It reads Git history, traces data flows across files, and understands business logic. It then challenges its own findings before surfacing them, an adversarial verification pass that filters out noise before it reaches you.\n\nEvery finding comes with a proposed fix. Not a suggestion to go fix it somewhere. An actual patch, ready for review.\n\n## What This Means for Vibe Coding\n\nVibe coding has a well-known problem. You prompt, the AI generates, you ship. The code works. But does it work securely?\n\nMost vibe coders are not security engineers. They do not trace SQL injection vectors or think about authentication bypasses while iterating fast on a product. The code gets written, it looks fine, it ships.\n\nClaude Code Security sits at exactly that gap. Write fast, ship fast, but have something that actually understands your codebase checking what you might have introduced. The speed of vibe coding without the security debt that usually comes with it.\n\nThis is arguably where it has the most immediate impact, not on large enterprise teams with dedicated security engineers, but on solo developers and small teams building fast with AI assistance who currently have no security review step at all.\n\n## The Cybersecurity Industry is Next\n\nThe security industry has always had a peculiar relationship with automation. Security tools automate detection but humans do the reasoning, the triage, the remediation decisions.\n\nClaude Code Security compresses that loop significantly. Detection, reasoning, triage, and proposed remediation in one pass. The human still approves. But the cognitive load of the security review just dropped dramatically.\n\nThe junior security analyst role, the person who runs scans, triages findings, and writes up remediation reports, is directly in the path of this. Not eliminated, but changed. The value moves from running the process to evaluating the AI's output, catching what it misses, and making judgment calls on complex tradeoffs.\n\nOn the offensive side, if Claude can find vulnerabilities in your code, the same capability applied to someone else's code finds zero-days. [Anthropic](/glossary/anthropic)'s own red team blog post on this is worth reading: Evaluating and mitigating the growing risk of LLM-discovered 0-days. The security arms race just got a new participant.\n\n## The Indian IT Industry Angle\n\nIndia's IT industry is built on a specific labour arbitrage model. Large teams of developers doing work that Western companies outsource. Code reviews, QA, security audits, maintenance work. Services that scale with headcount.\n\nThat model is already under pressure from AI coding tools. Claude Code Security adds another layer. Security audits, one of the more lucrative service offerings, can increasingly be handled by a tool rather than a team of analysts billing hours.\n\nThe market reacted accordingly. Indian IT stocks took a hit when this news broke, not catastrophically, but noticeably. Infosys, TCS, Wipro all saw downward pressure. The market is pricing in what the industry is not yet ready to say out loud: the headcount-based services model has a shorter runway than the five year plans suggest.\n\nThis does not mean Indian IT collapses. It means the value proposition shifts. From scale to expertise. From doing the work to knowing what the AI missed. That transition is survivable, but it requires acknowledging it is happening.\n\n## The Human in the Loop Argument\n\nThe standard reassurance has always been: AI needs humans in the loop. Humans review. Humans approve. Humans catch what AI misses.\n\nThat argument still holds, technically. Claude Code Security requires human review and approval for every patch. [Anthropic](/glossary/anthropic) is explicit about this: Claude can make mistakes, review before applying, especially for critical systems.\n\nBut the nature of that human role is changing. It used to be: human does the security work. Now it is: human reviews the AI's security work. That is a meaningful difference in how many humans you need and what skills they require.\n\nThe question was never if AI would handle security analysis. It was always when. Looks like when is now, at least in research preview.\n\nThe human in the loop is not disappearing. It is just moving up the stack."
  },
  {
    "title": "Picobot: The AI Agent That Fits in Your Pocket",
    "slug": "picobot-the-ai-agent-that-fits-in-your-pocket",
    "date": "2026-02-25",
    "summary": "We stumbled upon Picobot yesterday, 1000+ stars in two weeks on a repo we'd never heard of. As good citizens of the AI community, we had to check it out. Here are our findings.",
    "content": "Before we dive in, here's the GitHub link for the curious: [github.com/louisho5/picobot](https://github.com/louisho5/picobot)\n\n## Not About the Size. About the Claims.\n\nPicobot is interesting not because it's small, though it is, but because of what it claims to run on. A $5/month VPS. A Raspberry Pi sitting on your desk. An old Android phone via Termux. Still in beta as of writing, but those are bold claims for an AI agent framework.\n\nMost AI agent frameworks today assume you have a decent machine, a cloud subscription, and a tolerance for 500MB Docker images that take 30 seconds to cold start. Picobot assumes the opposite.\n\nThe numbers back it up:\n\n- ~9MB binary\n- ~29MB Docker image\n- Instant cold start\n- ~10MB idle RAM\n- Zero dependencies\n\nNot familiar with some of these terms? We've got you covered in the [glossary](/glossary), each one links directly to a full explanation.\n\n## How It Works\n\nPicobot follows a straightforward architecture. At its core is an agent loop that receives a message, thinks about what tools it needs, calls those tools, and responds. What makes it different is everything around that loop is stripped to the minimum.\n\nThe agent uses any LLM you point it at. OpenRouter for cloud models like Gemini or GPT, or Ollama for fully local models that never leave your machine. The LLM does the thinking. Picobot handles everything else: memory, tool calling, scheduling, and the interfaces people actually use to talk to it.\n\nSpeaking of interfaces, this is where it gets genuinely useful.\n\n## Features: Where It Gets Interesting\n\nPicobot ships with 11 built-in tools out of the box. But the most interesting angle isn't any single feature, it's how they combine.\n\nImagine this: you're away from your laptop. A file lands in your downloads folder, a PDF report, a code review, a document someone sent. You open Telegram on your phone, message your Picobot instance, and ask it to read the file and summarise it. It opens the file on your machine, reads it, and sends you back the key points. All from your phone, all through a chat interface you already use.\n\nSame setup works with Discord. Different interface, same idea. Your desktop becomes remotely accessible through a chat app you already have open anyway.\n\nThat's the filesystem tool combined with Telegram or Discord integration working together. The rest of the toolkit extends this further:\n\n- **exec** - run shell commands remotely. Ask it to run your test suite while you're in a meeting.\n- **web** - fetch any webpage or API. Pull live data, check a URL, query an endpoint.\n- **spawn** - launch background subagents for parallel tasks\n- **cron** - schedule recurring tasks in plain language instead of cryptic cron syntax\n- **write_memory** - the AI agent remembers things between conversations. Tell it your preferences once, it uses them going forward.\n- **create_skill** - teach it new tricks. Describe what you want it to do, it writes the skill itself and reuses it automatically.\n\nThe persistent memory system deserves a mention. Most AI assistants forget everything the moment you close the chat. Picobot's memory survives restarts, organised by date, with semantic search that finds the most relevant memory for each query, not just the most recent one.\n\n## The OpenClaw Connection\n\nPicobot's own README opens with a direct nod to [OpenClaw](/glossary/openclaw): \"love the idea of open-source AI agents like [OpenClaw](/glossary/openclaw) but tired of the bloat?\" That's deliberate positioning. Where [OpenClaw](/glossary/openclaw) went broad and feature-rich before its acqui-hire by OpenAI, Picobot goes the opposite direction. Maximum capability, minimum footprint.\n\nIt's the same philosophy that produced Go as a language. Less is more when the constraints are real.\n\n## Should You Try It?\n\nIf you have a Raspberry Pi collecting dust, an old Android phone in a drawer, or just hate the idea of spinning up a $20/month cloud instance for a personal agent, Picobot is worth an afternoon. The 30-second Docker setup actually takes 30 seconds.\n\nBeta software means rough edges exist. But 1000+ stars in two weeks suggests the rough edges aren't dealbreakers for the people who've tried it.\n\nWorth bookmarking. The edge AI space is moving fast and Picobot is an early signal of where personal agents are headed, off the cloud and onto the devices you already own."
  }
]