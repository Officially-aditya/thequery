[
  {
    "title": "Week of February 17, 2026",
    "slug": "week-of-february-17-2026",
    "date": "2026-02-22",
    "summary": "Google drops Gemini 3.1 Pro with record benchmarks, Moonshot's Kimi K2.5 shakes up the open-weight race, and the EU AI Act compliance clock starts ticking.",
    "content": "## This Week in AI\n\n### Gemini 3.1 Pro Tops 13 of 16 Benchmarks\n\nGoogle DeepMind released Gemini 3.1 Pro on February 19, claiming the top spot on 13 of 16 major industry benchmarks. The model scores 77.1% on ARC-AGI-2 (more than doubling its predecessor) and 94.3% on GPQA Diamond. A new \"medium\" effort parameter lets developers trade reasoning depth for latency - useful for production workloads where speed matters more than peak accuracy.\n\n### Kimi K2.5 Goes Open-Weight with Agent Swarm\n\nMoonshot AI's Kimi K2.5 dropped in late January and is already making waves. The model leads on BrowseComp (74.9%), outperforms Gemini 3 Pro on SWE-Bench Verified (76.8%), and introduces agent swarm capabilities for parallel multi-agent workflows. At 76% lower cost than comparable models, it's putting real pricing pressure on Western AI providers.\n\n### MiniMax IPO Surges 109% on Debut\n\nChinese AI startup MiniMax completed its Hong Kong IPO, raising $619M with shares doubling on day one. Their M2.5 model scores 80.2% on SWE-Bench Verified while costing 1/20th of Claude Opus 4.6 - a data point that keeps the \"intelligence too cheap to meter\" narrative alive.\n\n### EU AI Act: First Compliance Guidelines Published\n\nThe EU AI Office published detailed compliance guidelines for general-purpose AI models. Foundation model providers must now disclose training data sources, energy consumption, and safety evaluation results. Full compliance deadline: September 2026, with provisional reporting starting in April.\n\n---\n\n**What we're watching next week:** ByteDance's Seed 2.0 benchmarks, OpenAI's rumored open-weight expansion, and whether Gemini 3.1 Pro's benchmark lead holds up in real-world evaluations."
  },
  {
    "title": "Week of February 10, 2026",
    "slug": "week-of-february-10-2026",
    "date": "2026-02-15",
    "summary": "This week: new reasoning benchmarks, open-source model releases, and the latest in AI regulation.",
    "content": "## Highlights\n\n### New Reasoning Benchmarks\n\nThe AI research community received a significant new evaluation tool this week with the release of ReasonBench 2.0, a comprehensive benchmark suite designed to test multi-step logical reasoning, mathematical problem-solving, and causal inference in large language models. Unlike previous benchmarks that focused on pattern matching or factual recall, ReasonBench 2.0 emphasizes problems that require genuine chain-of-thought reasoning and the ability to decompose complex questions into manageable sub-problems.\n\nEarly results show a clear stratification among leading models. While top-tier systems achieve roughly 78% accuracy on the full suite, performance drops sharply on problems requiring more than five reasoning steps, suggesting that current architectures still struggle with sustained logical chains. The benchmark's creators have also included an adversarial subset specifically designed to expose shortcuts and memorization, pushing the field toward models with more robust reasoning capabilities.\n\n### Open-Source Model Releases\n\nThis week saw two notable open-source model releases that are already generating excitement in the developer community. The first is Meridian-70B, a 70-billion parameter language model released under the Apache 2.0 license that demonstrates competitive performance with proprietary models on coding, analysis, and instruction-following tasks. Meridian-70B was trained with a novel curriculum learning strategy that progressively increases task difficulty during pretraining, and early adopters report strong results when fine-tuned for domain-specific applications in healthcare and legal document processing.\n\nThe second release is VisionForge 3.0, an open multimodal model capable of processing interleaved text and image inputs. VisionForge achieves state-of-the-art results among open models on visual question answering and document understanding benchmarks. Its relatively modest size of 13 billion parameters makes it practical to run on consumer hardware, lowering the barrier to entry for developers building multimodal applications.\n\n### AI Regulation Updates\n\nOn the regulatory front, the European Union's AI Office published its first set of detailed compliance guidelines for general-purpose AI models under the EU AI Act. The guidelines clarify reporting requirements for foundation model providers, including mandatory transparency disclosures about training data sources, energy consumption during training, and evaluation results on standardized safety benchmarks. Companies have until September 2026 to achieve full compliance, though a provisional reporting framework takes effect in April.\n\nMeanwhile, in the United States, a bipartisan Senate working group released a discussion draft for federal AI legislation that takes a risk-tiered approach. The proposal would establish mandatory safety evaluations for models above a specified compute threshold while creating voluntary certification programs for smaller systems. Industry response has been cautiously positive, with several major AI companies expressing support for the risk-based framework while requesting more specificity on evaluation standards and timelines."
  },
  {
    "title": "Picobot: The AI Agent That Fits in Your Pocket",
    "slug": "picobot-the-ai-agent-that-fits-in-your-pocket",
    "date": "2026-02-25",
    "summary": "We stumbled upon Picobot yesterday, 1000+ stars in two weeks on a repo we'd never heard of. As good citizens of the AI community, we had to check it out. Here are our findings.",
    "content": "Before we dive in, here's the GitHub link for the curious: [github.com/louisho5/picobot](https://github.com/louisho5/picobot)\n\n## Not About the Size. About the Claims.\n\nPicobot is interesting not because it's small, though it is, but because of what it claims to run on. A $5/month VPS. A Raspberry Pi sitting on your desk. An old Android phone via Termux. Still in beta as of writing, but those are bold claims for an AI agent framework.\n\nMost AI agent frameworks today assume you have a decent machine, a cloud subscription, and a tolerance for 500MB Docker images that take 30 seconds to cold start. Picobot assumes the opposite.\n\nThe numbers back it up:\n\n- ~9MB binary\n- ~29MB Docker image\n- Instant cold start\n- ~10MB idle RAM\n- Zero dependencies\n\nNot familiar with some of these terms? We've got you covered in the [glossary](/glossary), each one links directly to a full explanation.\n\n## How It Works\n\nPicobot follows a straightforward architecture. At its core is an agent loop that receives a message, thinks about what tools it needs, calls those tools, and responds. What makes it different is everything around that loop is stripped to the minimum.\n\nThe agent uses any LLM you point it at. OpenRouter for cloud models like Gemini or GPT, or Ollama for fully local models that never leave your machine. The LLM does the thinking. Picobot handles everything else: memory, tool calling, scheduling, and the interfaces people actually use to talk to it.\n\nSpeaking of interfaces, this is where it gets genuinely useful.\n\n## Features: Where It Gets Interesting\n\nPicobot ships with 11 built-in tools out of the box. But the most interesting angle isn't any single feature, it's how they combine.\n\nImagine this: you're away from your laptop. A file lands in your downloads folder, a PDF report, a code review, a document someone sent. You open Telegram on your phone, message your Picobot instance, and ask it to read the file and summarise it. It opens the file on your machine, reads it, and sends you back the key points. All from your phone, all through a chat interface you already use.\n\nSame setup works with Discord. Different interface, same idea. Your desktop becomes remotely accessible through a chat app you already have open anyway.\n\nThat's the filesystem tool combined with Telegram or Discord integration working together. The rest of the toolkit extends this further:\n\n- **exec** - run shell commands remotely. Ask it to run your test suite while you're in a meeting.\n- **web** - fetch any webpage or API. Pull live data, check a URL, query an endpoint.\n- **spawn** - launch background subagents for parallel tasks\n- **cron** - schedule recurring tasks in plain language instead of cryptic cron syntax\n- **write_memory** - the AI agent remembers things between conversations. Tell it your preferences once, it uses them going forward.\n- **create_skill** - teach it new tricks. Describe what you want it to do, it writes the skill itself and reuses it automatically.\n\nThe persistent memory system deserves a mention. Most AI assistants forget everything the moment you close the chat. Picobot's memory survives restarts, organised by date, with semantic search that finds the most relevant memory for each query, not just the most recent one.\n\n## The OpenClaw Connection\n\nPicobot's own README opens with a direct nod to [OpenClaw](/glossary/openclaw): \"love the idea of open-source AI agents like [OpenClaw](/glossary/openclaw) but tired of the bloat?\" That's deliberate positioning. Where [OpenClaw](/glossary/openclaw) went broad and feature-rich before its acqui-hire by OpenAI, Picobot goes the opposite direction. Maximum capability, minimum footprint.\n\nIt's the same philosophy that produced Go as a language. Less is more when the constraints are real.\n\n## Should You Try It?\n\nIf you have a Raspberry Pi collecting dust, an old Android phone in a drawer, or just hate the idea of spinning up a $20/month cloud instance for a personal agent, Picobot is worth an afternoon. The 30-second Docker setup actually takes 30 seconds.\n\nBeta software means rough edges exist. But 1000+ stars in two weeks suggests the rough edges aren't dealbreakers for the people who've tried it.\n\nWorth bookmarking. The edge AI space is moving fast and Picobot is an early signal of where personal agents are headed, off the cloud and onto the devices you already own."
  },
  {
    "title": "Claude Code Security: The Argument for Human-in-the-Loop Just Got Harder",
    "slug": "claude-code-security-the-argument-for-human-in-the-loop-just-got-harder",
    "date": "2026-02-25",
    "summary": "Security was the last comfortable argument against AI replacing developers. Anthropic just made that argument significantly harder to make.",
    "content": "Security was the last comfortable argument against AI replacing developers. AI can write code, sure. But can it write secure code? Can it catch what it missed? Can it reason about vulnerabilities the way a seasoned security researcher does?\n\nAnthropic just made that argument significantly harder to make.\n\n## What It Actually Is\n\nClaude Code Security does three things: scans your entire codebase for vulnerabilities, validates each finding to minimise false positives, and suggests patches you can review and approve. Currently available in research preview for Claude Code Enterprise and Team customers.\n\nThe full details are on the official page: [claude.com/solutions/claude-code-security](https://claude.com/solutions/claude-code-security)\n\nWhat makes it different from existing tools is not the scanning part. Tools like Snyk, SonarQube, and Semgrep have been scanning codebases for years. The difference is in how it scans.\n\nTraditional security tools use pattern matching - they look for known vulnerability signatures. Fast, cheap, but limited. They miss context-dependent issues and produce high false positive rates that gradually train developers to ignore alerts.\n\nClaude Code Security reasons through your code like a security researcher. It reads Git history, traces data flows across files, and understands business logic. It then challenges its own findings before surfacing them, an adversarial verification pass that filters out noise before it reaches you.\n\nEvery finding comes with a proposed fix. Not a suggestion to go fix it somewhere. An actual patch, ready for review.\n\n## What This Means for Vibe Coding\n\nVibe coding has a well-known problem. You prompt, the AI generates, you ship. The code works. But does it work securely?\n\nMost vibe coders are not security engineers. They do not trace SQL injection vectors or think about authentication bypasses while iterating fast on a product. The code gets written, it looks fine, it ships.\n\nClaude Code Security sits at exactly that gap. Write fast, ship fast, but have something that actually understands your codebase checking what you might have introduced. The speed of vibe coding without the security debt that usually comes with it.\n\nThis is arguably where it has the most immediate impact, not on large enterprise teams with dedicated security engineers, but on solo developers and small teams building fast with AI assistance who currently have no security review step at all.\n\n## The Cybersecurity Industry is Next\n\nThe security industry has always had a peculiar relationship with automation. Security tools automate detection but humans do the reasoning, the triage, the remediation decisions.\n\nClaude Code Security compresses that loop significantly. Detection, reasoning, triage, and proposed remediation in one pass. The human still approves. But the cognitive load of the security review just dropped dramatically.\n\nThe junior security analyst role, the person who runs scans, triages findings, and writes up remediation reports, is directly in the path of this. Not eliminated, but changed. The value moves from running the process to evaluating the AI's output, catching what it misses, and making judgment calls on complex tradeoffs.\n\nOn the offensive side, if Claude can find vulnerabilities in your code, the same capability applied to someone else's code finds zero-days. [Anthropic](/glossary/anthropic)'s own red team blog post on this is worth reading: Evaluating and mitigating the growing risk of LLM-discovered 0-days. The security arms race just got a new participant.\n\n## The Indian IT Industry Angle\n\nIndia's IT industry is built on a specific labour arbitrage model. Large teams of developers doing work that Western companies outsource. Code reviews, QA, security audits, maintenance work. Services that scale with headcount.\n\nThat model is already under pressure from AI coding tools. Claude Code Security adds another layer. Security audits, one of the more lucrative service offerings, can increasingly be handled by a tool rather than a team of analysts billing hours.\n\nThe market reacted accordingly. Indian IT stocks took a hit when this news broke, not catastrophically, but noticeably. Infosys, TCS, Wipro all saw downward pressure. The market is pricing in what the industry is not yet ready to say out loud: the headcount-based services model has a shorter runway than the five year plans suggest.\n\nThis does not mean Indian IT collapses. It means the value proposition shifts. From scale to expertise. From doing the work to knowing what the AI missed. That transition is survivable, but it requires acknowledging it is happening.\n\n## The Human in the Loop Argument\n\nThe standard reassurance has always been: AI needs humans in the loop. Humans review. Humans approve. Humans catch what AI misses.\n\nThat argument still holds, technically. Claude Code Security requires human review and approval for every patch. [Anthropic](/glossary/anthropic) is explicit about this: Claude can make mistakes, review before applying, especially for critical systems.\n\nBut the nature of that human role is changing. It used to be: human does the security work. Now it is: human reviews the AI's security work. That is a meaningful difference in how many humans you need and what skills they require.\n\nThe question was never if AI would handle security analysis. It was always when. Looks like when is now, at least in research preview.\n\nThe human in the loop is not disappearing. It is just moving up the stack."
  }
]