[
  {
    "title": "Week of February 17, 2026",
    "slug": "week-of-february-17-2026",
    "date": "2026-02-22",
    "summary": "Google drops Gemini 3.1 Pro with record benchmarks, Moonshot's Kimi K2.5 shakes up the open-weight race, and the EU AI Act compliance clock starts ticking.",
    "content": "## This Week in AI\n\n### Gemini 3.1 Pro Tops 13 of 16 Benchmarks\n\nGoogle DeepMind released Gemini 3.1 Pro on February 19, claiming the top spot on 13 of 16 major industry benchmarks. The model scores 77.1% on ARC-AGI-2 (more than doubling its predecessor) and 94.3% on GPQA Diamond. A new \"medium\" effort parameter lets developers trade reasoning depth for latency - useful for production workloads where speed matters more than peak accuracy.\n\n### Kimi K2.5 Goes Open-Weight with Agent Swarm\n\nMoonshot AI's Kimi K2.5 dropped in late January and is already making waves. The model leads on BrowseComp (74.9%), outperforms Gemini 3 Pro on SWE-Bench Verified (76.8%), and introduces agent swarm capabilities for parallel multi-agent workflows. At 76% lower cost than comparable models, it's putting real pricing pressure on Western AI providers.\n\n### MiniMax IPO Surges 109% on Debut\n\nChinese AI startup MiniMax completed its Hong Kong IPO, raising $619M with shares doubling on day one. Their M2.5 model scores 80.2% on SWE-Bench Verified while costing 1/20th of Claude Opus 4.6 - a data point that keeps the \"intelligence too cheap to meter\" narrative alive.\n\n### EU AI Act: First Compliance Guidelines Published\n\nThe EU AI Office published detailed compliance guidelines for general-purpose AI models. Foundation model providers must now disclose training data sources, energy consumption, and safety evaluation results. Full compliance deadline: September 2026, with provisional reporting starting in April.\n\n---\n\n**What we're watching next week:** ByteDance's Seed 2.0 benchmarks, OpenAI's rumored open-weight expansion, and whether Gemini 3.1 Pro's benchmark lead holds up in real-world evaluations."
  },
  {
    "title": "Week of February 10, 2026",
    "slug": "week-of-february-10-2026",
    "date": "2026-02-15",
    "summary": "This week: new reasoning benchmarks, open-source model releases, and the latest in AI regulation.",
    "content": "## Highlights\n\n### New Reasoning Benchmarks\n\nThe AI research community received a significant new evaluation tool this week with the release of ReasonBench 2.0, a comprehensive benchmark suite designed to test multi-step logical reasoning, mathematical problem-solving, and causal inference in large language models. Unlike previous benchmarks that focused on pattern matching or factual recall, ReasonBench 2.0 emphasizes problems that require genuine chain-of-thought reasoning and the ability to decompose complex questions into manageable sub-problems.\n\nEarly results show a clear stratification among leading models. While top-tier systems achieve roughly 78% accuracy on the full suite, performance drops sharply on problems requiring more than five reasoning steps, suggesting that current architectures still struggle with sustained logical chains. The benchmark's creators have also included an adversarial subset specifically designed to expose shortcuts and memorization, pushing the field toward models with more robust reasoning capabilities.\n\n### Open-Source Model Releases\n\nThis week saw two notable open-source model releases that are already generating excitement in the developer community. The first is Meridian-70B, a 70-billion parameter language model released under the Apache 2.0 license that demonstrates competitive performance with proprietary models on coding, analysis, and instruction-following tasks. Meridian-70B was trained with a novel curriculum learning strategy that progressively increases task difficulty during pretraining, and early adopters report strong results when fine-tuned for domain-specific applications in healthcare and legal document processing.\n\nThe second release is VisionForge 3.0, an open multimodal model capable of processing interleaved text and image inputs. VisionForge achieves state-of-the-art results among open models on visual question answering and document understanding benchmarks. Its relatively modest size of 13 billion parameters makes it practical to run on consumer hardware, lowering the barrier to entry for developers building multimodal applications.\n\n### AI Regulation Updates\n\nOn the regulatory front, the European Union's AI Office published its first set of detailed compliance guidelines for general-purpose AI models under the EU AI Act. The guidelines clarify reporting requirements for foundation model providers, including mandatory transparency disclosures about training data sources, energy consumption during training, and evaluation results on standardized safety benchmarks. Companies have until September 2026 to achieve full compliance, though a provisional reporting framework takes effect in April.\n\nMeanwhile, in the United States, a bipartisan Senate working group released a discussion draft for federal AI legislation that takes a risk-tiered approach. The proposal would establish mandatory safety evaluations for models above a specified compute threshold while creating voluntary certification programs for smaller systems. Industry response has been cautiously positive, with several major AI companies expressing support for the risk-based framework while requesting more specificity on evaluation standards and timelines."
  },
  {
    "title": "Picobot: The AI Agent That Fits in Your Pocket",
    "slug": "picobot-the-ai-agent-that-fits-in-your-pocket",
    "date": "2026-02-25",
    "summary": "We stumbled upon Picobot yesterday, 1000+ stars in two weeks on a repo we'd never heard of. As good citizens of the AI community, we had to check it out. Here are our findings.",
    "content": "Before we dive in, here's the GitHub link for the curious: [github.com/louisho5/picobot](https://github.com/louisho5/picobot)\n\n## Not About the Size. About the Claims.\n\nPicobot is interesting not because it's small, though it is, but because of what it claims to run on. A $5/month VPS. A Raspberry Pi sitting on your desk. An old Android phone via Termux. Still in beta as of writing, but those are bold claims for an AI agent framework.\n\nMost AI agent frameworks today assume you have a decent machine, a cloud subscription, and a tolerance for 500MB Docker images that take 30 seconds to cold start. Picobot assumes the opposite.\n\nThe numbers back it up:\n\n- ~11MB binary\n- ~33MB Docker image\n- Instant cold start\n- ~20MB idle RAM\n- Zero dependencies\n\nNot familiar with some of these terms? We've got you covered in the [glossary](/glossary), each one links directly to a full explanation.\n\n## How It Works\n\nPicobot follows a straightforward architecture. At its core is an agent loop that receives a message, thinks about what tools it needs, calls those tools, and responds. What makes it different is everything around that loop is stripped to the minimum.\n\nThe agent uses any LLM you point it at. OpenRouter for cloud models like Gemini or GPT, or Ollama for fully local models that never leave your machine. The LLM does the thinking. Picobot handles everything else: memory, tool calling, scheduling, and the interfaces people actually use to talk to it.\n\nSpeaking of interfaces, this is where it gets genuinely useful.\n\n## Features: Where It Gets Interesting\n\nPicobot ships with 11 built-in tools out of the box. But the most interesting angle isn't any single feature, it's how they combine.\n\nImagine this: you're away from your laptop. A file lands in your downloads folder, a PDF report, a code review, a document someone sent. You open Telegram on your phone, message your Picobot instance, and ask it to read the file and summarise it. It opens the file on your machine, reads it, and sends you back the key points. All from your phone, all through a chat interface you already use.\n\nSame setup works with Discord. Different interface, same idea. Your desktop becomes remotely accessible through a chat app you already have open anyway.\n\nThat's the filesystem tool combined with Telegram or Discord integration working together. The rest of the toolkit extends this further:\n\n- **exec** - run shell commands remotely. Ask it to run your test suite while you're in a meeting.\n- **web** - fetch any webpage or API. Pull live data, check a URL, query an endpoint.\n- **spawn** - launch background subagents for parallel tasks\n- **cron** - schedule recurring tasks in plain language instead of cryptic cron syntax\n- **write_memory** - the AI agent remembers things between conversations. Tell it your preferences once, it uses them going forward.\n- **create_skill** - teach it new tricks. Describe what you want it to do, it writes the skill itself and reuses it automatically.\n\nThe persistent memory system deserves a mention. Most AI assistants forget everything the moment you close the chat. Picobot's memory survives restarts, organised by date, with semantic search that finds the most relevant memory for each query, not just the most recent one.\n\n## The OpenClaw Connection\n\nPicobot's own README opens with a direct nod to OpenClaw: \"love the idea of open-source AI agents like OpenClaw but tired of the bloat?\" That's deliberate positioning. Where OpenClaw went broad and feature-rich before its acqui-hire by OpenAI, Picobot goes the opposite direction. Maximum capability, minimum footprint.\n\nIt's the same philosophy that produced Go as a language. Less is more when the constraints are real.\n\n## Should You Try It?\n\nIf you have a Raspberry Pi collecting dust, an old Android phone in a drawer, or just hate the idea of spinning up a $20/month cloud instance for a personal agent, Picobot is worth an afternoon. The 30-second Docker setup actually takes 30 seconds.\n\nBeta software means rough edges exist. But 1000+ stars in two weeks suggests the rough edges aren't dealbreakers for the people who've tried it.\n\nWorth bookmarking. The edge AI space is moving fast and Picobot is an early signal of where personal agents are headed, off the cloud and onto the devices you already own."
  }
]