[
  {
    "title": "Week of February 17, 2026",
    "slug": "week-of-february-17-2026",
    "date": "2026-02-22",
    "summary": "Google drops Gemini 3.1 Pro with record benchmarks, Moonshot's Kimi K2.5 shakes up the open-weight race, and the EU AI Act compliance clock starts ticking.",
    "content": "## This Week in AI\n\n### Gemini 3.1 Pro Tops 13 of 16 Benchmarks\n\nGoogle DeepMind released Gemini 3.1 Pro on February 19, claiming the top spot on 13 of 16 major industry benchmarks. The model scores 77.1% on ARC-AGI-2 (more than doubling its predecessor) and 94.3% on GPQA Diamond. A new \"medium\" effort parameter lets developers trade reasoning depth for latency — useful for production workloads where speed matters more than peak accuracy.\n\n### Kimi K2.5 Goes Open-Weight with Agent Swarm\n\nMoonshot AI's Kimi K2.5 dropped in late January and is already making waves. The model leads on BrowseComp (74.9%), outperforms Gemini 3 Pro on SWE-Bench Verified (76.8%), and introduces agent swarm capabilities for parallel multi-agent workflows. At 76% lower cost than comparable models, it's putting real pricing pressure on Western AI providers.\n\n### MiniMax IPO Surges 109% on Debut\n\nChinese AI startup MiniMax completed its Hong Kong IPO, raising $619M with shares doubling on day one. Their M2.5 model scores 80.2% on SWE-Bench Verified while costing 1/20th of Claude Opus 4.6 — a data point that keeps the \"intelligence too cheap to meter\" narrative alive.\n\n### EU AI Act: First Compliance Guidelines Published\n\nThe EU AI Office published detailed compliance guidelines for general-purpose AI models. Foundation model providers must now disclose training data sources, energy consumption, and safety evaluation results. Full compliance deadline: September 2026, with provisional reporting starting in April.\n\n---\n\n**What we're watching next week:** ByteDance's Seed 2.0 benchmarks, OpenAI's rumored open-weight expansion, and whether Gemini 3.1 Pro's benchmark lead holds up in real-world evaluations."
  },
  {
    "title": "Week of February 10, 2026",
    "slug": "week-of-february-10-2026",
    "date": "2026-02-15",
    "summary": "This week: new reasoning benchmarks, open-source model releases, and the latest in AI regulation.",
    "content": "## Highlights\n\n### New Reasoning Benchmarks\n\nThe AI research community received a significant new evaluation tool this week with the release of ReasonBench 2.0, a comprehensive benchmark suite designed to test multi-step logical reasoning, mathematical problem-solving, and causal inference in large language models. Unlike previous benchmarks that focused on pattern matching or factual recall, ReasonBench 2.0 emphasizes problems that require genuine chain-of-thought reasoning and the ability to decompose complex questions into manageable sub-problems.\n\nEarly results show a clear stratification among leading models. While top-tier systems achieve roughly 78% accuracy on the full suite, performance drops sharply on problems requiring more than five reasoning steps, suggesting that current architectures still struggle with sustained logical chains. The benchmark's creators have also included an adversarial subset specifically designed to expose shortcuts and memorization, pushing the field toward models with more robust reasoning capabilities.\n\n### Open-Source Model Releases\n\nThis week saw two notable open-source model releases that are already generating excitement in the developer community. The first is Meridian-70B, a 70-billion parameter language model released under the Apache 2.0 license that demonstrates competitive performance with proprietary models on coding, analysis, and instruction-following tasks. Meridian-70B was trained with a novel curriculum learning strategy that progressively increases task difficulty during pretraining, and early adopters report strong results when fine-tuned for domain-specific applications in healthcare and legal document processing.\n\nThe second release is VisionForge 3.0, an open multimodal model capable of processing interleaved text and image inputs. VisionForge achieves state-of-the-art results among open models on visual question answering and document understanding benchmarks. Its relatively modest size of 13 billion parameters makes it practical to run on consumer hardware, lowering the barrier to entry for developers building multimodal applications.\n\n### AI Regulation Updates\n\nOn the regulatory front, the European Union's AI Office published its first set of detailed compliance guidelines for general-purpose AI models under the EU AI Act. The guidelines clarify reporting requirements for foundation model providers, including mandatory transparency disclosures about training data sources, energy consumption during training, and evaluation results on standardized safety benchmarks. Companies have until September 2026 to achieve full compliance, though a provisional reporting framework takes effect in April.\n\nMeanwhile, in the United States, a bipartisan Senate working group released a discussion draft for federal AI legislation that takes a risk-tiered approach. The proposal would establish mandatory safety evaluations for models above a specified compute threshold while creating voluntary certification programs for smaller systems. Industry response has been cautiously positive, with several major AI companies expressing support for the risk-based framework while requesting more specificity on evaluation standards and timelines."
  }
]
